{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jerry', 'Tom']\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = ['Jerry','Tom','Lemon']\n",
    "label = np.array([1,1,0])\n",
    "label_list = label.tolist()\n",
    "idx = -1\n",
    "for text in a:\n",
    "    idx = idx + 1\n",
    "    if text == 'Lemon':\n",
    "        a.remove(text)\n",
    "        label_list.remove(label_list[idx])\n",
    "label = np.array(label_list)\n",
    "print(a)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(100,1000):\n",
    "    hundred = i/100\n",
    "    ten = (i-hundred*100) / 10\n",
    "    one = i % 10\n",
    "    if (i == hundred**2+ten**2+one**2):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['imag', 'hello', '2020'], ['look', 'forward', 'to', 'see', 'you']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk.stem\n",
    "from string import punctuation\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "punct = punctuation\n",
    "\n",
    "tokenized_list = [['imaging','hello',',','2020'],['looking','forward','to','seeing','you']]\n",
    "tokens = []\n",
    "for doc in tokenized_list:\n",
    "    tokens.append([token.lower() for token in doc if token.lower() not in punct])\n",
    "stemmed = []\n",
    "for doc in tokens:\n",
    "    stemmed.append([s.stem(token) for token in doc])\n",
    "print(stemmed)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['imag', 'hello', '2020'], ['look', 'forward', 'to', 'see', 'you']]\n",
      "['imag', 'hello', '2020', 'look', 'forward', 'to', 'see', 'you']\n",
      "[['imag_hello', 'hello_2020'], ['look_forward', 'forward_to', 'to_see', 'see_you']]\n",
      "['imag_hello', 'hello_2020', 'look_forward', 'forward_to', 'to_see', 'see_you']\n",
      "[['imag_hello_2020'], ['look_forward_to', 'forward_to_see', 'to_see_you']]\n",
      "['imag_hello_2020', 'look_forward_to', 'forward_to_see', 'to_see_you']\n"
     ]
    }
   ],
   "source": [
    "def getUnigram(words):\n",
    "    \"\"\"\n",
    "        Input: a list of words, e.g., ['I', 'am', 'Denny']\n",
    "        Output: a list of unigram\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    return words\n",
    "\n",
    "def getBigram(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., ['I', 'am', 'Denny']\n",
    "        Output: a list of bigram, e.g., ['I_am', 'am_Denny']\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "        #print words\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L-1):\n",
    "            for k in range(1,skip+2):\n",
    "                if i+k < L:\n",
    "                    lst.append( join_string.join([words[i], words[i+k]]) )\n",
    "    else:\n",
    "        # set it as unigram\n",
    "            lst = getUnigram(words)\n",
    "        #print 'lst returned'\n",
    "    return lst\n",
    "\n",
    "def getTrigram(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., ['I', 'am', 'Denny']\n",
    "       Output: a list of trigram, e.g., ['I_am_Denny']\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in range(L-2):\n",
    "            for k1 in range(1,skip+2):\n",
    "                for k2 in range(1,skip+2):\n",
    "                    if i+k1 < L and i+k1+k2 < L:\n",
    "                        lst.append( join_string.join([words[i], words[i+k1], words[i+k1+k2]]) )\n",
    "    else:\n",
    "        # set it as bigram\n",
    "        lst = getBigram(words, join_string, skip)\n",
    "    return lst\n",
    "\n",
    "join_str = \"_\"\n",
    "unigram = []\n",
    "unigram_corpus = []\n",
    "bigram = []\n",
    "bigram_corpus = []\n",
    "trigram = []\n",
    "trigram_corpus = []\n",
    "for doc in stemmed:\n",
    "    unigram.append(getUnigram(doc))\n",
    "    unigram_corpus.extend(getUnigram(doc))\n",
    "    bigram.append(getBigram(doc, join_str))\n",
    "    bigram_corpus.extend(getBigram(doc, join_str))\n",
    "    trigram.append(getTrigram(doc, join_str))\n",
    "    trigram_corpus.extend(getTrigram(doc, join_str))\n",
    "\n",
    "print(unigram)\n",
    "print(unigram_corpus)\n",
    "print(bigram)\n",
    "print(bigram_corpus)\n",
    "print(trigram)\n",
    "print(trigram_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grams = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "for gram in grams:\n",
    "        df[\"count_of_%s_%s\" % (feat_name, gram)] = list(df.apply(lambda x: len(x[feat_name + \"_\" + gram]), axis=1))\n",
    "        df[\"count_of_unique_%s_%s\" % (feat_name, gram)] = \\\n",
    "        list(df.apply(lambda x: len(set(x[feat_name + \"_\" + gram])), axis=1))\n",
    "        df[\"ratio_of_unique_%s_%s\" % (feat_name, gram)] = \\\n",
    "        map(try_divide, df[\"count_of_unique_%s_%s\"%(feat_name,gram)], df[\"count_of_%s_%s\"%(feat_name,gram)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-717cfc7aa949>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mvecB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvecB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munigram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvecB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(unigram_corpus)\n",
    "print(tfidf.shape)\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "vecB = TfidfVectorizer(vocabulary=vocabulary)\n",
    "tfidf = vecB.fit_transform(unigram)\n",
    "\n",
    "words = vecB.get_feature_names()\n",
    "print(words)\n",
    "for i in range(len(unigram_corpus)):\n",
    "    print('----Document %d----' % (i))\n",
    "    for j in range(len(words)):\n",
    "        if tfidf[i,j] > 1e-5:\n",
    "              print( words[j], tfidf[i,j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5 3.5 4.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "list = [[1,2,3],[4,5,6]]\n",
    "\n",
    "print(np.array(list).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0},\n",
       " {'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0},\n",
       " {'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0},\n",
       " {'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "def compute_sentiment(sentences):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        vs = sid.polarity_scores(sentence)\n",
    "        result.append(vs)\n",
    "    \n",
    "#    result_np = np.array(result).mean(axis=0)\n",
    "#    result = result_np.tolist()\n",
    "    return result\n",
    "\n",
    "compute_sentiment(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most automated sentiment analysis tools are shit.\n",
      "{'neg': 0.375, 'neu': 0.625, 'pos': 0.0, 'compound': -0.5574}\n",
      "compound: -0.5574, neg: 0.375, neu: 0.625, pos: 0.0, VADER sentiment analysis is the shit.\n",
      "{'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.6124}\n",
      "compound: 0.6124, neg: 0.0, neu: 0.556, pos: 0.444, Sentiment analysis has never been good.\n",
      "{'neg': 0.325, 'neu': 0.675, 'pos': 0.0, 'compound': -0.3412}\n",
      "compound: -0.3412, neg: 0.325, neu: 0.675, pos: 0.0, Sentiment analysis with VADER has never been this good.\n",
      "{'neg': 0.0, 'neu': 0.703, 'pos': 0.297, 'compound': 0.5228}\n",
      "compound: 0.5228, neg: 0.0, neu: 0.703, pos: 0.297, Warren Beatty has never been so entertaining.\n",
      "{'neg': 0.0, 'neu': 0.616, 'pos': 0.384, 'compound': 0.5777}\n",
      "compound: 0.5777, neg: 0.0, neu: 0.616, pos: 0.384, I won't say that the movie is astounding and I wouldn't claim that the movie is too banal either.\n",
      "{'neg': 0.0, 'neu': 0.851, 'pos': 0.149, 'compound': 0.4215}\n",
      "compound: 0.4215, neg: 0.0, neu: 0.851, pos: 0.149, "
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "\n",
    "sentences = []\n",
    "tricky_sentences = [\n",
    "\"Most automated sentiment analysis tools are shit.\",\n",
    "\"VADER sentiment analysis is the shit.\",\n",
    "\"Sentiment analysis has never been good.\",\n",
    "\"Sentiment analysis with VADER has never been this good.\",\n",
    "\"Warren Beatty has never been so entertaining.\",\n",
    "\"I won't say that the movie is astounding and I wouldn't claim that \\\n",
    "the movie is too banal either.\",\n",
    "]\n",
    "sentences.extend(tricky_sentences)\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    print(ss)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
