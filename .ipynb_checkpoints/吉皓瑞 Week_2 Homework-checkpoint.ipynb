{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the \"movie_reviews\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>To an entire generation of filmgoers, it just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Pixar classic is one of the best kids' movies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Apesar de representar um imenso avanço tecnoló...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>When Woody perks up in the opening scene, it's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduced not one but two indelible character...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  To an entire generation of filmgoers, it just ...\n",
       "1      1  Pixar classic is one of the best kids' movies ...\n",
       "2      1  Apesar de representar um imenso avanço tecnoló...\n",
       "3      1  When Woody perks up in the opening scene, it's...\n",
       "4      1  Introduced not one but two indelible character..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Load movie reviews dataset\n",
    "df = pd.read_csv( './data/movie_reviews.csv')\n",
    "texts = df.text.values #pd.Series -> np.ndarray\n",
    "label = df.label.values #pd.Series -> np.ndarray\n",
    "df.head()\n",
    "#print(texts.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the raw text strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example review:\n",
      "   Raw: To an entire generation of filmgoers, it just might represent the most significant leap in storytelling that they will ever see... \n",
      "\n",
      "   Tokenized: ['To', 'an', 'entire', 'generation', 'of', 'filmgoers', ',', 'it', 'just', 'might', 'represent', 'the', 'most', 'significant', 'leap', 'in', 'storytelling', 'that', 'they', 'will', 'ever', 'see', '...']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Transform each review string as a list of token strings. May take a few seconds\n",
    "text_token = [nltk.word_tokenize(text) for text in texts]\n",
    "\n",
    "n = 0 #arbitrary pick\n",
    "print('Example review:\\n   Raw: {} \\n\\n   Tokenized: {}'.format(texts[n], [i for i in text_token[n]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.  the  freq:  749124\n",
      " 2.    ,  freq:  643961\n",
      " 3.    .  freq:  573701\n",
      " 4.    a  freq:  388669\n",
      " 5.  and  freq:  380567\n",
      " 6.   of  freq:  347832\n",
      " 7.   to  freq:  303867\n",
      " 8.   is  freq:  251629\n",
      " 9.   ''  freq:  229560\n",
      "10.   it  freq:  217043\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Note that we convert all tokens to lower case, otherwise words like *The* and *the* are different tokens.\n",
    "text_counter = Counter(token.lower() for sentence in text_token for token in sentence)\n",
    "top10 = text_counter.most_common()[:10]\n",
    "for i, t in enumerate(top10):\n",
    "    print('{:>2}.{:>5}  freq: {:>7}'.format(i+1, t[0], t[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning text data, check the effectiveness of lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many texts are empty: 15\n",
      "text cleaned:\n",
      " [['entire', 'generation', 'filmgoers', 'might', 'represent', 'significant', 'leap', 'storytelling', 'ever', 'see', '...'], ['pixar', 'classic', 'one', 'best', 'kids', 'movies', 'time'], ['apesar', 'de', 'representar', 'um', 'imenso', 'avanço', 'tecnológico', 'força', 'filme', 'reside', 'carisma', 'de', 'seus', 'personagens', 'e', 'charme', 'de', 'sua', 'história'], ['woody', 'perks', 'opening', 'scene', \"'s\", 'toy', 'cowboy', 'comes', 'alive', \"'re\", 'watching', 'rebirth', 'art', 'form'], ['introduced', 'one', 'two', 'indelible', 'characters', 'pop', 'culture', 'pantheon', 'cowboy', 'rag-doll', 'woody', 'tom', 'hanks', 'plastic', 'space', 'ranger', 'buzz', 'lightyear', 'tim', 'allen', 'blu-ray']]\n",
      " 1.   ''  freq:  229560\n",
      " 2.   's  freq:  157679\n",
      " 3.   ``  freq:  102253\n",
      " 4.movie  freq:   95453\n",
      " 5. film  freq:   92791\n",
      " 6.  n't  freq:   74589\n",
      " 7.  one  freq:   59797\n",
      " 8. like  freq:   44255\n",
      " 9.  ...  freq:   33143\n",
      "10. good  freq:   32642\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from itertools import chain\n",
    "\n",
    "def clean_text(tokenized_list, sw, punct, lemmatize=True):\n",
    "    new_list = []\n",
    "    for doc in tokenized_list:\n",
    "        new_list.append([token.lower() for token in doc if token.lower() not in chain(punct, sw)])\n",
    "    return new_list\n",
    "\n",
    "# Remove punctuations and stopwords, and lower-case text\n",
    "sw = stopwords.words('english')\n",
    "punct = punctuation\n",
    "text_cleaned = clean_text(text_token, sw, punct)\n",
    "\n",
    "idx = -1\n",
    "count = 0\n",
    "label_list = label.tolist()\n",
    "for text in text_cleaned:\n",
    "    idx = idx + 1\n",
    "    if len(text) == 0:\n",
    "        count = count + 1\n",
    "        text_cleaned.remove(text)\n",
    "        label_list.remove(label_list[idx])\n",
    "label = np.array(label_list)\n",
    "\n",
    "print('how many texts are empty:',count)\n",
    "print('text cleaned:\\n',text_cleaned[0:5])\n",
    "#print(len(text_cleaned))\n",
    "#Note that we convert all tokens to lower case, otherwise words like *The* and *the* are different tokens.\n",
    "text_counter = Counter(token.lower() for sentence in text_cleaned for token in sentence)\n",
    "top10 = text_counter.most_common()[:10]\n",
    "for i, t in enumerate(top10):\n",
    "    print('{:>2}.{:>5}  freq: {:>7}'.format(i+1, t[0], t[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW，TF-IDF，Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models discussed in this section are all trained on our \"movie_reviews\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:...\n",
      "1:entire\n",
      "2:ever\n",
      "3:filmgoers\n",
      "4:generation\n",
      "5:leap\n",
      "6:might\n",
      "7:represent\n",
      "8:see\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n",
      "[(11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n",
      "[(18, 1), (19, 1), (20, 1), (21, 1), (22, 3), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]\n",
      "[(35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1)]\n",
      "[(15, 1), (40, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Create a dictionary from list of documents in order to create BOW model\n",
    "dictionary = corpora.Dictionary(text_cleaned)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_cleaned]\n",
    "\n",
    "#print dictionary and corpus\n",
    "count=0\n",
    "for key,value in dictionary.items():\n",
    "    count = count + 1\n",
    "    if count<10:\n",
    "        print('{key}:{value}'.format(key = key, value = value))\n",
    "        \n",
    "for i in range(5):\n",
    "    print(corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example review:\n",
      " ['entire', 'generation', 'filmgoers', 'might', 'represent', 'significant', 'leap', 'storytelling', 'ever', 'see', '...']\n",
      "TFIDF scores:\n",
      ": [[ 0.     0.122]\n",
      " [ 1.     0.233]\n",
      " [ 2.     0.152]\n",
      " [ 3.     0.475]\n",
      " [ 4.     0.32 ]\n",
      " [ 5.     0.403]\n",
      " [ 6.     0.187]\n",
      " [ 7.     0.38 ]\n",
      " [ 8.     0.126]\n",
      " [ 9.     0.344]\n",
      " [10.     0.323]]\n",
      "Example review:\n",
      " ['pixar', 'classic', 'one', 'best', 'kids', 'movies', 'time']\n",
      "TFIDF scores:\n",
      ": [[11.     0.252]\n",
      " [12.     0.358]\n",
      " [13.     0.388]\n",
      " [14.     0.255]\n",
      " [15.     0.147]\n",
      " [16.     0.726]\n",
      " [17.     0.21 ]]\n",
      "Example review:\n",
      " ['apesar', 'de', 'representar', 'um', 'imenso', 'avanço', 'tecnológico', 'força', 'filme', 'reside', 'carisma', 'de', 'seus', 'personagens', 'e', 'charme', 'de', 'sua', 'história']\n",
      "TFIDF scores:\n",
      ": [[18.     0.222]\n",
      " [19.     0.307]\n",
      " [20.     0.248]\n",
      " [21.     0.245]\n",
      " [22.     0.343]\n",
      " [23.     0.144]\n",
      " [24.     0.162]\n",
      " [25.     0.266]\n",
      " [26.     0.207]\n",
      " [27.     0.272]\n",
      " [28.     0.201]\n",
      " [29.     0.288]\n",
      " [30.     0.23 ]\n",
      " [31.     0.197]\n",
      " [32.     0.193]\n",
      " [33.     0.325]\n",
      " [34.     0.156]]\n",
      "Example review:\n",
      " ['woody', 'perks', 'opening', 'scene', \"'s\", 'toy', 'cowboy', 'comes', 'alive', \"'re\", 'watching', 'rebirth', 'art', 'form']\n",
      "TFIDF scores:\n",
      ": [[3.50e+01 1.59e-01]\n",
      " [3.60e+01 4.40e-02]\n",
      " [3.70e+01 2.56e-01]\n",
      " [3.80e+01 2.14e-01]\n",
      " [3.90e+01 1.76e-01]\n",
      " [4.00e+01 3.27e-01]\n",
      " [4.10e+01 2.32e-01]\n",
      " [4.20e+01 2.24e-01]\n",
      " [4.30e+01 4.58e-01]\n",
      " [4.40e+01 4.15e-01]\n",
      " [4.50e+01 1.51e-01]\n",
      " [4.60e+01 3.21e-01]\n",
      " [4.70e+01 1.50e-01]\n",
      " [4.80e+01 2.99e-01]]\n",
      "Example review:\n",
      " ['introduced', 'one', 'two', 'indelible', 'characters', 'pop', 'culture', 'pantheon', 'cowboy', 'rag-doll', 'woody', 'tom', 'hanks', 'plastic', 'space', 'ranger', 'buzz', 'lightyear', 'tim', 'allen', 'blu-ray']\n",
      "TFIDF scores:\n",
      ": [[1.50e+01 4.90e-02]\n",
      " [4.00e+01 2.14e-01]\n",
      " [4.80e+01 1.95e-01]\n",
      " [4.90e+01 1.78e-01]\n",
      " [5.00e+01 2.02e-01]\n",
      " [5.10e+01 2.43e-01]\n",
      " [5.20e+01 8.40e-02]\n",
      " [5.30e+01 1.67e-01]\n",
      " [5.40e+01 2.18e-01]\n",
      " [5.50e+01 2.60e-01]\n",
      " [5.60e+01 1.87e-01]\n",
      " [5.70e+01 3.65e-01]\n",
      " [5.80e+01 2.66e-01]\n",
      " [5.90e+01 2.09e-01]\n",
      " [6.00e+01 1.78e-01]\n",
      " [6.10e+01 4.02e-01]\n",
      " [6.20e+01 2.44e-01]\n",
      " [6.30e+01 1.59e-01]\n",
      " [6.40e+01 1.89e-01]\n",
      " [6.50e+01 1.64e-01]\n",
      " [6.60e+01 8.90e-02]]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "import numpy as np\n",
    "\n",
    "#Create a TFIDF Model for the corpus\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "for i in range(5):\n",
    "    print('Example review:\\n',text_cleaned[i])\n",
    "    print('TFIDF scores:\\n:',np.round(tfidf[corpus[i]],3))\n",
    "#print('Example review featurized with TF-IDF scores : \\n{}'.format([(dictionary[i[0]], round(i[1],3)) for i in tfidf[corpus[n]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# Training word2vec model on already cleaned text. This may take a few minutes.\n",
    "word2vec = models.Word2Vec(text_cleaned,\n",
    "                        size = 300,\n",
    "                        window = 5,\n",
    "                        min_count = 1,                      # set \"min_count\" = 1 to make sure every word corresponds to a vector, in case something go wrong in the Naive Doc2Vec process\n",
    "                        sg = 0,\n",
    "                        alpha = 0.025,                      # if I set alpha = 0.01, performance will be much worse\n",
    "                        iter=10,\n",
    "                        batch_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 7.92698469e-03, -2.01397687e-02, -2.53254939e-02,  2.97184605e-02,\n",
       "        4.93387831e-03,  3.49686816e-02, -6.64250180e-02,  3.59089039e-02,\n",
       "        5.81033267e-02, -4.68371361e-02, -2.28206646e-02,  2.80665997e-02,\n",
       "       -9.75980163e-02,  2.08341088e-02,  3.15778307e-03,  6.60883170e-03,\n",
       "       -3.25060338e-02,  8.95441324e-02,  2.66021118e-03,  6.97508603e-02,\n",
       "        3.22876424e-02, -1.52867977e-02, -2.22618449e-02, -5.51112089e-03,\n",
       "        2.43894150e-03, -2.32183766e-02, -6.03412762e-02, -1.16388589e-01,\n",
       "        7.80114755e-02,  2.99245119e-02, -5.32919429e-02, -3.17887333e-03,\n",
       "       -4.18883860e-02,  7.06827417e-02,  4.22851332e-02, -2.74281092e-02,\n",
       "        1.09859653e-01,  3.76331806e-03, -4.56196629e-02, -3.83603573e-02,\n",
       "        4.16262150e-02,  5.43430820e-03, -1.28748771e-02,  3.60228983e-03,\n",
       "       -5.37535772e-02, -1.69625729e-02,  1.05463624e-01, -5.87137826e-02,\n",
       "       -5.74758835e-02, -3.43802869e-02,  7.84358233e-02,  5.03958762e-03,\n",
       "       -1.24559822e-02, -4.91448529e-02, -6.26383349e-02, -3.53463888e-02,\n",
       "       -5.10642910e-03, -9.21593159e-02,  1.83049473e-03,  4.63044718e-02,\n",
       "       -7.32546225e-02, -2.74609886e-02, -4.76539619e-02,  4.83957455e-02,\n",
       "        2.22019292e-02,  4.10759412e-02, -3.60504985e-02,  4.55619358e-02,\n",
       "       -1.84582379e-02, -7.17757717e-02,  3.96111310e-02, -8.81130025e-02,\n",
       "       -6.02807067e-02, -3.98249105e-02, -9.99869332e-02, -5.52006252e-02,\n",
       "        3.43571119e-02,  2.09649261e-02,  1.88028924e-02, -3.96619588e-02,\n",
       "       -8.82137939e-02, -1.09131038e-02,  1.45422459e-01,  3.22075151e-02,\n",
       "        2.62756627e-02,  5.31084351e-02,  3.99630750e-03,  3.87130231e-02,\n",
       "        1.61192827e-02, -2.88642254e-02, -8.36513191e-02, -2.34043039e-02,\n",
       "        6.40934035e-02, -6.13611042e-02,  2.79846657e-02,  4.52811122e-02,\n",
       "        5.05697206e-02,  2.02466827e-02,  9.59872175e-03, -3.38168181e-02,\n",
       "       -1.89818982e-02,  2.59114485e-02,  5.68354316e-02,  4.08597291e-02,\n",
       "       -1.23676667e-02, -7.61462701e-03, -3.21238935e-02,  1.50023801e-02,\n",
       "       -4.08177860e-02, -3.86397913e-02, -3.46076228e-02,  2.97574066e-02,\n",
       "       -6.50011301e-02, -2.05642078e-02, -5.20509388e-03, -3.82998884e-02,\n",
       "       -1.91067234e-02, -8.27612802e-02,  4.63668741e-02,  3.33283134e-02,\n",
       "       -1.82511937e-02, -3.59327532e-02,  4.83716130e-02,  1.32367117e-02,\n",
       "       -1.09345004e-01,  1.01990819e-01,  2.46823989e-02, -7.79766589e-02,\n",
       "        2.26303414e-02,  5.51409670e-04,  4.70542647e-02, -4.72560944e-03,\n",
       "        4.08685505e-02,  1.04795903e-01,  1.49531756e-02, -2.37176027e-02,\n",
       "       -1.10652121e-02,  4.64033820e-02, -5.35982326e-02, -1.75009854e-03,\n",
       "        1.02812849e-01,  2.40959041e-02, -6.23963997e-02, -6.71763495e-02,\n",
       "       -3.02218404e-02,  6.66672587e-02,  2.41402909e-02,  8.50625932e-02,\n",
       "       -7.99335465e-02,  1.18262082e-01, -3.55876022e-04, -1.92239005e-02,\n",
       "       -2.93273404e-02,  5.71740838e-03,  1.48524763e-02,  5.29346429e-03,\n",
       "       -1.37312248e-01, -7.25340098e-02,  3.69589515e-02,  3.20621729e-02,\n",
       "        4.49886173e-02,  4.97237369e-02, -3.77012193e-02, -2.73916721e-02,\n",
       "       -1.59657318e-02, -1.05285116e-01, -3.61708133e-03,  2.54335217e-02,\n",
       "        5.33401929e-02,  9.42799300e-02,  1.10230349e-01, -1.95800420e-02,\n",
       "        3.06379404e-02,  6.87718838e-02, -2.09995098e-02, -4.36172038e-02,\n",
       "        5.24063781e-02, -3.34573053e-02, -5.75890532e-03, -2.69666594e-02,\n",
       "       -6.03265725e-02, -7.39301071e-02, -1.15708873e-01, -2.19648099e-03,\n",
       "        4.92374413e-02,  1.05708186e-02,  2.89373528e-02,  7.11922497e-02,\n",
       "        2.08576545e-02, -5.06226830e-02,  4.42011543e-02,  1.98956858e-02,\n",
       "        1.95837170e-02,  2.46260054e-02, -2.47979388e-02, -2.11902801e-02,\n",
       "       -3.41330767e-02, -1.08031388e-02,  1.09954216e-01,  8.14032182e-03,\n",
       "        8.08614939e-02,  6.43320978e-02,  1.41591672e-02, -1.22572236e-01,\n",
       "        9.01587903e-02, -1.06725580e-04,  1.92933939e-02, -2.60778237e-02,\n",
       "       -1.33836537e-03,  7.69608933e-03,  4.61055711e-02, -1.23086609e-01,\n",
       "       -5.88198937e-02,  3.43781151e-02, -7.72206404e-04, -3.74068171e-02,\n",
       "       -5.33265993e-02,  7.83535987e-02,  5.13980724e-02, -9.32056010e-02,\n",
       "        8.96423981e-02, -4.66437526e-02,  2.73532723e-03,  1.66252218e-02,\n",
       "       -5.78117266e-04, -8.11904576e-03,  2.69539542e-02, -8.53819251e-02,\n",
       "        1.48174027e-02, -4.86536026e-02, -6.84299245e-02, -3.15671302e-02,\n",
       "        4.59277257e-02, -5.99957891e-02, -7.82436803e-02, -1.36315189e-02,\n",
       "        2.52357009e-03, -1.58140864e-02, -5.31420000e-02,  6.30026162e-02,\n",
       "       -1.30349062e-02, -3.20318597e-03, -5.68012102e-03,  6.54891953e-02,\n",
       "       -3.03983712e-03, -8.24797433e-03, -2.06693504e-02,  4.89538498e-02,\n",
       "       -1.23332813e-01,  6.58215303e-03, -1.03569932e-01, -4.36365604e-02,\n",
       "       -1.27085656e-01, -1.09495949e-02,  3.87053378e-02,  1.59270167e-02,\n",
       "        9.17864963e-02, -1.35763278e-02, -7.41378739e-02,  2.87584141e-02,\n",
       "       -4.88840900e-02,  5.26758134e-02, -5.44935465e-04, -2.18227878e-02,\n",
       "       -9.39664394e-02,  5.76189272e-02, -2.17093043e-02,  4.45885002e-04,\n",
       "        9.83413085e-02, -5.75792976e-03, -2.25054659e-03, -2.48529445e-02,\n",
       "       -6.33496717e-02,  5.27585074e-02, -6.66069274e-04,  2.55044992e-03,\n",
       "       -4.62811161e-03, -3.22562233e-02, -3.64180584e-03,  1.38854915e-02,\n",
       "        2.74758488e-02, -6.26420975e-02, -4.26667891e-02, -8.73351321e-02,\n",
       "       -1.27956256e-01, -3.49118523e-02, -5.92820253e-03, -2.26676166e-02,\n",
       "        5.31108752e-02, -2.99836919e-02, -7.30351806e-02, -1.91046968e-02,\n",
       "       -1.06758075e-02, -1.33439135e-02, -7.11969808e-02, -9.97175053e-02,\n",
       "        3.23553681e-02,  3.73712629e-02,  4.55549024e-02, -2.68843677e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector = model.wv['food']\n",
    "# print(vector)\n",
    "word2vec['representar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('girl', 0.7022062540054321),\n",
       " ('kid', 0.6464823484420776),\n",
       " ('orphan', 0.5869698524475098),\n",
       " ('lad', 0.5416067838668823),\n",
       " ('sister', 0.5377366542816162),\n",
       " ('son', 0.5364755392074585),\n",
       " ('boys', 0.5278647541999817),\n",
       " ('daughter', 0.5146600008010864),\n",
       " ('child', 0.5090888738632202),\n",
       " ('brother', 0.5007424354553223)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words closest to the token 'boy'\n",
    "word2vec.wv.most_similar(positive=['boy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Doc2Vec method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_docs:\n",
      " ['entire', 'generation', 'filmgoers', 'might', 'represent', 'significant', 'leap', 'storytelling', 'ever', 'see', '...']\n",
      "tagged_docs:\n",
      " TaggedDocument(['entire', 'generation', 'filmgoers', 'might', 'represent', 'significant', 'leap', 'storytelling', 'ever', 'see', '...'], [0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:531: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:535: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "tokenized_docs = text_cleaned\n",
    "\n",
    "print('tokenized_docs:\\n',tokenized_docs[0])\n",
    "\n",
    "# Convert tokenized documents to TaggedDocuments\n",
    "tagged_docs = [TaggedDocument(doc, tags=[idx]) for idx, doc in enumerate(tokenized_docs)]\n",
    "\n",
    "print('tagged_docs:\\n',tagged_docs[0])\n",
    "\n",
    "# Create and train the doc2vec model. May take a few seconds\n",
    "doc2vec = Doc2Vec(size=300, window=5, min_count=5, dm = 1, iter=10)\n",
    "\n",
    "# Build the word2vec model from the corpus\n",
    "doc2vec.build_vocab(tagged_docs)\n",
    "\n",
    "# Train the models\n",
    "doc2vec.train(tagged_docs, epochs=10, total_examples=doc2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11640756,  0.00959367,  0.0027819 , -0.03638881, -0.01558619,\n",
       "        0.0022028 , -0.04430351, -0.02634423,  0.01320325,  0.03729074,\n",
       "       -0.02903551,  0.01164167, -0.02520508, -0.04448992, -0.04869303,\n",
       "        0.06765765, -0.07196426,  0.06566668, -0.01529197,  0.11903958,\n",
       "        0.0127807 , -0.0612068 ,  0.04332183,  0.04714288, -0.02795585,\n",
       "       -0.06994917,  0.22520344, -0.07885246,  0.08964897,  0.09731934,\n",
       "       -0.04219134,  0.01476393, -0.06538308,  0.05677607,  0.0147265 ,\n",
       "        0.0333243 ,  0.11887433, -0.07735306, -0.0982732 ,  0.11280705,\n",
       "       -0.02923373, -0.06992949,  0.02145182, -0.04887317,  0.00080843,\n",
       "        0.02430671,  0.04117583, -0.02046664, -0.09429718, -0.04877159,\n",
       "        0.09297715, -0.03404789, -0.02101492, -0.0170896 , -0.024143  ,\n",
       "        0.04043116, -0.00307455, -0.04466435, -0.03466206, -0.03094201,\n",
       "       -0.00222797, -0.01393872,  0.02641464,  0.00348142, -0.06175144,\n",
       "        0.01550744, -0.03016403, -0.00313367,  0.01504597, -0.18567738,\n",
       "        0.06485661, -0.01854459, -0.01309706, -0.05223663, -0.11051056,\n",
       "       -0.02846253, -0.03072369, -0.01763399,  0.07622418, -0.02023587,\n",
       "       -0.10408194, -0.00025746,  0.01931514, -0.05929755,  0.01778987,\n",
       "        0.0895633 ,  0.0544152 ,  0.05172607, -0.05341789,  0.05519091,\n",
       "       -0.05702354, -0.01117283,  0.02778181,  0.01508191, -0.06341685,\n",
       "        0.09658783, -0.03476693, -0.0312394 , -0.05040399, -0.10132077,\n",
       "       -0.06728273, -0.04609793,  0.08020507, -0.01830772,  0.04557818,\n",
       "        0.02261424, -0.0344439 , -0.03533129,  0.02487174, -0.05888785,\n",
       "        0.04016107, -0.05152484,  0.13428284, -0.02645024,  0.01141511,\n",
       "        0.00550641,  0.03174864, -0.02274797,  0.02637715,  0.03341655,\n",
       "        0.0161781 ,  0.1755378 ,  0.06534956, -0.09765124, -0.08280792,\n",
       "        0.10407055, -0.01920002, -0.11565746,  0.00872982,  0.0081294 ,\n",
       "       -0.03885027,  0.00660436, -0.04405238,  0.01665265, -0.04954473,\n",
       "       -0.00736535, -0.06511164,  0.01735022, -0.04482542,  0.04522494,\n",
       "        0.0462653 ,  0.06484409, -0.00645228,  0.01204239, -0.05105898,\n",
       "       -0.00416989,  0.10691442,  0.09714053, -0.07294137,  0.04256179,\n",
       "       -0.01305964,  0.07192144,  0.08739352,  0.03317479, -0.02205103,\n",
       "        0.00505555, -0.04060845, -0.01940287, -0.07507485, -0.04323168,\n",
       "       -0.07096013,  0.07771724, -0.16310813,  0.10631341, -0.07211865,\n",
       "       -0.10556349,  0.07856053,  0.0143789 , -0.04854641,  0.06395889,\n",
       "        0.06466649, -0.08029193, -0.10401589,  0.04424488, -0.06939929,\n",
       "       -0.04323299,  0.00717427,  0.02556253, -0.01802251, -0.09271941,\n",
       "       -0.06283835, -0.13922068, -0.1238972 ,  0.03148718,  0.00660945,\n",
       "        0.06672706, -0.05620045, -0.07132495, -0.02280171,  0.01411034,\n",
       "        0.07871521,  0.05947236, -0.02090888, -0.02440393, -0.04331876,\n",
       "       -0.04822361,  0.05839046, -0.0851039 ,  0.04248895,  0.04011114,\n",
       "       -0.06425288, -0.01499498, -0.03135968, -0.04834793,  0.00736427,\n",
       "       -0.0466897 ,  0.06429645, -0.07257446, -0.03643911, -0.05117283,\n",
       "        0.00980146, -0.05709855, -0.04451871,  0.05762404, -0.08021232,\n",
       "        0.03530934, -0.10138479,  0.07975583,  0.12507397, -0.12985611,\n",
       "        0.01326462,  0.05581178,  0.00593579,  0.07907721, -0.00101018,\n",
       "        0.07430117,  0.05635588, -0.10943062,  0.1110448 ,  0.11370493,\n",
       "        0.02063491, -0.03653767,  0.03589424,  0.05839916, -0.03986126,\n",
       "        0.0676474 ,  0.08360238, -0.13628602, -0.0147323 ,  0.08627751,\n",
       "        0.06749982,  0.0378057 ,  0.02070681,  0.07559314,  0.1425963 ,\n",
       "       -0.01245932, -0.02182029, -0.01544264, -0.15234019, -0.09512551,\n",
       "       -0.01256717, -0.08068318, -0.03171637,  0.01737116,  0.12041898,\n",
       "        0.08817781,  0.01513645,  0.04019929, -0.04782558,  0.017235  ,\n",
       "       -0.05093199,  0.01400808, -0.02553047,  0.08268709,  0.00111132,\n",
       "       -0.0714687 ,  0.01123042,  0.01692259,  0.03551181, -0.05132157,\n",
       "       -0.08991324, -0.080499  ,  0.07402078, -0.01387823, -0.03774115,\n",
       "        0.01629521,  0.06883538, -0.0166228 , -0.00981619, -0.08586415,\n",
       "       -0.00180066,  0.0028883 , -0.01552448,  0.06339417, -0.06049567,\n",
       "       -0.12591055, -0.0772297 ,  0.02190904, -0.00920587, -0.05773886,\n",
       "       -0.01773267,  0.05246793,  0.00412819, -0.0624607 ,  0.05135524,\n",
       "       -0.02907965, -0.0186481 ,  0.1311352 ,  0.09438346,  0.04673885],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.infer_vector(text_cleaned[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec each word and add all word vectors within the same text to make up the text vector(remember to normalize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "naive_doc2vec = np.zeros((len(text_cleaned),300))\n",
    "idx = -1\n",
    "for text in text_cleaned:\n",
    "    idx = idx + 1\n",
    "    num_words = len(text)\n",
    "    for word in text:\n",
    "        word_vector = word2vec[word]\n",
    "        naive_doc2vec[idx,:] = naive_doc2vec[idx,:] + word_vector\n",
    "    naive_doc2vec[idx,:] = naive_doc2vec[idx,:] / num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.27868995 -0.01163037  0.02616349 -0.11340432  0.17190187 -0.11143054\n",
      " -0.20620216 -0.44378885  0.12310633  0.23665084 -0.18205675  0.12810443\n",
      " -0.30301312  0.05627629  0.00477885  0.0982552   0.30097052 -0.03005744\n",
      "  0.14404079  0.63472626 -0.15718583  0.12605314  0.28221462  0.05259475\n",
      "  0.34176543  0.04152222 -0.24683218 -0.48777249  0.75362668  0.22371268\n",
      " -0.12088574  0.48317955  0.33968828  0.08922517  0.24257583  0.15982012\n",
      "  0.21012554 -0.43060528 -0.09351016  0.43406987 -0.22121981  0.27096202\n",
      " -0.11485293  0.014093   -0.22046518  0.00972378  0.07254423 -0.46137426\n",
      " -0.266652   -0.03980379  0.4158588   0.07519136 -0.30022692  0.20392187\n",
      "  0.11854771 -0.35231757 -0.89835919 -0.09728083  0.22352538 -0.25093949\n",
      " -0.25188945  0.39162992  0.23554688  0.39454835 -0.04212488 -0.40168226\n",
      " -0.11605915 -0.31589135  0.10637169 -0.62099302 -0.27728408  0.06855654\n",
      "  0.00346043 -0.20456885 -0.28070957  0.32892583  0.12659171 -0.24138619\n",
      " -0.25910763  0.42475589 -0.59049432  0.24278067  0.04789417 -0.00873153\n",
      " -0.19642469 -0.1903894  -0.09546985  0.5501897  -0.44908691  0.24520657\n",
      " -0.1143024  -0.04964227  0.02209153 -0.27530868 -0.32154646 -0.03706793\n",
      " -0.05763515  0.02970959 -0.01709986  0.31553365  0.46983848 -0.6885373\n",
      "  0.57694863 -0.28705882  0.24761172  0.14611982  0.47202453 -0.21725984\n",
      "  0.16878521  0.42445142 -0.42065814 -0.0235413   0.30857791 -0.51016019\n",
      "  0.25201243  0.32874501  0.18279949 -0.52710901 -0.61850551 -0.01816728\n",
      "  0.42541899  0.3701295   0.19989096 -0.12581116 -0.16235969  0.0017552\n",
      "  0.32309168  0.3215735  -0.08770387 -0.51794389  0.27040666  0.49149593\n",
      " -0.50524967 -0.36080657 -0.16787056  0.37389822 -0.71106203  0.22881473\n",
      " -0.19587074  0.40955241 -0.16103954  0.16066966  0.04408125  0.29734888\n",
      " -0.12430897  0.13713176  0.18194356  0.62271941 -0.43255448 -0.21930614\n",
      " -0.65944734  0.1534967   0.18261489  0.4496011  -0.37537435 -0.33916526\n",
      "  0.5219582  -0.11142145 -0.46338087 -0.63378011  0.103387    0.18541493\n",
      " -0.97406523  0.67422504 -0.03626155  0.02146474 -0.34033004  0.06979538\n",
      " -0.28192579 -0.15295897  0.08842286 -0.39951242 -0.11474081  0.08244167\n",
      " -0.29443784  0.58125628 -0.32074953 -0.22292223  0.58781688  0.14063124\n",
      " -0.19921102  0.45489281 -0.0146688   0.60551904 -0.35880833  0.69308697\n",
      " -0.63711162  0.24352169 -0.76825772 -0.25753829  0.41516724  0.52818369\n",
      " -0.27143621  0.18685717 -0.04064342  0.01024421  0.22523444 -0.01954002\n",
      " -0.38688156 -0.23166158  0.11511662 -0.28294201  0.50049211  0.37016489\n",
      "  0.1292059   0.18271346  0.61132842  0.07261253 -0.24614917 -0.01176122\n",
      "  0.49240626  0.72618904  0.00744461  0.1877018  -0.12344555 -0.67986215\n",
      " -0.38312439 -0.24661965  0.15614403  0.39955661 -0.24820709 -0.0946132\n",
      " -0.55108497 -0.32496009  0.248933    0.16926376  0.34857256 -0.22918062\n",
      "  0.32987821  0.46185598 -0.29780835  0.33530146 -0.1130877  -0.29073844\n",
      " -0.37641603  0.15923172  0.36470433  0.26630634  0.64410774  0.01684698\n",
      "  0.04568621  0.28976291 -0.00609725  0.1411977   0.42429135  0.29837939\n",
      " -0.44930891 -0.40809318 -0.4565602  -0.71425006  0.31495306 -0.55320534\n",
      "  0.3537719   0.05894523  0.34921162  0.17834077 -0.13487542 -0.08821344\n",
      " -0.16507212 -0.45774038  0.0907358  -0.32144736  0.17129217  0.09669239\n",
      "  0.31319925 -0.24729357  0.14535805 -0.02675804 -0.12606092 -0.02583958\n",
      " -0.0490315   0.2085613   0.18118548 -0.5120675   0.04807901 -0.26919419\n",
      " -0.2107811   0.06641979  0.2499523  -0.42534248  0.31659931 -0.30802624\n",
      "  0.96243566  0.21451455 -0.38751483 -0.30110863  0.00214207 -0.65800033\n",
      " -0.12522311  0.31876855  0.23221039 -0.18682326 -0.82766341 -0.34741129\n",
      "  0.22792496  0.37337948 -0.78194628  0.43014929  0.49747779 -0.01685012]\n"
     ]
    }
   ],
   "source": [
    "print(naive_doc2vec[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put Doc2Vec model into the classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim \"Doc2Vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12993325  0.01495038 -0.01516713  0.00803873 -0.03303552  0.00829881\n",
      " -0.056199   -0.02831089  0.0205844   0.08184966 -0.03804151  0.0425854\n",
      " -0.04435135 -0.09430112 -0.06735714  0.00923983 -0.0673715   0.08545925\n",
      " -0.00976392  0.09746398  0.03904494 -0.01941598  0.02679177  0.09253461\n",
      " -0.03168099 -0.1321025   0.1341363  -0.073345    0.0878863   0.09139118\n",
      " -0.00156319  0.00442715 -0.01662348  0.08107968 -0.00208479  0.00113985\n",
      "  0.06923959 -0.03663416 -0.1182862   0.08386566 -0.00256242 -0.04221505\n",
      " -0.01267311 -0.04487883  0.00712917  0.02610446  0.06214851 -0.03210143\n",
      " -0.08627249 -0.05941227  0.10935897 -0.0680353  -0.04132422 -0.00228863\n",
      " -0.00982817  0.02722536 -0.03139601 -0.02445724  0.01061779 -0.07104012\n",
      " -0.05112461 -0.02387296  0.00058168  0.02013861 -0.03728822  0.01044006\n",
      " -0.02754978 -0.05414862 -0.00578358 -0.16648366  0.06323683  0.0053912\n",
      "  0.00907841 -0.08694317 -0.0680019  -0.03028907 -0.01701905 -0.00586718\n",
      "  0.02852613  0.01246372 -0.10654023 -0.01166201 -0.01662655 -0.04286269\n",
      " -0.02248656  0.11730495  0.0295157   0.05155858 -0.00892338  0.05966768\n",
      " -0.04522463 -0.00443726  0.02682627  0.02229494 -0.04067271  0.07176352\n",
      " -0.04587387 -0.04934872 -0.07152286 -0.04287967 -0.04568147 -0.06427031\n",
      "  0.05943715 -0.01322215  0.03357679 -0.00387129  0.0085612  -0.03620604\n",
      "  0.00650012 -0.07122957  0.0505448  -0.07424565  0.1223399  -0.0170198\n",
      " -0.01110953  0.02208258  0.01535452  0.00085347  0.00924587  0.00974071\n",
      " -0.01859828  0.16756856  0.06940965 -0.05905144 -0.04822502  0.16492391\n",
      " -0.00944876 -0.06152632  0.04130059  0.02384782 -0.03937784  0.00394487\n",
      " -0.00852181 -0.00402734 -0.03667409 -0.03531474 -0.05484041  0.03271187\n",
      " -0.04688478  0.03838882  0.03472443  0.05440231  0.04147451 -0.01862943\n",
      "  0.00595067 -0.03986985  0.05654511  0.07106246 -0.09309316  0.06930636\n",
      " -0.0091589   0.08571556  0.07912555  0.02594326 -0.07438871  0.00589654\n",
      " -0.04284369  0.01417585 -0.07998355  0.01039691 -0.05586803  0.08638655\n",
      " -0.11758745  0.11617509 -0.06800089 -0.07097735  0.07282067 -0.00359285\n",
      " -0.04270261  0.03788416  0.04927278 -0.10437656 -0.07894819  0.03742585\n",
      " -0.0440721  -0.02544419  0.018474    0.03837873  0.00872514 -0.04305041\n",
      " -0.0670532  -0.14118327 -0.10259085  0.02004245 -0.01209551  0.08842082\n",
      " -0.07242979 -0.04844994  0.00415424  0.0549027   0.06886162 -0.00833511\n",
      " -0.00823648  0.01987359 -0.01611683 -0.06246096 -0.0097275  -0.0599394\n",
      "  0.05753815  0.06897593 -0.04880127 -0.00182208 -0.01940395 -0.04163738\n",
      "  0.03326692 -0.03792439  0.05680836 -0.03272315 -0.03899795 -0.0003642\n",
      " -0.01781209 -0.04741019 -0.06041489  0.05775271 -0.09948185  0.03057046\n",
      " -0.04471234  0.0837556   0.11041487 -0.1178038   0.02254856  0.13713729\n",
      "  0.05816583  0.0827093   0.02432003  0.03976856  0.1168948  -0.13556588\n",
      "  0.12103318  0.06161068  0.01375131 -0.0361321   0.06490708  0.04623303\n",
      " -0.03224637  0.08242022  0.07929202 -0.10099375 -0.00994749  0.06008989\n",
      "  0.02586725  0.07064736 -0.01070824  0.06668152  0.09404714  0.03857007\n",
      "  0.00962784 -0.01323052 -0.20863552 -0.06905574  0.05930503 -0.0683156\n",
      "  0.01750089  0.01837259  0.12578921  0.10536654 -0.00748224  0.01348846\n",
      " -0.02744252 -0.00349736 -0.04750284  0.00934844  0.01311235  0.10540945\n",
      " -0.00022822 -0.08236171 -0.01029918 -0.00180032 -0.01567671 -0.05519581\n",
      " -0.03596389 -0.07906345  0.06970116 -0.019669   -0.00362757  0.02136236\n",
      "  0.06297202 -0.06800617 -0.00651675 -0.11532964 -0.03118746  0.01509968\n",
      "  0.02484281  0.08623743 -0.01572838 -0.05621484 -0.04978393  0.03085498\n",
      " -0.04865189 -0.04784971 -0.00193616  0.0696127   0.04713035 -0.02827842\n",
      "  0.06962933 -0.01552188 -0.01668527  0.1065073   0.0371105   0.05937093]\n"
     ]
    }
   ],
   "source": [
    "doc2vec_data = np.zeros((len(text_cleaned),300))\n",
    "for i in range (len(text_cleaned)):\n",
    "    doc2vec_data[i] = doc2vec.infer_vector(text_cleaned[i])\n",
    "\n",
    "print(doc2vec_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50357, 300)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train and test sets\n",
    "seed = 42\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc2vec_data, label, test_size=test_size, random_state=seed)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 1 1 1 1 1]\n",
      "[[ 8639 12131]\n",
      " [ 2974 26613]]\n",
      "accuracy: 0.7000417022459638\n",
      "precision: 0.6868934544703696\n",
      "recall: 0.8994828809950316\n",
      "f1score: 0.7789436712473109\n"
     ]
    }
   ],
   "source": [
    "# instantiate a SVM regression model, and fit with X and y\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X_train, y_train.astype(int))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(y_pred[0:10])\n",
    "# check the accuracy on the training set\n",
    "print(confusion_matrix(y_true=y_test.astype(int), y_pred=y_pred))\n",
    "score = model.score(X_test, y_test.astype(int))\n",
    "\n",
    "p = precision_score(y_test.astype(int), y_pred, average='binary')\n",
    "r = recall_score(y_test.astype(int), y_pred, average='binary')\n",
    "f1score = f1_score(y_test.astype(int), y_pred, average='binary')\n",
    "print('accuracy:',score)\n",
    "print('precision:',p)\n",
    "print('recall:',r)\n",
    "print('f1score:',f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Doce2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50357, 300)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train and test sets\n",
    "seed = 42\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(naive_doc2vec, label, test_size=test_size, random_state=seed)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 0 1 1 1 1]\n",
      "[[ 9054 11716]\n",
      " [ 4390 25197]]\n",
      "accuracy: 0.680163631669877\n",
      "precision: 0.6826050442933383\n",
      "recall: 0.851624024064623\n",
      "f1score: 0.7578045112781955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X_train, y_train.astype(int))\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(y_pred[0:10])\n",
    "# check the accuracy on the training set\n",
    "print(confusion_matrix(y_true=y_test.astype(int), y_pred=y_pred))\n",
    "score = model.score(X_test, y_test.astype(int))\n",
    "\n",
    "p = precision_score(y_test.astype(int), y_pred, average='binary')\n",
    "r = recall_score(y_test.astype(int), y_pred, average='binary')\n",
    "f1score = f1_score(y_test.astype(int), y_pred, average='binary')\n",
    "print('accuracy:',score)\n",
    "print('precision:',p)\n",
    "print('recall:',r)\n",
    "print('f1score:',f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Final results show that the performance of Gensim Doc2Vec model is better than Naive Doc2Vec model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
