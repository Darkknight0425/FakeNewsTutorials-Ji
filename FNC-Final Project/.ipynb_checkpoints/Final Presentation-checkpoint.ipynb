{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Presentation\n",
    "\n",
    "\n",
    "## Members: Haorui Ji, Mengze Zhang, Weihao Tan\n",
    "\n",
    "\n",
    "# Abstract\n",
    "\n",
    "* Combine title and text corpus together to retrieve more information.\n",
    "* Concatenate vectors made from w2v and d2v methods (tf-idf: optional) as final feature vectors(600d).\n",
    "* Using an Ensemble classifier(composed of three tuned estimators) to train and make predictions on the feature vectors we get.\n",
    "* Reach an over 0.94 f-1 score in test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "\n",
    "First, we need to import the data. Since we plan to test if using the combination of title vectors and text vectors would yield a better result, we import title corpus and text corpus seperately.In addition, since we consider combining title and text corpus would give us more useful information, an additional \"titleandtext\" corpus is generated by concatenating title and text in every review into one list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>title_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[ 1.1533764e-02  4.2144405e-03  1.9692603e-02 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[ 0.11267698  0.02518966 -0.00212591  0.021095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[ 0.04253004  0.04300297  0.01848392  0.048672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[ 0.10801624  0.11583211  0.02874823  0.061732...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[ 1.69016439e-02  7.13498285e-03 -7.81233795e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                       title_vectors  \n",
       "0  [ 1.1533764e-02  4.2144405e-03  1.9692603e-02 ...  \n",
       "1  [ 0.11267698  0.02518966 -0.00212591  0.021095...  \n",
       "2  [ 0.04253004  0.04300297  0.01848392  0.048672...  \n",
       "3  [ 0.10801624  0.11583211  0.02874823  0.061732...  \n",
       "4  [ 1.69016439e-02  7.13498285e-03 -7.81233795e-...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# import all the data and split all strings to words in it\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from itertools import chain\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report as cr\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from numpy import linspace\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "df=pd.read_csv('../data/fake_or_real_news.csv')\n",
    "title_raw=np.asarray(df.title)\n",
    "text_raw=np.asarray(df.text)\n",
    "y_raw=np.asarray(df.label)\n",
    "title_raw=[titles.lower().split() for titles in title_raw]\n",
    "text_raw=[texts.lower().split() for texts in text_raw]\n",
    "# Haorui Ji 2018-9-24\n",
    "# Combine title and text in every review into one list \n",
    "titleandtext_raw = copy.deepcopy(title_raw)\n",
    "for i in range(len(titleandtext_raw)):\n",
    "    titleandtext_raw[i].extend(text_raw[i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first title : \n",
      " ['you', 'can', 'smell', 'hillary’s', 'fear']\n",
      "first text : \n",
      "  ['daniel', 'greenfield,', 'a', 'shillman', 'journalism', 'fellow', 'at', 'the', 'freedom', 'center,', 'is', 'a', 'new', 'york', 'writer', 'focusing', 'on', 'radical', 'islam.', 'in', 'the', 'final', 'stretch', 'of', 'the', 'election,', 'hillary', 'rodham', 'clinton', 'has', 'gone', 'to', 'war', 'with', 'the', 'fbi.', 'the', 'word', '“unprecedented”', 'has', 'been', 'thrown', 'around', 'so', 'often', 'this', 'election', 'that', 'it', 'ought', 'to', 'be', 'retired.', 'but', 'it’s', 'still', 'unprecedented', 'for', 'the', 'nominee', 'of', 'a', 'major', 'political', 'party', 'to', 'go', 'war', 'with', 'the', 'fbi.', 'but', 'that’s', 'exactly', 'what', 'hillary', 'and', 'her', 'people', 'have', 'done.', 'coma', 'patients', 'just', 'waking', 'up', 'now', 'and', 'watching', 'an', 'hour', 'of', 'cnn', 'from', 'their', 'hospital', 'beds', 'would', 'assume', 'that', 'fbi', 'director', 'james', 'comey', 'is', 'hillary’s', 'opponent', 'in', 'this', 'election.', 'the', 'fbi', 'is', 'under', 'attack', 'by', 'everyone', 'from', 'obama', 'to', 'cnn.', 'hillary’s', 'people', 'have', 'circulated', 'a', 'letter', 'attacking', 'comey.', 'there', 'are', 'currently', 'more', 'media', 'hit', 'pieces', 'lambasting', 'him', 'than', 'targeting', 'trump.', 'it', 'wouldn’t', 'be', 'too', 'surprising', 'if', 'the', 'clintons', 'or', 'their', 'allies', 'were', 'to', 'start', 'running', 'attack', 'ads', 'against', 'the', 'fbi.', 'the', 'fbi’s', 'leadership', 'is', 'being', 'warned', 'that', 'the', 'entire', 'left-wing', 'establishment', 'will', 'form', 'a', 'lynch', 'mob', 'if', 'they', 'continue', 'going', 'after', 'hillary.', 'and', 'the', 'fbi’s', 'credibility', 'is', 'being', 'attacked', 'by', 'the', 'media', 'and', 'the', 'democrats', 'to', 'preemptively', 'head', 'off', 'the', 'results', 'of', 'the', 'investigation', 'of', 'the', 'clinton', 'foundation', 'and', 'hillary', 'clinton.', 'the', 'covert', 'struggle', 'between', 'fbi', 'agents', 'and', 'obama’s', 'doj', 'people', 'has', 'gone', 'explosively', 'public.', 'the', 'new', 'york', 'times', 'has', 'compared', 'comey', 'to', 'j.', 'edgar', 'hoover.', 'its', 'bizarre', 'headline,', '“james', 'comey', 'role', 'recalls', 'hoover’s', 'fbi,', 'fairly', 'or', 'not”', 'practically', 'admits', 'up', 'front', 'that', 'it’s', 'spouting', 'nonsense.', 'the', 'boston', 'globe', 'has', 'published', 'a', 'column', 'calling', 'for', 'comey’s', 'resignation.', 'not', 'to', 'be', 'outdone,', 'time', 'has', 'an', 'editorial', 'claiming', 'that', 'the', 'scandal', 'is', 'really', 'an', 'attack', 'on', 'all', 'women.', 'james', 'carville', 'appeared', 'on', 'msnbc', 'to', 'remind', 'everyone', 'that', 'he', 'was', 'still', 'alive', 'and', 'insane.', 'he', 'accused', 'comey', 'of', 'coordinating', 'with', 'house', 'republicans', 'and', 'the', 'kgb.', 'and', 'you', 'thought', 'the', '“vast', 'right', 'wing', 'conspiracy”', 'was', 'a', 'stretch.', 'countless', 'media', 'stories', 'charge', 'comey', 'with', 'violating', 'procedure.', 'do', 'you', 'know', 'what’s', 'a', 'procedural', 'violation?', 'emailing', 'classified', 'information', 'stored', 'on', 'your', 'bathroom', 'server.', 'senator', 'harry', 'reid', 'has', 'sent', 'comey', 'a', 'letter', 'accusing', 'him', 'of', 'violating', 'the', 'hatch', 'act.', 'the', 'hatch', 'act', 'is', 'a', 'nice', 'idea', 'that', 'has', 'as', 'much', 'relevance', 'in', 'the', 'age', 'of', 'obama', 'as', 'the', 'tenth', 'amendment.', 'but', 'the', 'cable', 'news', 'spectrum', 'quickly', 'filled', 'with', 'media', 'hacks', 'glancing', 'at', 'the', 'wikipedia', 'article', 'on', 'the', 'hatch', 'act', 'under', 'the', 'table', 'while', 'accusing', 'the', 'fbi', 'director', 'of', 'one', 'of', 'the', 'most', 'awkward', 'conspiracies', 'against', 'hillary', 'ever.', 'if', 'james', 'comey', 'is', 'really', 'out', 'to', 'hurt', 'hillary,', 'he', 'picked', 'one', 'hell', 'of', 'a', 'strange', 'way', 'to', 'do', 'it.', 'not', 'too', 'long', 'ago', 'democrats', 'were', 'breathing', 'a', 'sigh', 'of', 'relief', 'when', 'he', 'gave', 'hillary', 'clinton', 'a', 'pass', 'in', 'a', 'prominent', 'public', 'statement.', 'if', 'he', 'really', 'were', 'out', 'to', 'elect', 'trump', 'by', 'keeping', 'the', 'email', 'scandal', 'going,', 'why', 'did', 'he', 'trash', 'the', 'investigation?', 'was', 'he', 'on', 'the', 'payroll', 'of', 'house', 'republicans', 'and', 'the', 'kgb', 'back', 'then', 'and', 'playing', 'it', 'coy', 'or', 'was', 'it', 'a', 'sudden', 'development', 'where', 'vladimir', 'putin', 'and', 'paul', 'ryan', 'talked', 'him', 'into', 'taking', 'a', 'look', 'at', 'anthony', 'weiner’s', 'computer?', 'either', 'comey', 'is', 'the', 'most', 'cunning', 'fbi', 'director', 'that', 'ever', 'lived', 'or', 'he’s', 'just', 'awkwardly', 'trying', 'to', 'navigate', 'a', 'political', 'mess', 'that', 'has', 'trapped', 'him', 'between', 'a', 'doj', 'leadership', 'whose', 'political', 'futures', 'are', 'tied', 'to', 'hillary’s', 'victory', 'and', 'his', 'own', 'bureau', 'whose', 'apolitical', 'agents', 'just', 'want', 'to', 'be', 'allowed', 'to', 'do', 'their', 'jobs.', 'the', 'only', 'truly', 'mysterious', 'thing', 'is', 'why', 'hillary', 'and', 'her', 'associates', 'decided', 'to', 'go', 'to', 'war', 'with', 'a', 'respected', 'federal', 'agency.', 'most', 'americans', 'like', 'the', 'fbi', 'while', 'hillary', 'clinton', 'enjoys', 'a', '60%', 'unfavorable', 'rating.', 'and', 'it’s', 'an', 'interesting', 'question.', 'hillary’s', 'old', 'strategy', 'was', 'to', 'lie', 'and', 'deny', 'that', 'the', 'fbi', 'even', 'had', 'a', 'criminal', 'investigation', 'underway.', 'instead', 'her', 'associates', 'insisted', 'that', 'it', 'was', 'a', 'security', 'review.', 'the', 'fbi', 'corrected', 'her', 'and', 'she', 'shrugged', 'it', 'off.', 'but', 'the', 'old', 'breezy', 'denial', 'approach', 'has', 'given', 'way', 'to', 'a', 'savage', 'assault', 'on', 'the', 'fbi.', 'pretending', 'that', 'nothing', 'was', 'wrong', 'was', 'a', 'bad', 'strategy,', 'but', 'it', 'was', 'a', 'better', 'one', 'that', 'picking', 'a', 'fight', 'with', 'the', 'fbi', 'while', 'lunatic', 'clinton', 'associates', 'try', 'to', 'claim', 'that', 'the', 'fbi', 'is', 'really', 'the', 'kgb.', 'there', 'are', 'two', 'possible', 'explanations.', 'hillary', 'clinton', 'might', 'be', 'arrogant', 'enough', 'to', 'lash', 'out', 'at', 'the', 'fbi', 'now', 'that', 'she', 'believes', 'that', 'victory', 'is', 'near.', 'the', 'same', 'kind', 'of', 'hubris', 'that', 'led', 'her', 'to', 'plan', 'her', 'victory', 'fireworks', 'display', 'could', 'lead', 'her', 'to', 'declare', 'a', 'war', 'on', 'the', 'fbi', 'for', 'irritating', 'her', 'during', 'the', 'final', 'miles', 'of', 'her', 'campaign.', 'but', 'the', 'other', 'explanation', 'is', 'that', 'her', 'people', 'panicked.', 'going', 'to', 'war', 'with', 'the', 'fbi', 'is', 'not', 'the', 'behavior', 'of', 'a', 'smart', 'and', 'focused', 'presidential', 'campaign.', 'it’s', 'an', 'act', 'of', 'desperation.', 'when', 'a', 'presidential', 'candidate', 'decides', 'that', 'her', 'only', 'option', 'is', 'to', 'try', 'and', 'destroy', 'the', 'credibility', 'of', 'the', 'fbi,', 'that’s', 'not', 'hubris,', 'it’s', 'fear', 'of', 'what', 'the', 'fbi', 'might', 'be', 'about', 'to', 'reveal', 'about', 'her.', 'during', 'the', 'original', 'fbi', 'investigation,', 'hillary', 'clinton', 'was', 'confident', 'that', 'she', 'could', 'ride', 'it', 'out.', 'and', 'she', 'had', 'good', 'reason', 'for', 'believing', 'that.', 'but', 'that', 'hillary', 'clinton', 'is', 'gone.', 'in', 'her', 'place', 'is', 'a', 'paranoid', 'wreck.', 'within', 'a', 'short', 'space', 'of', 'time', 'the', '“positive”', 'clinton', 'campaign', 'promising', 'to', 'unite', 'the', 'country', 'has', 'been', 'replaced', 'by', 'a', 'desperate', 'and', 'flailing', 'operation', 'that', 'has', 'focused', 'all', 'its', 'energy', 'on', 'fighting', 'the', 'fbi.', 'there’s', 'only', 'one', 'reason', 'for', 'such', 'bizarre', 'behavior.', 'the', 'clinton', 'campaign', 'has', 'decided', 'that', 'an', 'fbi', 'investigation', 'of', 'the', 'latest', 'batch', 'of', 'emails', 'poses', 'a', 'threat', 'to', 'its', 'survival.', 'and', 'so', 'it’s', 'gone', 'all', 'in', 'on', 'fighting', 'the', 'fbi.', 'it’s', 'an', 'unprecedented', 'step', 'born', 'of', 'fear.', 'it’s', 'hard', 'to', 'know', 'whether', 'that', 'fear', 'is', 'justified.', 'but', 'the', 'existence', 'of', 'that', 'fear', 'already', 'tells', 'us', 'a', 'whole', 'lot.', 'clinton', 'loyalists', 'rigged', 'the', 'old', 'investigation.', 'they', 'knew', 'the', 'outcome', 'ahead', 'of', 'time', 'as', 'well', 'as', 'they', 'knew', 'the', 'debate', 'questions.', 'now', 'suddenly', 'they', 'are', 'no', 'longer', 'in', 'control.', 'and', 'they', 'are', 'afraid.', 'you', 'can', 'smell', 'the', 'fear.', 'the', 'fbi', 'has', 'wiretaps', 'from', 'the', 'investigation', 'of', 'the', 'clinton', 'foundation.', 'it’s', 'finding', 'new', 'emails', 'all', 'the', 'time.', 'and', 'clintonworld', 'panicked.', 'the', 'spinmeisters', 'of', 'clintonworld', 'have', 'claimed', 'that', 'the', 'email', 'scandal', 'is', 'just', 'so', 'much', 'smoke', 'without', 'fire.', 'all', 'that’s', 'here', 'is', 'the', 'appearance', 'of', 'impropriety', 'without', 'any', 'of', 'the', 'substance.', 'but', 'this', 'isn’t', 'how', 'you', 'react', 'to', 'smoke.', 'it’s', 'how', 'you', 'respond', 'to', 'a', 'fire.', 'the', 'misguided', 'assault', 'on', 'the', 'fbi', 'tells', 'us', 'that', 'hillary', 'clinton', 'and', 'her', 'allies', 'are', 'afraid', 'of', 'a', 'revelation', 'bigger', 'than', 'the', 'fundamental', 'illegality', 'of', 'her', 'email', 'setup.', 'the', 'email', 'setup', 'was', 'a', 'preemptive', 'cover', 'up.', 'the', 'clinton', 'campaign', 'has', 'panicked', 'badly', 'out', 'of', 'the', 'belief,', 'right', 'or', 'wrong,', 'that', 'whatever', 'crime', 'the', 'illegal', 'setup', 'was', 'meant', 'to', 'cover', 'up', 'is', 'at', 'risk', 'of', 'being', 'exposed.', 'the', 'clintons', 'have', 'weathered', 'countless', 'scandals', 'over', 'the', 'years.', 'whatever', 'they', 'are', 'protecting', 'this', 'time', 'around', 'is', 'bigger', 'than', 'the', 'usual', 'corruption,', 'bribery,', 'sexual', 'assaults', 'and', 'abuses', 'of', 'power', 'that', 'have', 'followed', 'them', 'around', 'throughout', 'the', 'years.', 'this', 'is', 'bigger', 'and', 'more', 'damaging', 'than', 'any', 'of', 'the', 'allegations', 'that', 'have', 'already', 'come', 'out.', 'and', 'they', 'don’t', 'want', 'fbi', 'investigators', 'anywhere', 'near', 'it.', 'the', 'campaign', 'against', 'comey', 'is', 'pure', 'intimidation.', 'it’s', 'also', 'a', 'warning.', 'any', 'senior', 'fbi', 'people', 'who', 'value', 'their', 'careers', 'are', 'being', 'warned', 'to', 'stay', 'away.', 'the', 'democrats', 'are', 'closing', 'ranks', 'around', 'their', 'nominee', 'against', 'the', 'fbi.', 'it’s', 'an', 'ugly', 'and', 'unprecedented', 'scene.', 'it', 'may', 'also', 'be', 'their', 'last', 'stand.', 'hillary', 'clinton', 'has', 'awkwardly', 'wound', 'her', 'way', 'through', 'numerous', 'scandals', 'in', 'just', 'this', 'election', 'cycle.', 'but', 'she’s', 'never', 'shown', 'fear', 'or', 'desperation', 'before.', 'now', 'that', 'has', 'changed.', 'whatever', 'she', 'is', 'afraid', 'of,', 'it', 'lies', 'buried', 'in', 'her', 'emails', 'with', 'huma', 'abedin.', 'and', 'it', 'can', 'bring', 'her', 'down', 'like', 'nothing', 'else', 'has.']\n",
      "first titleandtext : \n",
      "  ['you', 'can', 'smell', 'hillary’s', 'fear', 'daniel', 'greenfield,', 'a', 'shillman', 'journalism', 'fellow', 'at', 'the', 'freedom', 'center,', 'is', 'a', 'new', 'york', 'writer', 'focusing', 'on', 'radical', 'islam.', 'in', 'the', 'final', 'stretch', 'of', 'the', 'election,', 'hillary', 'rodham', 'clinton', 'has', 'gone', 'to', 'war', 'with', 'the', 'fbi.', 'the', 'word', '“unprecedented”', 'has', 'been', 'thrown', 'around', 'so', 'often', 'this', 'election', 'that', 'it', 'ought', 'to', 'be', 'retired.', 'but', 'it’s', 'still', 'unprecedented', 'for', 'the', 'nominee', 'of', 'a', 'major', 'political', 'party', 'to', 'go', 'war', 'with', 'the', 'fbi.', 'but', 'that’s', 'exactly', 'what', 'hillary', 'and', 'her', 'people', 'have', 'done.', 'coma', 'patients', 'just', 'waking', 'up', 'now', 'and', 'watching', 'an', 'hour', 'of', 'cnn', 'from', 'their', 'hospital', 'beds', 'would', 'assume', 'that', 'fbi', 'director', 'james', 'comey', 'is', 'hillary’s', 'opponent', 'in', 'this', 'election.', 'the', 'fbi', 'is', 'under', 'attack', 'by', 'everyone', 'from', 'obama', 'to', 'cnn.', 'hillary’s', 'people', 'have', 'circulated', 'a', 'letter', 'attacking', 'comey.', 'there', 'are', 'currently', 'more', 'media', 'hit', 'pieces', 'lambasting', 'him', 'than', 'targeting', 'trump.', 'it', 'wouldn’t', 'be', 'too', 'surprising', 'if', 'the', 'clintons', 'or', 'their', 'allies', 'were', 'to', 'start', 'running', 'attack', 'ads', 'against', 'the', 'fbi.', 'the', 'fbi’s', 'leadership', 'is', 'being', 'warned', 'that', 'the', 'entire', 'left-wing', 'establishment', 'will', 'form', 'a', 'lynch', 'mob', 'if', 'they', 'continue', 'going', 'after', 'hillary.', 'and', 'the', 'fbi’s', 'credibility', 'is', 'being', 'attacked', 'by', 'the', 'media', 'and', 'the', 'democrats', 'to', 'preemptively', 'head', 'off', 'the', 'results', 'of', 'the', 'investigation', 'of', 'the', 'clinton', 'foundation', 'and', 'hillary', 'clinton.', 'the', 'covert', 'struggle', 'between', 'fbi', 'agents', 'and', 'obama’s', 'doj', 'people', 'has', 'gone', 'explosively', 'public.', 'the', 'new', 'york', 'times', 'has', 'compared', 'comey', 'to', 'j.', 'edgar', 'hoover.', 'its', 'bizarre', 'headline,', '“james', 'comey', 'role', 'recalls', 'hoover’s', 'fbi,', 'fairly', 'or', 'not”', 'practically', 'admits', 'up', 'front', 'that', 'it’s', 'spouting', 'nonsense.', 'the', 'boston', 'globe', 'has', 'published', 'a', 'column', 'calling', 'for', 'comey’s', 'resignation.', 'not', 'to', 'be', 'outdone,', 'time', 'has', 'an', 'editorial', 'claiming', 'that', 'the', 'scandal', 'is', 'really', 'an', 'attack', 'on', 'all', 'women.', 'james', 'carville', 'appeared', 'on', 'msnbc', 'to', 'remind', 'everyone', 'that', 'he', 'was', 'still', 'alive', 'and', 'insane.', 'he', 'accused', 'comey', 'of', 'coordinating', 'with', 'house', 'republicans', 'and', 'the', 'kgb.', 'and', 'you', 'thought', 'the', '“vast', 'right', 'wing', 'conspiracy”', 'was', 'a', 'stretch.', 'countless', 'media', 'stories', 'charge', 'comey', 'with', 'violating', 'procedure.', 'do', 'you', 'know', 'what’s', 'a', 'procedural', 'violation?', 'emailing', 'classified', 'information', 'stored', 'on', 'your', 'bathroom', 'server.', 'senator', 'harry', 'reid', 'has', 'sent', 'comey', 'a', 'letter', 'accusing', 'him', 'of', 'violating', 'the', 'hatch', 'act.', 'the', 'hatch', 'act', 'is', 'a', 'nice', 'idea', 'that', 'has', 'as', 'much', 'relevance', 'in', 'the', 'age', 'of', 'obama', 'as', 'the', 'tenth', 'amendment.', 'but', 'the', 'cable', 'news', 'spectrum', 'quickly', 'filled', 'with', 'media', 'hacks', 'glancing', 'at', 'the', 'wikipedia', 'article', 'on', 'the', 'hatch', 'act', 'under', 'the', 'table', 'while', 'accusing', 'the', 'fbi', 'director', 'of', 'one', 'of', 'the', 'most', 'awkward', 'conspiracies', 'against', 'hillary', 'ever.', 'if', 'james', 'comey', 'is', 'really', 'out', 'to', 'hurt', 'hillary,', 'he', 'picked', 'one', 'hell', 'of', 'a', 'strange', 'way', 'to', 'do', 'it.', 'not', 'too', 'long', 'ago', 'democrats', 'were', 'breathing', 'a', 'sigh', 'of', 'relief', 'when', 'he', 'gave', 'hillary', 'clinton', 'a', 'pass', 'in', 'a', 'prominent', 'public', 'statement.', 'if', 'he', 'really', 'were', 'out', 'to', 'elect', 'trump', 'by', 'keeping', 'the', 'email', 'scandal', 'going,', 'why', 'did', 'he', 'trash', 'the', 'investigation?', 'was', 'he', 'on', 'the', 'payroll', 'of', 'house', 'republicans', 'and', 'the', 'kgb', 'back', 'then', 'and', 'playing', 'it', 'coy', 'or', 'was', 'it', 'a', 'sudden', 'development', 'where', 'vladimir', 'putin', 'and', 'paul', 'ryan', 'talked', 'him', 'into', 'taking', 'a', 'look', 'at', 'anthony', 'weiner’s', 'computer?', 'either', 'comey', 'is', 'the', 'most', 'cunning', 'fbi', 'director', 'that', 'ever', 'lived', 'or', 'he’s', 'just', 'awkwardly', 'trying', 'to', 'navigate', 'a', 'political', 'mess', 'that', 'has', 'trapped', 'him', 'between', 'a', 'doj', 'leadership', 'whose', 'political', 'futures', 'are', 'tied', 'to', 'hillary’s', 'victory', 'and', 'his', 'own', 'bureau', 'whose', 'apolitical', 'agents', 'just', 'want', 'to', 'be', 'allowed', 'to', 'do', 'their', 'jobs.', 'the', 'only', 'truly', 'mysterious', 'thing', 'is', 'why', 'hillary', 'and', 'her', 'associates', 'decided', 'to', 'go', 'to', 'war', 'with', 'a', 'respected', 'federal', 'agency.', 'most', 'americans', 'like', 'the', 'fbi', 'while', 'hillary', 'clinton', 'enjoys', 'a', '60%', 'unfavorable', 'rating.', 'and', 'it’s', 'an', 'interesting', 'question.', 'hillary’s', 'old', 'strategy', 'was', 'to', 'lie', 'and', 'deny', 'that', 'the', 'fbi', 'even', 'had', 'a', 'criminal', 'investigation', 'underway.', 'instead', 'her', 'associates', 'insisted', 'that', 'it', 'was', 'a', 'security', 'review.', 'the', 'fbi', 'corrected', 'her', 'and', 'she', 'shrugged', 'it', 'off.', 'but', 'the', 'old', 'breezy', 'denial', 'approach', 'has', 'given', 'way', 'to', 'a', 'savage', 'assault', 'on', 'the', 'fbi.', 'pretending', 'that', 'nothing', 'was', 'wrong', 'was', 'a', 'bad', 'strategy,', 'but', 'it', 'was', 'a', 'better', 'one', 'that', 'picking', 'a', 'fight', 'with', 'the', 'fbi', 'while', 'lunatic', 'clinton', 'associates', 'try', 'to', 'claim', 'that', 'the', 'fbi', 'is', 'really', 'the', 'kgb.', 'there', 'are', 'two', 'possible', 'explanations.', 'hillary', 'clinton', 'might', 'be', 'arrogant', 'enough', 'to', 'lash', 'out', 'at', 'the', 'fbi', 'now', 'that', 'she', 'believes', 'that', 'victory', 'is', 'near.', 'the', 'same', 'kind', 'of', 'hubris', 'that', 'led', 'her', 'to', 'plan', 'her', 'victory', 'fireworks', 'display', 'could', 'lead', 'her', 'to', 'declare', 'a', 'war', 'on', 'the', 'fbi', 'for', 'irritating', 'her', 'during', 'the', 'final', 'miles', 'of', 'her', 'campaign.', 'but', 'the', 'other', 'explanation', 'is', 'that', 'her', 'people', 'panicked.', 'going', 'to', 'war', 'with', 'the', 'fbi', 'is', 'not', 'the', 'behavior', 'of', 'a', 'smart', 'and', 'focused', 'presidential', 'campaign.', 'it’s', 'an', 'act', 'of', 'desperation.', 'when', 'a', 'presidential', 'candidate', 'decides', 'that', 'her', 'only', 'option', 'is', 'to', 'try', 'and', 'destroy', 'the', 'credibility', 'of', 'the', 'fbi,', 'that’s', 'not', 'hubris,', 'it’s', 'fear', 'of', 'what', 'the', 'fbi', 'might', 'be', 'about', 'to', 'reveal', 'about', 'her.', 'during', 'the', 'original', 'fbi', 'investigation,', 'hillary', 'clinton', 'was', 'confident', 'that', 'she', 'could', 'ride', 'it', 'out.', 'and', 'she', 'had', 'good', 'reason', 'for', 'believing', 'that.', 'but', 'that', 'hillary', 'clinton', 'is', 'gone.', 'in', 'her', 'place', 'is', 'a', 'paranoid', 'wreck.', 'within', 'a', 'short', 'space', 'of', 'time', 'the', '“positive”', 'clinton', 'campaign', 'promising', 'to', 'unite', 'the', 'country', 'has', 'been', 'replaced', 'by', 'a', 'desperate', 'and', 'flailing', 'operation', 'that', 'has', 'focused', 'all', 'its', 'energy', 'on', 'fighting', 'the', 'fbi.', 'there’s', 'only', 'one', 'reason', 'for', 'such', 'bizarre', 'behavior.', 'the', 'clinton', 'campaign', 'has', 'decided', 'that', 'an', 'fbi', 'investigation', 'of', 'the', 'latest', 'batch', 'of', 'emails', 'poses', 'a', 'threat', 'to', 'its', 'survival.', 'and', 'so', 'it’s', 'gone', 'all', 'in', 'on', 'fighting', 'the', 'fbi.', 'it’s', 'an', 'unprecedented', 'step', 'born', 'of', 'fear.', 'it’s', 'hard', 'to', 'know', 'whether', 'that', 'fear', 'is', 'justified.', 'but', 'the', 'existence', 'of', 'that', 'fear', 'already', 'tells', 'us', 'a', 'whole', 'lot.', 'clinton', 'loyalists', 'rigged', 'the', 'old', 'investigation.', 'they', 'knew', 'the', 'outcome', 'ahead', 'of', 'time', 'as', 'well', 'as', 'they', 'knew', 'the', 'debate', 'questions.', 'now', 'suddenly', 'they', 'are', 'no', 'longer', 'in', 'control.', 'and', 'they', 'are', 'afraid.', 'you', 'can', 'smell', 'the', 'fear.', 'the', 'fbi', 'has', 'wiretaps', 'from', 'the', 'investigation', 'of', 'the', 'clinton', 'foundation.', 'it’s', 'finding', 'new', 'emails', 'all', 'the', 'time.', 'and', 'clintonworld', 'panicked.', 'the', 'spinmeisters', 'of', 'clintonworld', 'have', 'claimed', 'that', 'the', 'email', 'scandal', 'is', 'just', 'so', 'much', 'smoke', 'without', 'fire.', 'all', 'that’s', 'here', 'is', 'the', 'appearance', 'of', 'impropriety', 'without', 'any', 'of', 'the', 'substance.', 'but', 'this', 'isn’t', 'how', 'you', 'react', 'to', 'smoke.', 'it’s', 'how', 'you', 'respond', 'to', 'a', 'fire.', 'the', 'misguided', 'assault', 'on', 'the', 'fbi', 'tells', 'us', 'that', 'hillary', 'clinton', 'and', 'her', 'allies', 'are', 'afraid', 'of', 'a', 'revelation', 'bigger', 'than', 'the', 'fundamental', 'illegality', 'of', 'her', 'email', 'setup.', 'the', 'email', 'setup', 'was', 'a', 'preemptive', 'cover', 'up.', 'the', 'clinton', 'campaign', 'has', 'panicked', 'badly', 'out', 'of', 'the', 'belief,', 'right', 'or', 'wrong,', 'that', 'whatever', 'crime', 'the', 'illegal', 'setup', 'was', 'meant', 'to', 'cover', 'up', 'is', 'at', 'risk', 'of', 'being', 'exposed.', 'the', 'clintons', 'have', 'weathered', 'countless', 'scandals', 'over', 'the', 'years.', 'whatever', 'they', 'are', 'protecting', 'this', 'time', 'around', 'is', 'bigger', 'than', 'the', 'usual', 'corruption,', 'bribery,', 'sexual', 'assaults', 'and', 'abuses', 'of', 'power', 'that', 'have', 'followed', 'them', 'around', 'throughout', 'the', 'years.', 'this', 'is', 'bigger', 'and', 'more', 'damaging', 'than', 'any', 'of', 'the', 'allegations', 'that', 'have', 'already', 'come', 'out.', 'and', 'they', 'don’t', 'want', 'fbi', 'investigators', 'anywhere', 'near', 'it.', 'the', 'campaign', 'against', 'comey', 'is', 'pure', 'intimidation.', 'it’s', 'also', 'a', 'warning.', 'any', 'senior', 'fbi', 'people', 'who', 'value', 'their', 'careers', 'are', 'being', 'warned', 'to', 'stay', 'away.', 'the', 'democrats', 'are', 'closing', 'ranks', 'around', 'their', 'nominee', 'against', 'the', 'fbi.', 'it’s', 'an', 'ugly', 'and', 'unprecedented', 'scene.', 'it', 'may', 'also', 'be', 'their', 'last', 'stand.', 'hillary', 'clinton', 'has', 'awkwardly', 'wound', 'her', 'way', 'through', 'numerous', 'scandals', 'in', 'just', 'this', 'election', 'cycle.', 'but', 'she’s', 'never', 'shown', 'fear', 'or', 'desperation', 'before.', 'now', 'that', 'has', 'changed.', 'whatever', 'she', 'is', 'afraid', 'of,', 'it', 'lies', 'buried', 'in', 'her', 'emails', 'with', 'huma', 'abedin.', 'and', 'it', 'can', 'bring', 'her', 'down', 'like', 'nothing', 'else', 'has.']\n"
     ]
    }
   ],
   "source": [
    "print(\"first title : \\n\",title_raw[0])\n",
    "print(\"first text : \\n \",text_raw[0])\n",
    "print(\"first titleandtext : \\n \",titleandtext_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "After importing all the data we have, we preprocessed the dataset in order to filter unuseful noise and make our data more informative. In this process, we first removed all stopwords and punctuations, then lemmatize and stem the rest to generate our \"cleaned\" version of corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove noise data from raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# Haorui Ji 2018-9-24\n",
    "# Check how may texts are empty, they are the \"noise\" in the dataset and we shall remove them\n",
    "\n",
    "count = 0\n",
    "for text in text_raw:\n",
    "    if len(text) == 0:\n",
    "        count = count + 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6299\n",
      "6299\n",
      "6299\n",
      "6299\n"
     ]
    }
   ],
   "source": [
    "deleteindex = []\n",
    "idx = -1\n",
    "for text in text_raw:\n",
    "    idx += 1\n",
    "    num_words = len(text)\n",
    "    if num_words == 0:\n",
    "        deleteindex.append(idx)\n",
    "        \n",
    "y=np.delete(y_raw,deleteindex)\n",
    "title = np.delete(title_raw,deleteindex)\n",
    "text = np.delete(text_raw,deleteindex)\n",
    "titleandtext = np.delete(titleandtext_raw,deleteindex)\n",
    "\n",
    "print(len(title))\n",
    "print(len(text))\n",
    "print(len(titleandtext))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Haorui Ji 2018-9-24\n",
    "# Defining a preprocessing function that can lemmatize, stem and remove stopwords/punctuations from the corpus\n",
    "# preprocessing both title corpus, text corpus and titleandtext corpus\n",
    "\n",
    "import nltk\n",
    "from itertools import chain\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "english_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punct = punctuation\n",
    "\n",
    "def lemmatize_tokens(tokens, lemmatizer):\n",
    "    lemmatized = []\n",
    "    for doc in tokens:\n",
    "        lemmatized.append([lemmatizer.lemmatize(token) for token in doc])\n",
    "    return lemmatized\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for doc in tokens:\n",
    "        stemmed.append([stemmer.stem(token) for token in doc])\n",
    "    return stemmed\n",
    "\n",
    "def clean_text(tokenized_list,lemmatize=True,stem=True):\n",
    "    tokens = []\n",
    "    for doc in tokenized_list:\n",
    "        tokens.append([token for token in doc if token not in chain(punct, stopwords)])\n",
    "    tokens_cleaned = tokens\n",
    "    if lemmatize:\n",
    "        tokens_cleaned = lemmatize_tokens(tokens_cleaned, english_lemmatizer)\n",
    "    if stem:\n",
    "        tokens_cleaned = stem_tokens(tokens_cleaned, english_stemmer)\n",
    "      \n",
    "    return tokens_cleaned\n",
    "\n",
    "clean_titleandtext=clean_text(titleandtext,True,True)\n",
    "clean_title=clean_text(title,True,True)\n",
    "clean_text=clean_text(text,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first title cleaned : \n",
      " ['smell', 'hillari', 'fear']\n",
      "first text cleaned : \n",
      "  ['daniel', 'greenfield,', 'shillman', 'journal', 'fellow', 'freedom', 'center,', 'new', 'york', 'writer', 'focus', 'radic', 'islam.', 'final', 'stretch', 'election,', 'hillari', 'rodham', 'clinton', 'gone', 'war', 'fbi.', 'word', '“unprecedented”', 'thrown', 'around', 'often', 'elect', 'ought', 'retired.', 'it', 'still', 'unpreced', 'nomine', 'major', 'polit', 'parti', 'go', 'war', 'fbi.', 'that', 'exact', 'hillari', 'peopl', 'done.', 'coma', 'patient', 'wake', 'watch', 'hour', 'cnn', 'hospit', 'bed', 'would', 'assum', 'fbi', 'director', 'jame', 'comey', 'hillari', 'oppon', 'election.', 'fbi', 'attack', 'everyon', 'obama', 'cnn.', 'hillari', 'peopl', 'circul', 'letter', 'attack', 'comey.', 'current', 'medium', 'hit', 'piec', 'lambast', 'target', 'trump.', \"wouldn't\", 'surpris', 'clinton', 'alli', 'start', 'run', 'attack', 'ad', 'fbi.', 'fbi', 'leadership', 'warn', 'entir', 'left-w', 'establish', 'form', 'lynch', 'mob', 'continu', 'go', 'hillary.', 'fbi', 'credibl', 'attack', 'medium', 'democrat', 'preemptiv', 'head', 'result', 'investig', 'clinton', 'foundat', 'hillari', 'clinton.', 'covert', 'struggl', 'fbi', 'agent', 'obama', 'doj', 'peopl', 'gone', 'explos', 'public.', 'new', 'york', 'time', 'compar', 'comey', 'j.', 'edgar', 'hoover.', 'bizarr', 'headline,', '“jame', 'comey', 'role', 'recal', 'hoover', 'fbi,', 'fair', 'not”', 'practic', 'admit', 'front', 'it', 'spout', 'nonsense.', 'boston', 'globe', 'publish', 'column', 'call', 'comey', 'resignation.', 'outdone,', 'time', 'editori', 'claim', 'scandal', 'realli', 'attack', 'women.', 'jame', 'carvill', 'appear', 'msnbc', 'remind', 'everyon', 'still', 'aliv', 'insane.', 'accus', 'comey', 'coordin', 'hous', 'republican', 'kgb.', 'thought', '“vast', 'right', 'wing', 'conspiracy”', 'stretch.', 'countless', 'medium', 'stori', 'charg', 'comey', 'violat', 'procedure.', 'know', 'what', 'procedur', 'violation?', 'email', 'classifi', 'inform', 'store', 'bathroom', 'server.', 'senat', 'harri', 'reid', 'sent', 'comey', 'letter', 'accus', 'violat', 'hatch', 'act.', 'hatch', 'act', 'nice', 'idea', 'much', 'relev', 'age', 'obama', 'tenth', 'amendment.', 'cabl', 'news', 'spectrum', 'quick', 'fill', 'medium', 'hack', 'glanc', 'wikipedia', 'articl', 'hatch', 'act', 'tabl', 'accus', 'fbi', 'director', 'one', 'awkward', 'conspiraci', 'hillari', 'ever.', 'jame', 'comey', 'realli', 'hurt', 'hillary,', 'pick', 'one', 'hell', 'strang', 'way', 'it.', 'long', 'ago', 'democrat', 'breath', 'sigh', 'relief', 'gave', 'hillari', 'clinton', 'pas', 'promin', 'public', 'statement.', 'realli', 'elect', 'trump', 'keep', 'email', 'scandal', 'going,', 'trash', 'investigation?', 'payrol', 'hous', 'republican', 'kgb', 'back', 'play', 'coy', 'sudden', 'develop', 'vladimir', 'putin', 'paul', 'ryan', 'talk', 'take', 'look', 'anthoni', 'weiner', 'computer?', 'either', 'comey', 'cun', 'fbi', 'director', 'ever', 'live', 'he', 'awkward', 'tri', 'navig', 'polit', 'mess', 'trap', 'doj', 'leadership', 'whose', 'polit', 'futur', 'tie', 'hillari', 'victori', 'bureau', 'whose', 'apolit', 'agent', 'want', 'allow', 'jobs.', 'truli', 'mysteri', 'thing', 'hillari', 'associ', 'decid', 'go', 'war', 'respect', 'feder', 'agency.', 'american', 'like', 'fbi', 'hillari', 'clinton', 'enjoy', '60%', 'unfavor', 'rating.', 'it', 'interest', 'question.', 'hillari', 'old', 'strategi', 'lie', 'deni', 'fbi', 'even', 'crimin', 'investig', 'underway.', 'instead', 'associ', 'insist', 'secur', 'review.', 'fbi', 'correct', 'shrug', 'off.', 'old', 'breezi', 'denial', 'approach', 'given', 'way', 'savag', 'assault', 'fbi.', 'pretend', 'noth', 'wrong', 'bad', 'strategy,', 'better', 'one', 'pick', 'fight', 'fbi', 'lunat', 'clinton', 'associ', 'tri', 'claim', 'fbi', 'realli', 'kgb.', 'two', 'possibl', 'explanations.', 'hillari', 'clinton', 'might', 'arrog', 'enough', 'lash', 'fbi', 'belief', 'victori', 'near.', 'kind', 'hubri', 'led', 'plan', 'victori', 'firework', 'display', 'could', 'lead', 'declar', 'war', 'fbi', 'irrit', 'final', 'mile', 'campaign.', 'explan', 'peopl', 'panicked.', 'go', 'war', 'fbi', 'behavior', 'smart', 'focus', 'presidenti', 'campaign.', 'it', 'act', 'desperation.', 'presidenti', 'candid', 'decid', 'option', 'tri', 'destroy', 'credibl', 'fbi,', 'that', 'hubris,', 'it', 'fear', 'fbi', 'might', 'reveal', 'her.', 'origin', 'fbi', 'investigation,', 'hillari', 'clinton', 'confid', 'could', 'ride', 'out.', 'good', 'reason', 'believ', 'that.', 'hillari', 'clinton', 'gone.', 'place', 'paranoid', 'wreck.', 'within', 'short', 'space', 'time', '“positive”', 'clinton', 'campaign', 'promis', 'unit', 'countri', 'replac', 'desper', 'flail', 'oper', 'focus', 'energi', 'fight', 'fbi.', 'there', 'one', 'reason', 'bizarr', 'behavior.', 'clinton', 'campaign', 'decid', 'fbi', 'investig', 'latest', 'batch', 'email', 'pose', 'threat', 'survival.', 'it', 'gone', 'fight', 'fbi.', 'it', 'unpreced', 'step', 'born', 'fear.', 'it', 'hard', 'know', 'whether', 'fear', 'justified.', 'exist', 'fear', 'alreadi', 'tell', 'u', 'whole', 'lot.', 'clinton', 'loyalist', 'rig', 'old', 'investigation.', 'knew', 'outcom', 'ahead', 'time', 'well', 'knew', 'debat', 'questions.', 'sudden', 'longer', 'control.', 'afraid.', 'smell', 'fear.', 'fbi', 'wiretap', 'investig', 'clinton', 'foundation.', 'it', 'find', 'new', 'email', 'time.', 'clintonworld', 'panicked.', 'spinmeist', 'clintonworld', 'claim', 'email', 'scandal', 'much', 'smoke', 'without', 'fire.', 'that', 'appear', 'improprieti', 'without', 'substance.', \"isn't\", 'react', 'smoke.', 'it', 'respond', 'fire.', 'misguid', 'assault', 'fbi', 'tell', 'u', 'hillari', 'clinton', 'alli', 'afraid', 'revel', 'bigger', 'fundament', 'illeg', 'email', 'setup.', 'email', 'setup', 'preemptiv', 'cover', 'up.', 'clinton', 'campaign', 'panick', 'bad', 'belief,', 'right', 'wrong,', 'whatev', 'crime', 'illeg', 'setup', 'meant', 'cover', 'risk', 'exposed.', 'clinton', 'weather', 'countless', 'scandal', 'years.', 'whatev', 'protect', 'time', 'around', 'bigger', 'usual', 'corruption,', 'bribery,', 'sexual', 'assault', 'abus', 'power', 'follow', 'around', 'throughout', 'years.', 'bigger', 'damag', 'alleg', 'alreadi', 'come', 'out.', \"don't\", 'want', 'fbi', 'investig', 'anywher', 'near', 'it.', 'campaign', 'comey', 'pure', 'intimidation.', 'it', 'also', 'warning.', 'senior', 'fbi', 'peopl', 'valu', 'career', 'warn', 'stay', 'away.', 'democrat', 'close', 'rank', 'around', 'nomine', 'fbi.', 'it', 'ugli', 'unpreced', 'scene.', 'may', 'also', 'last', 'stand.', 'hillari', 'clinton', 'awkward', 'wound', 'way', 'numer', 'scandal', 'elect', 'cycle.', 'she', 'never', 'shown', 'fear', 'desper', 'before.', 'changed.', 'whatev', 'afraid', 'of,', 'lie', 'buri', 'email', 'huma', 'abedin.', 'bring', 'like', 'noth', 'els', 'has.']\n",
      "first titleandtext cleaned : \n",
      "  ['smell', 'hillari', 'fear', 'daniel', 'greenfield,', 'shillman', 'journal', 'fellow', 'freedom', 'center,', 'new', 'york', 'writer', 'focus', 'radic', 'islam.', 'final', 'stretch', 'election,', 'hillari', 'rodham', 'clinton', 'gone', 'war', 'fbi.', 'word', '“unprecedented”', 'thrown', 'around', 'often', 'elect', 'ought', 'retired.', 'it', 'still', 'unpreced', 'nomine', 'major', 'polit', 'parti', 'go', 'war', 'fbi.', 'that', 'exact', 'hillari', 'peopl', 'done.', 'coma', 'patient', 'wake', 'watch', 'hour', 'cnn', 'hospit', 'bed', 'would', 'assum', 'fbi', 'director', 'jame', 'comey', 'hillari', 'oppon', 'election.', 'fbi', 'attack', 'everyon', 'obama', 'cnn.', 'hillari', 'peopl', 'circul', 'letter', 'attack', 'comey.', 'current', 'medium', 'hit', 'piec', 'lambast', 'target', 'trump.', \"wouldn't\", 'surpris', 'clinton', 'alli', 'start', 'run', 'attack', 'ad', 'fbi.', 'fbi', 'leadership', 'warn', 'entir', 'left-w', 'establish', 'form', 'lynch', 'mob', 'continu', 'go', 'hillary.', 'fbi', 'credibl', 'attack', 'medium', 'democrat', 'preemptiv', 'head', 'result', 'investig', 'clinton', 'foundat', 'hillari', 'clinton.', 'covert', 'struggl', 'fbi', 'agent', 'obama', 'doj', 'peopl', 'gone', 'explos', 'public.', 'new', 'york', 'time', 'compar', 'comey', 'j.', 'edgar', 'hoover.', 'bizarr', 'headline,', '“jame', 'comey', 'role', 'recal', 'hoover', 'fbi,', 'fair', 'not”', 'practic', 'admit', 'front', 'it', 'spout', 'nonsense.', 'boston', 'globe', 'publish', 'column', 'call', 'comey', 'resignation.', 'outdone,', 'time', 'editori', 'claim', 'scandal', 'realli', 'attack', 'women.', 'jame', 'carvill', 'appear', 'msnbc', 'remind', 'everyon', 'still', 'aliv', 'insane.', 'accus', 'comey', 'coordin', 'hous', 'republican', 'kgb.', 'thought', '“vast', 'right', 'wing', 'conspiracy”', 'stretch.', 'countless', 'medium', 'stori', 'charg', 'comey', 'violat', 'procedure.', 'know', 'what', 'procedur', 'violation?', 'email', 'classifi', 'inform', 'store', 'bathroom', 'server.', 'senat', 'harri', 'reid', 'sent', 'comey', 'letter', 'accus', 'violat', 'hatch', 'act.', 'hatch', 'act', 'nice', 'idea', 'much', 'relev', 'age', 'obama', 'tenth', 'amendment.', 'cabl', 'news', 'spectrum', 'quick', 'fill', 'medium', 'hack', 'glanc', 'wikipedia', 'articl', 'hatch', 'act', 'tabl', 'accus', 'fbi', 'director', 'one', 'awkward', 'conspiraci', 'hillari', 'ever.', 'jame', 'comey', 'realli', 'hurt', 'hillary,', 'pick', 'one', 'hell', 'strang', 'way', 'it.', 'long', 'ago', 'democrat', 'breath', 'sigh', 'relief', 'gave', 'hillari', 'clinton', 'pas', 'promin', 'public', 'statement.', 'realli', 'elect', 'trump', 'keep', 'email', 'scandal', 'going,', 'trash', 'investigation?', 'payrol', 'hous', 'republican', 'kgb', 'back', 'play', 'coy', 'sudden', 'develop', 'vladimir', 'putin', 'paul', 'ryan', 'talk', 'take', 'look', 'anthoni', 'weiner', 'computer?', 'either', 'comey', 'cun', 'fbi', 'director', 'ever', 'live', 'he', 'awkward', 'tri', 'navig', 'polit', 'mess', 'trap', 'doj', 'leadership', 'whose', 'polit', 'futur', 'tie', 'hillari', 'victori', 'bureau', 'whose', 'apolit', 'agent', 'want', 'allow', 'jobs.', 'truli', 'mysteri', 'thing', 'hillari', 'associ', 'decid', 'go', 'war', 'respect', 'feder', 'agency.', 'american', 'like', 'fbi', 'hillari', 'clinton', 'enjoy', '60%', 'unfavor', 'rating.', 'it', 'interest', 'question.', 'hillari', 'old', 'strategi', 'lie', 'deni', 'fbi', 'even', 'crimin', 'investig', 'underway.', 'instead', 'associ', 'insist', 'secur', 'review.', 'fbi', 'correct', 'shrug', 'off.', 'old', 'breezi', 'denial', 'approach', 'given', 'way', 'savag', 'assault', 'fbi.', 'pretend', 'noth', 'wrong', 'bad', 'strategy,', 'better', 'one', 'pick', 'fight', 'fbi', 'lunat', 'clinton', 'associ', 'tri', 'claim', 'fbi', 'realli', 'kgb.', 'two', 'possibl', 'explanations.', 'hillari', 'clinton', 'might', 'arrog', 'enough', 'lash', 'fbi', 'belief', 'victori', 'near.', 'kind', 'hubri', 'led', 'plan', 'victori', 'firework', 'display', 'could', 'lead', 'declar', 'war', 'fbi', 'irrit', 'final', 'mile', 'campaign.', 'explan', 'peopl', 'panicked.', 'go', 'war', 'fbi', 'behavior', 'smart', 'focus', 'presidenti', 'campaign.', 'it', 'act', 'desperation.', 'presidenti', 'candid', 'decid', 'option', 'tri', 'destroy', 'credibl', 'fbi,', 'that', 'hubris,', 'it', 'fear', 'fbi', 'might', 'reveal', 'her.', 'origin', 'fbi', 'investigation,', 'hillari', 'clinton', 'confid', 'could', 'ride', 'out.', 'good', 'reason', 'believ', 'that.', 'hillari', 'clinton', 'gone.', 'place', 'paranoid', 'wreck.', 'within', 'short', 'space', 'time', '“positive”', 'clinton', 'campaign', 'promis', 'unit', 'countri', 'replac', 'desper', 'flail', 'oper', 'focus', 'energi', 'fight', 'fbi.', 'there', 'one', 'reason', 'bizarr', 'behavior.', 'clinton', 'campaign', 'decid', 'fbi', 'investig', 'latest', 'batch', 'email', 'pose', 'threat', 'survival.', 'it', 'gone', 'fight', 'fbi.', 'it', 'unpreced', 'step', 'born', 'fear.', 'it', 'hard', 'know', 'whether', 'fear', 'justified.', 'exist', 'fear', 'alreadi', 'tell', 'u', 'whole', 'lot.', 'clinton', 'loyalist', 'rig', 'old', 'investigation.', 'knew', 'outcom', 'ahead', 'time', 'well', 'knew', 'debat', 'questions.', 'sudden', 'longer', 'control.', 'afraid.', 'smell', 'fear.', 'fbi', 'wiretap', 'investig', 'clinton', 'foundation.', 'it', 'find', 'new', 'email', 'time.', 'clintonworld', 'panicked.', 'spinmeist', 'clintonworld', 'claim', 'email', 'scandal', 'much', 'smoke', 'without', 'fire.', 'that', 'appear', 'improprieti', 'without', 'substance.', \"isn't\", 'react', 'smoke.', 'it', 'respond', 'fire.', 'misguid', 'assault', 'fbi', 'tell', 'u', 'hillari', 'clinton', 'alli', 'afraid', 'revel', 'bigger', 'fundament', 'illeg', 'email', 'setup.', 'email', 'setup', 'preemptiv', 'cover', 'up.', 'clinton', 'campaign', 'panick', 'bad', 'belief,', 'right', 'wrong,', 'whatev', 'crime', 'illeg', 'setup', 'meant', 'cover', 'risk', 'exposed.', 'clinton', 'weather', 'countless', 'scandal', 'years.', 'whatev', 'protect', 'time', 'around', 'bigger', 'usual', 'corruption,', 'bribery,', 'sexual', 'assault', 'abus', 'power', 'follow', 'around', 'throughout', 'years.', 'bigger', 'damag', 'alleg', 'alreadi', 'come', 'out.', \"don't\", 'want', 'fbi', 'investig', 'anywher', 'near', 'it.', 'campaign', 'comey', 'pure', 'intimidation.', 'it', 'also', 'warning.', 'senior', 'fbi', 'peopl', 'valu', 'career', 'warn', 'stay', 'away.', 'democrat', 'close', 'rank', 'around', 'nomine', 'fbi.', 'it', 'ugli', 'unpreced', 'scene.', 'may', 'also', 'last', 'stand.', 'hillari', 'clinton', 'awkward', 'wound', 'way', 'numer', 'scandal', 'elect', 'cycle.', 'she', 'never', 'shown', 'fear', 'desper', 'before.', 'changed.', 'whatev', 'afraid', 'of,', 'lie', 'buri', 'email', 'huma', 'abedin.', 'bring', 'like', 'noth', 'els', 'has.']\n"
     ]
    }
   ],
   "source": [
    "print(\"first title cleaned : \\n\",clean_title[0])\n",
    "print(\"first text cleaned : \\n \",clean_text[0])\n",
    "print(\"first titleandtext cleaned : \\n \",clean_titleandtext[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the real number version of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# Get real number version label \n",
    "\n",
    "realnumber_y=[1]*len(titleandtext)\n",
    "count=0\n",
    "for word in y:\n",
    "    if(word=='FAKE'):\n",
    "        realnumber_y[count]=0\n",
    "    else:\n",
    "        realnumber_y[count]=1\n",
    "    count+=1\n",
    "realnumber_y = np.array(realnumber_y).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Between titles and texts\n",
    "\n",
    "## Concatenation\n",
    "Our first notion of this issue is that we would like to explore the relationship between titles and texts? How do we take fully advatange of information lie in the dataset especially for FakeNews dataset? First of all, we regard titles as part of the corpus and train them together. We do this by concatenating vectors generated from the title corpus and text corpus seperately into one long vector. And then we will see if the long vector can provide sufficient information that we need. The vectorization method we used was Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Haorui Ji 2018-9-27\n",
    "# Train a w2v model based on the whole corpus\n",
    "\n",
    "model = Word2Vec(clean_titleandtext,\n",
    "                       size = 300,\n",
    "                       window = 5,\n",
    "                       min_count = 0,\n",
    "                       sg = 0,\n",
    "                       alpha = 0.025,\n",
    "                       iter=10,\n",
    "                       batch_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Haorui Ji 2018-9-24\n",
    "# Develope doc vectors from word vectors by taking average of word vectors(Naive Doc2Vec)\n",
    "# Remove those with no text---deleteindex\n",
    "\n",
    "titleandtext_list = []\n",
    "idx = -1\n",
    "for review in clean_titleandtext:\n",
    "    idx += 1\n",
    "    init_vec = np.zeros([300,])\n",
    "    num_words = len(review)\n",
    "    for word in review:\n",
    "        init_vec += model.wv[word]\n",
    "    init_vec /= num_words\n",
    "    titleandtext_list.append(init_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6299, 300)\n"
     ]
    }
   ],
   "source": [
    "titleandtext_array=np.asarray(titleandtext_list)\n",
    "print(titleandtext_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Haorui Ji 2018-9-24\n",
    "# Some data in the vectors are NaN, so we initialize them with mean value\n",
    "\n",
    "# titleandtext\n",
    "print(np.isnan(titleandtext_array).any())\n",
    "titleandtext_array[np.isnan(titleandtext_array)] = np.mean(titleandtext_array[~np.isnan(titleandtext_array)])\n",
    "print(np.isnan(titleandtext_array).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the vector, the next step is to find an accurate classifier, in order to do so, we tried four different kinds of classification methods which are Logistic Regression, Random Forest, XGBoost and SVC( we also intended to try multinomial naive Bayesian model, however it seems that MNB model doesn't take negative inputs and some elements in our vectors are inevitably negative. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_f1_score: 0.90 (+/- 0.00) [LR]\n",
      "text_f1_score: 0.87 (+/- 0.01) [RF]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_f1_score: 0.90 (+/- 0.00) [XGB]\n",
      "text_f1_score: 0.88 (+/- 0.00) [SVM]\n"
     ]
    }
   ],
   "source": [
    "clf1=LogisticRegression()\n",
    "clf2=RandomForestClassifier()\n",
    "clf3=XGBClassifier()\n",
    "clf4=SVC(probability=True)\n",
    "\n",
    "for clf,label in zip([clf1,clf2,clf3,clf4],['LR','RF','XGB','SVM']):\n",
    "    scores=cross_validation.cross_val_score(clf,titleandtext_array,realnumber_y,scoring='f1')\n",
    "    print(\"text_f1_score: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Weighted Average\n",
    "\n",
    "People always regard titles as part of the corpus and train them together. However, as we all know, title is the most informative message part in an article, and lots of fake news are clickbait. We wonder if we can train with the title alone and combine the results with those trained from the text corpus, in this way we may get a better result. Therefore, we seperately build two models, one trained from the text corpus and the other trained on title corpus, then we take the weighted average of these two different models' results. The weights of different models will be decided based on their classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# train two w2v models, one trained on text corpus, the other trained on title corpus\n",
    "\n",
    "\n",
    "model1 = Word2Vec(clean_title,\n",
    "                       size = 300,\n",
    "                       window = 5,\n",
    "                       min_count = 0,\n",
    "                       sg = 0,\n",
    "                       alpha = 0.025,\n",
    "                       iter=10,\n",
    "                       batch_words = 10000)\n",
    "model2 = Word2Vec(clean_text,\n",
    "                        size = 300,\n",
    "                        window = 5,\n",
    "                        min_count = 1,\n",
    "                        sg = 0,\n",
    "                        alpha = 0.025,\n",
    "                        iter=10,\n",
    "                        batch_words = 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# Haorui Ji 2018-9-24\n",
    "# Develope doc vectors from word vectors by taking average of word vectors(Naive Doc2Vec)\n",
    "# Remove those with no text---deleteindex\n",
    "\n",
    "title_list = []\n",
    "text_list = []\n",
    "idx = -1\n",
    "for title,text in zip(clean_title,clean_text):\n",
    "    idx += 1\n",
    "    init_vec_title = np.zeros([300,])\n",
    "    init_vec_text = np.zeros([300,])\n",
    "    num_words_title = len(title)\n",
    "    num_words_text = len(text)\n",
    "        \n",
    "    for tiword in title:\n",
    "        init_vec_title += model1.wv[tiword]    \n",
    "    init_vec_title /= num_words_title\n",
    "    title_list.append(init_vec_title)\n",
    "    for teword in text:\n",
    "        init_vec_text += model2.wv[teword]\n",
    "    init_vec_text /= num_words_text\n",
    "    text_list.append(init_vec_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6299, 600)\n",
      "(6299, 300)\n",
      "(6299, 300)\n"
     ]
    }
   ],
   "source": [
    "# Haorui Ji 2018-9-27\n",
    "# generate title and text vectors respectivly\n",
    "\n",
    "# In this time, \"titleandtext\" vectors are generated by concatenating title and text vectors \n",
    "# because they both nake up our training and test set\n",
    "\n",
    "titleandtext_list=[]\n",
    "for title,text in zip(title_list,text_list):\n",
    "    titleandtext_list.append([*list(title),*list(text)])\n",
    "titleandtext_array=np.asarray(titleandtext_list)\n",
    "title_array=np.asarray(title_list)\n",
    "text_array=np.asarray(text_list)\n",
    "print(titleandtext_array.shape)\n",
    "print(title_array.shape)\n",
    "print(text_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Haorui Ji 2018-9-24\n",
    "# Some data in the vectors are NaN, so we initialize them with mean value\n",
    "\n",
    "# titleandtext\n",
    "print(np.isnan(titleandtext_array).any())\n",
    "titleandtext_array[np.isnan(titleandtext_array)] = np.mean(titleandtext_array[~np.isnan(titleandtext_array)])\n",
    "print(np.isnan(titleandtext_array).any())\n",
    "\n",
    "#title\n",
    "print(np.isnan(title_array).any())\n",
    "title_array[np.isnan(title_array)] = np.mean(title_array[~np.isnan(title_array)])\n",
    "print(np.isnan(title_array).any())\n",
    "\n",
    "#text\n",
    "print(np.isnan(text_array).any())\n",
    "text_array[np.isnan(text_array)] = np.mean(text_array[~np.isnan(text_array)])\n",
    "print(np.isnan(text_array).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first evaluatre models based on text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_f1_score: 0.90 (+/- 0.00) [LR]\n",
      "text_f1_score: 0.87 (+/- 0.01) [RF]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_f1_score: 0.90 (+/- 0.00) [XGB]\n",
      "text_f1_score: 0.88 (+/- 0.00) [SVM]\n"
     ]
    }
   ],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# test the text vectors on four different classifiers(using crossvalidation)\n",
    "# evaluate their mean value and std of f-1 values\n",
    "\n",
    "w_text_array = text_array\n",
    "\n",
    "for clf,label in zip([clf1,clf2,clf3,clf4],['LR','RF','XGB','SVM']):\n",
    "    scores=cross_validation.cross_val_score(clf,w_text_array,realnumber_y,scoring='f1')\n",
    "    print(\"text_f1_score: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we evaluate the model trained only on title corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_f1_score: 0.60 (+/- 0.01) [LR]\n",
      "text_f1_score: 0.62 (+/- 0.01) [RF]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_f1_score: 0.67 (+/- 0.00) [XGB]\n",
      "text_f1_score: 0.60 (+/- 0.01) [SVM]\n"
     ]
    }
   ],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# test the title vectors on four different classifiers(using crossvalidation)\n",
    "# evaluate their mean value and std of f-1 values\n",
    "\n",
    "w_title_array = title_array\n",
    "\n",
    "for clf,label in zip([clf1,clf2,clf3,clf4],['LR','RF','XGB','SVM']):\n",
    "    scores=cross_validation.cross_val_score(clf,w_title_array,realnumber_y,scoring='f1')\n",
    "    print(\"text_f1_score: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine results from different models, we built an ensemble classifier to calculate the weighted average of different models'results, the class of EnsembleClassifier we built shows as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# Define the EnsembleClassifier class\n",
    "\n",
    "class EnsembleClassifier():\n",
    "    \n",
    "    def __init__(self,textclfs,titleclfs,weights):\n",
    "        self.textclfs=textclfs\n",
    "        self.titleclfs=titleclfs\n",
    "        self.weights=weights\n",
    "\n",
    "    def fit(self,doc_vec,title_vec, y):\n",
    "        for text_clf in self.textclfs:\n",
    "            text_clf.fit(doc_vec,y)\n",
    "        for title_clf in self.titleclfs:\n",
    "            title_clf.fit(title_vec,y)\n",
    "        #print(\"fitting process is over\")\n",
    "    def predict(self, title_vec,doc_vec):\n",
    "        \n",
    "        print(\"predict process begins\")\n",
    "        avg=self.predict_proba(doc_vec,title_vec)\n",
    "        results=[]\n",
    "        for item in avg:\n",
    "          if(item[0]>item[1]):\n",
    "              results.append('FAKE')\n",
    "          else:\n",
    "              results.append('REAL')\n",
    "        print(\"predict process is over\")\n",
    "        return np.asarray(results)\n",
    "\n",
    "    def predict_proba(self, doc_vec, title_vec):\n",
    "        self.probas_ = [np.array(text_clf.predict_proba(doc_vec),dtype='float') for text_clf in self.textclfs]\n",
    "        for title_clf in self.titleclfs:\n",
    "            self.probas_.append(title_clf.predict_proba(title_vec))\n",
    "        avg = np.average(self.probas_, axis=0, weights=self.weights)\n",
    "#        for result,weight in zip(self.probas_,self.weights):\n",
    "#            sumresult+=result*weight\n",
    "#            sumweight+=weight\n",
    "#        avg=sumresult/sumweight\n",
    "        print(avg)\n",
    "        return avg  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initiation of this EnsembleClassifier takes three parameters, the first is the list of classifiers we want to train with text corpus, the second is the list of models we want to train with title corpus, the third is the list of weights for all classifiers we provided in the first two parameters. From the result of our previous tests, we can see that the best classifier for text corpus and title corpus is Logistic Regression and XGBoost respectively. So we used these two classifiers to build our Ensemble Classifier, and since the f-1  score of title corpus based model is fairly low, we set its weight to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# divide title corpus, text corpus and labels into training set and testing set\n",
    "\n",
    "teandti_train,teandti_test,y_train,y_test=tts(titleandtext_list,y,test_size=0.33,random_state=22)\n",
    "ultraclf=EnsembleClassifier(textclfs=[clf1],titleclfs=[clf3],weights=[1,0.5])\n",
    "text_train=[]\n",
    "title_train=[]\n",
    "text_test=[]\n",
    "title_test=[]\n",
    "for train in teandti_train:\n",
    "    title_train.append(train[0:300])\n",
    "    text_train.append(train[300:])\n",
    "for test in teandti_test:\n",
    "    title_test.append(test[0:300])\n",
    "    text_test.append(test[300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict process begins\n",
      "[[0.24226075 0.75773924]\n",
      " [0.93897326 0.06102675]\n",
      " [0.17325811 0.82674189]\n",
      " ...\n",
      " [0.03689138 0.96310862]\n",
      " [0.86441097 0.13558904]\n",
      " [0.08708648 0.91291352]]\n",
      "predict process is over\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       FAKE     0.8810    0.9138    0.8971      1021\n",
      "       REAL     0.9137    0.8809    0.8970      1058\n",
      "\n",
      "avg / total     0.8977    0.8971    0.8971      2079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# using the ensemble classifier to trian on title train corpus and text train corpus\n",
    "# make predicitons on title test corpus and text test corpus\n",
    "# evaluate its performance\n",
    "\n",
    "text_train=np.asarray(text_train)\n",
    "text_test=np.asarray(text_test)\n",
    "title_train=np.asarray(title_train)\n",
    "title_test=np.asarray(title_test)\n",
    "ultraclf.fit(text_train,title_train,y_train)\n",
    "prediction=ultraclf.predict(title_test,text_test)\n",
    "target_names=['FAKE','REAL']\n",
    "print(cr(y_test,prediction,target_names=target_names,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Using different embedding method\n",
    "\n",
    "\n",
    "We think the reason why weighted average method by ensembling the training results from title and text respectively shows no better performance is that the information provided by titles is either too few to influence the final resullt or too conflicting with those provided by the text. Therefore, we think maybe using the combinations of different vectors made by different vectorization methods would provide us with more information, and the way of combination we tried is still concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF by ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we like to do is strength the learning representation of the word vectors. In order to do so, one way is to multiply these word vectors we obtained earlier by an the tfidf coefficient. That is, combined word2vec with tfidf to add a weight to the word, to judge its importance. However, diectly applying tf-idf model in gensim and sklearn cannot meet our needs of extracting tf-idf coefficient of a particular word, so we have to write one on our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'fbi': 22, 'hillari': 17, 'clinton': 17, 'it': 12, 'comey': 10, 'email': 8, 'fbi.': 7, 'fear': 5, 'war': 5, 'peopl': 5, 'attack': 5, 'investig': 5, 'time': 5, 'scandal': 5, 'around': 4, 'go': 4, 'medium': 4, 'realli': 4, 'one': 4, 'campaign': 4, 'new': 3, 'focus': 3, 'gone': 3, 'elect': 3, 'unpreced': 3, 'polit': 3, 'that': 3, 'director': 3, 'jame': 3, 'obama': 3, 'democrat': 3, 'claim': 3, 'accus': 3, 'hatch': 3, 'act': 3, 'awkward': 3, 'way': 3, 'tri': 3, 'victori': 3, 'associ': 3, 'decid': 3, 'old': 3, 'assault': 3, 'fight': 3, 'bigger': 3, 'whatev': 3, 'smell': 2, 'york': 2, 'final': 2, 'still': 2, 'nomine': 2, 'everyon': 2, 'letter': 2, 'alli': 2, 'leadership': 2, 'warn': 2, 'credibl': 2, 'preemptiv': 2, 'agent': 2, 'doj': 2, 'bizarr': 2, 'fbi,': 2, 'appear': 2, 'hous': 2, 'republican': 2, 'kgb.': 2, 'right': 2, 'countless': 2, 'violat': 2, 'know': 2, 'much': 2, 'pick': 2, 'it.': 2, 'sudden': 2, 'whose': 2, 'want': 2, 'like': 2, 'lie': 2, 'noth': 2, 'bad': 2, 'might': 2, 'could': 2, 'campaign.': 2, 'panicked.': 2, 'presidenti': 2, 'out.': 2, 'reason': 2, 'desper': 2, 'fear.': 2, 'alreadi': 2, 'tell': 2, 'u': 2, 'knew': 2, 'clintonworld': 2, 'without': 2, 'fire.': 2, 'afraid': 2, 'illeg': 2, 'setup': 2, 'cover': 2, 'years.': 2, 'also': 2, 'daniel': 1, 'greenfield,': 1, 'shillman': 1, 'journal': 1, 'fellow': 1, 'freedom': 1, 'center,': 1, 'writer': 1, 'radic': 1, 'islam.': 1, 'stretch': 1, 'election,': 1, 'rodham': 1, 'word': 1, '“unprecedented”': 1, 'thrown': 1, 'often': 1, 'ought': 1, 'retired.': 1, 'major': 1, 'parti': 1, 'exact': 1, 'done.': 1, 'coma': 1, 'patient': 1, 'wake': 1, 'watch': 1, 'hour': 1, 'cnn': 1, 'hospit': 1, 'bed': 1, 'would': 1, 'assum': 1, 'oppon': 1, 'election.': 1, 'cnn.': 1, 'circul': 1, 'comey.': 1, 'current': 1, 'hit': 1, 'piec': 1, 'lambast': 1, 'target': 1, 'trump.': 1, \"wouldn't\": 1, 'surpris': 1, 'start': 1, 'run': 1, 'ad': 1, 'entir': 1, 'left-w': 1, 'establish': 1, 'form': 1, 'lynch': 1, 'mob': 1, 'continu': 1, 'hillary.': 1, 'head': 1, 'result': 1, 'foundat': 1, 'clinton.': 1, 'covert': 1, 'struggl': 1, 'explos': 1, 'public.': 1, 'compar': 1, 'j.': 1, 'edgar': 1, 'hoover.': 1, 'headline,': 1, '“jame': 1, 'role': 1, 'recal': 1, 'hoover': 1, 'fair': 1, 'not”': 1, 'practic': 1, 'admit': 1, 'front': 1, 'spout': 1, 'nonsense.': 1, 'boston': 1, 'globe': 1, 'publish': 1, 'column': 1, 'call': 1, 'resignation.': 1, 'outdone,': 1, 'editori': 1, 'women.': 1, 'carvill': 1, 'msnbc': 1, 'remind': 1, 'aliv': 1, 'insane.': 1, 'coordin': 1, 'thought': 1, '“vast': 1, 'wing': 1, 'conspiracy”': 1, 'stretch.': 1, 'stori': 1, 'charg': 1, 'procedure.': 1, 'what': 1, 'procedur': 1, 'violation?': 1, 'classifi': 1, 'inform': 1, 'store': 1, 'bathroom': 1, 'server.': 1, 'senat': 1, 'harri': 1, 'reid': 1, 'sent': 1, 'act.': 1, 'nice': 1, 'idea': 1, 'relev': 1, 'age': 1, 'tenth': 1, 'amendment.': 1, 'cabl': 1, 'news': 1, 'spectrum': 1, 'quick': 1, 'fill': 1, 'hack': 1, 'glanc': 1, 'wikipedia': 1, 'articl': 1, 'tabl': 1, 'conspiraci': 1, 'ever.': 1, 'hurt': 1, 'hillary,': 1, 'hell': 1, 'strang': 1, 'long': 1, 'ago': 1, 'breath': 1, 'sigh': 1, 'relief': 1, 'gave': 1, 'pas': 1, 'promin': 1, 'public': 1, 'statement.': 1, 'trump': 1, 'keep': 1, 'going,': 1, 'trash': 1, 'investigation?': 1, 'payrol': 1, 'kgb': 1, 'back': 1, 'play': 1, 'coy': 1, 'develop': 1, 'vladimir': 1, 'putin': 1, 'paul': 1, 'ryan': 1, 'talk': 1, 'take': 1, 'look': 1, 'anthoni': 1, 'weiner': 1, 'computer?': 1, 'either': 1, 'cun': 1, 'ever': 1, 'live': 1, 'he': 1, 'navig': 1, 'mess': 1, 'trap': 1, 'futur': 1, 'tie': 1, 'bureau': 1, 'apolit': 1, 'allow': 1, 'jobs.': 1, 'truli': 1, 'mysteri': 1, 'thing': 1, 'respect': 1, 'feder': 1, 'agency.': 1, 'american': 1, 'enjoy': 1, '60%': 1, 'unfavor': 1, 'rating.': 1, 'interest': 1, 'question.': 1, 'strategi': 1, 'deni': 1, 'even': 1, 'crimin': 1, 'underway.': 1, 'instead': 1, 'insist': 1, 'secur': 1, 'review.': 1, 'correct': 1, 'shrug': 1, 'off.': 1, 'breezi': 1, 'denial': 1, 'approach': 1, 'given': 1, 'savag': 1, 'pretend': 1, 'wrong': 1, 'strategy,': 1, 'better': 1, 'lunat': 1, 'two': 1, 'possibl': 1, 'explanations.': 1, 'arrog': 1, 'enough': 1, 'lash': 1, 'belief': 1, 'near.': 1, 'kind': 1, 'hubri': 1, 'led': 1, 'plan': 1, 'firework': 1, 'display': 1, 'lead': 1, 'declar': 1, 'irrit': 1, 'mile': 1, 'explan': 1, 'behavior': 1, 'smart': 1, 'desperation.': 1, 'candid': 1, 'option': 1, 'destroy': 1, 'hubris,': 1, 'reveal': 1, 'her.': 1, 'origin': 1, 'investigation,': 1, 'confid': 1, 'ride': 1, 'good': 1, 'believ': 1, 'that.': 1, 'gone.': 1, 'place': 1, 'paranoid': 1, 'wreck.': 1, 'within': 1, 'short': 1, 'space': 1, '“positive”': 1, 'promis': 1, 'unit': 1, 'countri': 1, 'replac': 1, 'flail': 1, 'oper': 1, 'energi': 1, 'there': 1, 'behavior.': 1, 'latest': 1, 'batch': 1, 'pose': 1, 'threat': 1, 'survival.': 1, 'step': 1, 'born': 1, 'hard': 1, 'whether': 1, 'justified.': 1, 'exist': 1, 'whole': 1, 'lot.': 1, 'loyalist': 1, 'rig': 1, 'investigation.': 1, 'outcom': 1, 'ahead': 1, 'well': 1, 'debat': 1, 'questions.': 1, 'longer': 1, 'control.': 1, 'afraid.': 1, 'wiretap': 1, 'foundation.': 1, 'find': 1, 'time.': 1, 'spinmeist': 1, 'smoke': 1, 'improprieti': 1, 'substance.': 1, \"isn't\": 1, 'react': 1, 'smoke.': 1, 'respond': 1, 'misguid': 1, 'revel': 1, 'fundament': 1, 'setup.': 1, 'up.': 1, 'panick': 1, 'belief,': 1, 'wrong,': 1, 'crime': 1, 'meant': 1, 'risk': 1, 'exposed.': 1, 'weather': 1, 'protect': 1, 'usual': 1, 'corruption,': 1, 'bribery,': 1, 'sexual': 1, 'abus': 1, 'power': 1, 'follow': 1, 'throughout': 1, 'damag': 1, 'alleg': 1, 'come': 1, \"don't\": 1, 'anywher': 1, 'near': 1, 'pure': 1, 'intimidation.': 1, 'warning.': 1, 'senior': 1, 'valu': 1, 'career': 1, 'stay': 1, 'away.': 1, 'close': 1, 'rank': 1, 'ugli': 1, 'scene.': 1, 'may': 1, 'last': 1, 'stand.': 1, 'wound': 1, 'numer': 1, 'cycle.': 1, 'she': 1, 'never': 1, 'shown': 1, 'before.': 1, 'changed.': 1, 'of,': 1, 'buri': 1, 'huma': 1, 'abedin.': 1, 'bring': 1, 'els': 1, 'has.': 1})\n"
     ]
    }
   ],
   "source": [
    "# Haorui Ji 2018-9-24\n",
    "# Compute tf-idf coefficient for every word\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Count words' frequency in title and text\n",
    "countlist = []\n",
    "for i in range(len(clean_titleandtext)):\n",
    "    count = Counter(clean_titleandtext[i])\n",
    "    countlist.append(count)\n",
    "\n",
    "print(countlist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf(word, count):\n",
    "    return count[word] / sum(count.values())\n",
    "\n",
    "def n_containing(word, count_list):\n",
    "    return sum(1 for count in count_list if word in count)\n",
    "\n",
    "def idf(word, count_list):\n",
    "    return math.log(len(count_list) / (1 + n_containing(word, count_list)))\n",
    "\n",
    "def tfidf(word, count, count_list):\n",
    "    return tf(word, count) * idf(word, count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "tf_idf_manual = []\n",
    "\n",
    "for i, count in enumerate(countlist):\n",
    "    scores = {word: tfidf(word, count, countlist) for word in count}\n",
    "    if i%100 == 0:\n",
    "        print(i)\n",
    "    tf_idf_manual.append(scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: fbi, TF-IDF: 0.0727\n",
      "\tWord: comey, TF-IDF: 0.04622\n",
      "\tWord: fbi., TF-IDF: 0.04513\n",
      "\tWord: hillari, TF-IDF: 0.02468\n",
      "\tWord: clinton, TF-IDF: 0.02458\n",
      "\tWord: scandal, TF-IDF: 0.02204\n",
      "\tWord: kgb., TF-IDF: 0.02164\n",
      "\tWord: it, TF-IDF: 0.02163\n",
      "\tWord: panicked., TF-IDF: 0.02083\n",
      "\tWord: email, TF-IDF: 0.01979\n",
      "\tWord: clintonworld, TF-IDF: 0.01968\n",
      "\tWord: awkward, TF-IDF: 0.01922\n",
      "\tWord: hatch, TF-IDF: 0.01886\n",
      "\tWord: setup, TF-IDF: 0.01673\n",
      "\tWord: fear, TF-IDF: 0.01607\n",
      "\tWord: preemptiv, TF-IDF: 0.016\n",
      "\tWord: unpreced, TF-IDF: 0.01447\n",
      "\tWord: smell, TF-IDF: 0.01431\n",
      "\tWord: fear., TF-IDF: 0.01392\n",
      "\tWord: bigger, TF-IDF: 0.01373\n",
      "\tWord: investig, TF-IDF: 0.01365\n",
      "\tWord: fire., TF-IDF: 0.01321\n",
      "\tWord: doj, TF-IDF: 0.01239\n",
      "\tWord: assault, TF-IDF: 0.01216\n",
      "\tWord: gone, TF-IDF: 0.01196\n",
      "\tWord: bizarr, TF-IDF: 0.01193\n",
      "\tWord: countless, TF-IDF: 0.01181\n",
      "\tWord: war, TF-IDF: 0.01172\n",
      "\tWord: fbi,, TF-IDF: 0.01166\n",
      "\tWord: whatev, TF-IDF: 0.01162\n",
      "\tWord: computer?, TF-IDF: 0.01139\n",
      "\tWord: spinmeist, TF-IDF: 0.01139\n",
      "\tWord: jame, TF-IDF: 0.01097\n",
      "\tWord: violation?, TF-IDF: 0.01082\n",
      "\tWord: wreck., TF-IDF: 0.01082\n",
      "\tWord: “positive”, TF-IDF: 0.01082\n",
      "\tWord: attack, TF-IDF: 0.01064\n",
      "\tWord: afraid, TF-IDF: 0.01052\n",
      "\tWord: retired., TF-IDF: 0.01041\n",
      "\tWord: breezi, TF-IDF: 0.01041\n",
      "\tWord: old, TF-IDF: 0.01039\n",
      "\tWord: victori, TF-IDF: 0.0101\n",
      "\tWord: hoover., TF-IDF: 0.0101\n",
      "\tWord: “jame, TF-IDF: 0.0101\n",
      "\tWord: outdone,, TF-IDF: 0.0101\n",
      "\tWord: setup., TF-IDF: 0.0101\n",
      "\tWord: greenfield,, TF-IDF: 0.00984\n",
      "\tWord: shillman, TF-IDF: 0.00984\n",
      "\tWord: not”, TF-IDF: 0.00984\n",
      "\tWord: carvill, TF-IDF: 0.00984\n",
      "\tWord: hubris,, TF-IDF: 0.00984\n",
      "\tWord: bribery,, TF-IDF: 0.00984\n",
      "\tWord: credibl, TF-IDF: 0.00983\n",
      "\tWord: coma, TF-IDF: 0.00962\n",
      "\tWord: explanations., TF-IDF: 0.00962\n",
      "\tWord: resignation., TF-IDF: 0.00943\n",
      "\tWord: kgb, TF-IDF: 0.00943\n",
      "\tWord: smoke., TF-IDF: 0.00943\n",
      "\tWord: sudden, TF-IDF: 0.00943\n",
      "\tWord: accus, TF-IDF: 0.00929\n",
      "\tWord: “unprecedented”, TF-IDF: 0.00927\n",
      "\tWord: near., TF-IDF: 0.00927\n",
      "\tWord: desperation., TF-IDF: 0.00927\n",
      "\tWord: improprieti, TF-IDF: 0.00927\n",
      "\tWord: desper, TF-IDF: 0.00913\n",
      "\tWord: “vast, TF-IDF: 0.00912\n",
      "\tWord: hubri, TF-IDF: 0.00912\n",
      "\tWord: warning., TF-IDF: 0.00912\n",
      "\tWord: cun, TF-IDF: 0.00898\n",
      "\tWord: apolit, TF-IDF: 0.00898\n",
      "\tWord: rating., TF-IDF: 0.00898\n",
      "\tWord: realli, TF-IDF: 0.0089\n",
      "\tWord: stretch., TF-IDF: 0.00886\n",
      "\tWord: procedure., TF-IDF: 0.00886\n",
      "\tWord: investigation?, TF-IDF: 0.00886\n",
      "\tWord: coy, TF-IDF: 0.00886\n",
      "\tWord: justified., TF-IDF: 0.00886\n",
      "\tWord: director, TF-IDF: 0.00872\n",
      "\tWord: associ, TF-IDF: 0.00868\n",
      "\tWord: headline,, TF-IDF: 0.00864\n",
      "\tWord: insane., TF-IDF: 0.00864\n",
      "\tWord: intimidation., TF-IDF: 0.00864\n",
      "\tWord: agent, TF-IDF: 0.00863\n",
      "\tWord: edgar, TF-IDF: 0.00854\n",
      "\tWord: belief,, TF-IDF: 0.00854\n",
      "\tWord: medium, TF-IDF: 0.00852\n",
      "\tWord: spout, TF-IDF: 0.00845\n",
      "\tWord: conspiracy”, TF-IDF: 0.00845\n",
      "\tWord: afraid., TF-IDF: 0.00845\n",
      "\tWord: wiretap, TF-IDF: 0.00837\n",
      "\tWord: firework, TF-IDF: 0.00829\n",
      "\tWord: flail, TF-IDF: 0.00829\n",
      "\tWord: exposed., TF-IDF: 0.00829\n",
      "\tWord: around, TF-IDF: 0.00817\n",
      "\tWord: tenth, TF-IDF: 0.00814\n",
      "\tWord: substance., TF-IDF: 0.00814\n",
      "\tWord: letter, TF-IDF: 0.0081\n",
      "\tWord: decid, TF-IDF: 0.00807\n",
      "\tWord: sigh, TF-IDF: 0.00807\n",
      "\tWord: knew, TF-IDF: 0.00803\n",
      "\tWord: lunat, TF-IDF: 0.008\n",
      "\tWord: stand., TF-IDF: 0.008\n",
      "\tWord: out., TF-IDF: 0.00794\n",
      "\tWord: wikipedia, TF-IDF: 0.00794\n",
      "\tWord: glanc, TF-IDF: 0.00788\n",
      "\tWord: paranoid, TF-IDF: 0.00788\n",
      "\tWord: misguid, TF-IDF: 0.00788\n",
      "\tWord: focus, TF-IDF: 0.0078\n",
      "\tWord: panick, TF-IDF: 0.00777\n",
      "\tWord: 60%, TF-IDF: 0.00771\n",
      "\tWord: underway., TF-IDF: 0.00771\n",
      "\tWord: violat, TF-IDF: 0.00771\n",
      "\tWord: act, TF-IDF: 0.0077\n",
      "\tWord: survival., TF-IDF: 0.00766\n",
      "\tWord: abedin., TF-IDF: 0.00766\n",
      "\tWord: going,, TF-IDF: 0.00761\n",
      "\tWord: leadership, TF-IDF: 0.00759\n",
      "\tWord: nonsense., TF-IDF: 0.00756\n",
      "\tWord: review., TF-IDF: 0.00756\n",
      "\tWord: hoover, TF-IDF: 0.00752\n",
      "\tWord: payrol, TF-IDF: 0.00752\n",
      "\tWord: irrit, TF-IDF: 0.00739\n",
      "\tWord: lot., TF-IDF: 0.00739\n",
      "\tWord: illeg, TF-IDF: 0.00732\n",
      "\tWord: loyalist, TF-IDF: 0.0073\n",
      "\tWord: lambast, TF-IDF: 0.00727\n",
      "\tWord: comey., TF-IDF: 0.00723\n",
      "\tWord: fight, TF-IDF: 0.00719\n",
      "\tWord: pick, TF-IDF: 0.00713\n",
      "\tWord: lie, TF-IDF: 0.00706\n",
      "\tWord: has., TF-IDF: 0.00705\n",
      "\tWord: scene., TF-IDF: 0.00702\n",
      "\tWord: alli, TF-IDF: 0.00699\n",
      "\tWord: amendment., TF-IDF: 0.00696\n",
      "\tWord: islam., TF-IDF: 0.00693\n",
      "\tWord: gone., TF-IDF: 0.0069\n",
      "\tWord: mob, TF-IDF: 0.00687\n",
      "\tWord: shrug, TF-IDF: 0.00684\n",
      "\tWord: arrog, TF-IDF: 0.00684\n",
      "\tWord: batch, TF-IDF: 0.00684\n",
      "\tWord: savag, TF-IDF: 0.00681\n",
      "\tWord: years., TF-IDF: 0.00671\n",
      "\tWord: campaign., TF-IDF: 0.00667\n",
      "\tWord: agency., TF-IDF: 0.00666\n",
      "\tWord: trash, TF-IDF: 0.00663\n",
      "\tWord: bathroom, TF-IDF: 0.00661\n",
      "\tWord: changed., TF-IDF: 0.00658\n",
      "\tWord: cover, TF-IDF: 0.00656\n",
      "\tWord: claim, TF-IDF: 0.0065\n",
      "\tWord: cycle., TF-IDF: 0.00649\n",
      "\tWord: left-w, TF-IDF: 0.00647\n",
      "\tWord: denial, TF-IDF: 0.00647\n",
      "\tWord: corruption,, TF-IDF: 0.00647\n",
      "\tWord: lash, TF-IDF: 0.00641\n",
      "\tWord: covert, TF-IDF: 0.00638\n",
      "\tWord: bed, TF-IDF: 0.00636\n",
      "\tWord: strategy,, TF-IDF: 0.00636\n",
      "\tWord: everyon, TF-IDF: 0.00633\n",
      "\tWord: spectrum, TF-IDF: 0.00631\n",
      "\tWord: warn, TF-IDF: 0.00625\n",
      "\tWord: behavior., TF-IDF: 0.00618\n",
      "\tWord: bad, TF-IDF: 0.00616\n",
      "\tWord: unfavor, TF-IDF: 0.00614\n",
      "\tWord: campaign, TF-IDF: 0.00613\n",
      "\tWord: of,, TF-IDF: 0.00609\n",
      "\tWord: whose, TF-IDF: 0.00609\n",
      "\tWord: globe, TF-IDF: 0.00606\n",
      "\tWord: reid, TF-IDF: 0.00604\n",
      "\tWord: ever., TF-IDF: 0.00601\n",
      "\tWord: rodham, TF-IDF: 0.00599\n",
      "\tWord: circul, TF-IDF: 0.00599\n",
      "\tWord: daniel, TF-IDF: 0.00596\n",
      "\tWord: trap, TF-IDF: 0.00596\n",
      "\tWord: nomine, TF-IDF: 0.00595\n",
      "\tWord: weather, TF-IDF: 0.00595\n",
      "\tWord: aliv, TF-IDF: 0.0059\n",
      "\tWord: questions., TF-IDF: 0.0059\n",
      "\tWord: buri, TF-IDF: 0.0059\n",
      "\tWord: wrong,, TF-IDF: 0.00585\n",
      "\tWord: question., TF-IDF: 0.00578\n",
      "\tWord: cabl, TF-IDF: 0.00576\n",
      "\tWord: that, TF-IDF: 0.00575\n",
      "\tWord: server., TF-IDF: 0.00574\n",
      "\tWord: ugli, TF-IDF: 0.00574\n",
      "\tWord: mess, TF-IDF: 0.00573\n",
      "\tWord: msnbc, TF-IDF: 0.0057\n",
      "\tWord: breath, TF-IDF: 0.0057\n",
      "\tWord: navig, TF-IDF: 0.0057\n",
      "\tWord: hillary., TF-IDF: 0.00567\n",
      "\tWord: boston, TF-IDF: 0.00565\n",
      "\tWord: strang, TF-IDF: 0.00565\n",
      "\tWord: foundation., TF-IDF: 0.00565\n",
      "\tWord: thrown, TF-IDF: 0.0056\n",
      "\tWord: cnn., TF-IDF: 0.00559\n",
      "\tWord: done., TF-IDF: 0.00558\n",
      "\tWord: lynch, TF-IDF: 0.00557\n",
      "\tWord: weiner, TF-IDF: 0.00557\n",
      "\tWord: investigation,, TF-IDF: 0.00557\n",
      "\tWord: relief, TF-IDF: 0.00551\n",
      "\tWord: jobs., TF-IDF: 0.0055\n",
      "\tWord: control., TF-IDF: 0.0055\n",
      "\tWord: editori, TF-IDF: 0.00548\n",
      "\tWord: mysteri, TF-IDF: 0.00546\n",
      "\tWord: anywher, TF-IDF: 0.00546\n",
      "\tWord: procedur, TF-IDF: 0.00545\n",
      "\tWord: patient, TF-IDF: 0.00544\n",
      "\tWord: smoke, TF-IDF: 0.00544\n",
      "\tWord: ought, TF-IDF: 0.0054\n",
      "\tWord: hillary,, TF-IDF: 0.0054\n",
      "\tWord: column, TF-IDF: 0.00539\n",
      "\tWord: huma, TF-IDF: 0.00539\n",
      "\tWord: react, TF-IDF: 0.00534\n",
      "\tWord: noth, TF-IDF: 0.00534\n",
      "\tWord: smart, TF-IDF: 0.00532\n",
      "\tWord: pure, TF-IDF: 0.00532\n",
      "\tWord: stretch, TF-IDF: 0.00531\n",
      "\tWord: ride, TF-IDF: 0.0053\n",
      "\tWord: final, TF-IDF: 0.00528\n",
      "\tWord: off., TF-IDF: 0.00527\n",
      "\tWord: pretend, TF-IDF: 0.00525\n",
      "\tWord: investigation., TF-IDF: 0.00525\n",
      "\tWord: relev, TF-IDF: 0.00524\n",
      "\tWord: center,, TF-IDF: 0.00523\n",
      "\tWord: act., TF-IDF: 0.00522\n",
      "\tWord: before., TF-IDF: 0.00519\n",
      "\tWord: hospit, TF-IDF: 0.00515\n",
      "\tWord: conspiraci, TF-IDF: 0.00514\n",
      "\tWord: explan, TF-IDF: 0.00514\n",
      "\tWord: time, TF-IDF: 0.00514\n",
      "\tWord: women., TF-IDF: 0.00513\n",
      "\tWord: tri, TF-IDF: 0.0051\n",
      "\tWord: public., TF-IDF: 0.00509\n",
      "\tWord: harri, TF-IDF: 0.00508\n",
      "\tWord: nice, TF-IDF: 0.00508\n",
      "\tWord: hell, TF-IDF: 0.00508\n",
      "\tWord: york, TF-IDF: 0.00508\n",
      "\tWord: tabl, TF-IDF: 0.00508\n",
      "\tWord: away., TF-IDF: 0.00504\n",
      "\tWord: obama, TF-IDF: 0.00501\n",
      "\tWord: explos, TF-IDF: 0.00501\n",
      "\tWord: wing, TF-IDF: 0.00498\n",
      "\tWord: bureau, TF-IDF: 0.00497\n",
      "\tWord: reason, TF-IDF: 0.00493\n",
      "\tWord: display, TF-IDF: 0.00492\n",
      "\tWord: elect, TF-IDF: 0.00488\n",
      "\tWord: vladimir, TF-IDF: 0.00487\n",
      "\tWord: tell, TF-IDF: 0.00486\n",
      "\tWord: writer, TF-IDF: 0.00484\n",
      "\tWord: peopl, TF-IDF: 0.00481\n",
      "\tWord: anthoni, TF-IDF: 0.0048\n",
      "\tWord: j., TF-IDF: 0.00478\n",
      "\tWord: hack, TF-IDF: 0.00475\n",
      "\tWord: store, TF-IDF: 0.00474\n",
      "\tWord: behavior, TF-IDF: 0.00474\n",
      "\tWord: rig, TF-IDF: 0.00474\n",
      "\tWord: coordin, TF-IDF: 0.00474\n",
      "\tWord: recal, TF-IDF: 0.00473\n",
      "\tWord: classifi, TF-IDF: 0.00473\n",
      "\tWord: revel, TF-IDF: 0.00472\n",
      "\tWord: wound, TF-IDF: 0.0047\n",
      "\tWord: rank, TF-IDF: 0.00469\n",
      "\tWord: numer, TF-IDF: 0.00468\n",
      "\tWord: abus, TF-IDF: 0.00467\n",
      "\tWord: career, TF-IDF: 0.00466\n",
      "\tWord: space, TF-IDF: 0.00465\n",
      "\tWord: democrat, TF-IDF: 0.00457\n",
      "\tWord: way, TF-IDF: 0.00457\n",
      "\tWord: statement., TF-IDF: 0.00456\n",
      "\tWord: mile, TF-IDF: 0.00456\n",
      "\tWord: putin, TF-IDF: 0.00452\n",
      "\tWord: option, TF-IDF: 0.0045\n",
      "\tWord: born, TF-IDF: 0.00449\n",
      "\tWord: ryan, TF-IDF: 0.00449\n",
      "\tWord: promin, TF-IDF: 0.00447\n",
      "\tWord: her., TF-IDF: 0.00442\n",
      "\tWord: it., TF-IDF: 0.00441\n",
      "\tWord: enjoy, TF-IDF: 0.00439\n",
      "\tWord: fundament, TF-IDF: 0.00438\n",
      "\tWord: alreadi, TF-IDF: 0.00437\n",
      "\tWord: hurt, TF-IDF: 0.00434\n",
      "\tWord: correct, TF-IDF: 0.00433\n",
      "\tWord: wouldn't, TF-IDF: 0.00431\n",
      "\tWord: truli, TF-IDF: 0.00431\n",
      "\tWord: assum, TF-IDF: 0.0043\n",
      "\tWord: outcom, TF-IDF: 0.00429\n",
      "\tWord: she, TF-IDF: 0.00428\n",
      "\tWord: without, TF-IDF: 0.00425\n",
      "\tWord: might, TF-IDF: 0.00425\n",
      "\tWord: sexual, TF-IDF: 0.00424\n",
      "\tWord: meant, TF-IDF: 0.00423\n",
      "\tWord: radic, TF-IDF: 0.00422\n",
      "\tWord: pose, TF-IDF: 0.00422\n",
      "\tWord: wake, TF-IDF: 0.00421\n",
      "\tWord: polit, TF-IDF: 0.0042\n",
      "\tWord: fill, TF-IDF: 0.00419\n",
      "\tWord: shown, TF-IDF: 0.00415\n",
      "\tWord: replac, TF-IDF: 0.00414\n",
      "\tWord: up., TF-IDF: 0.00414\n",
      "\tWord: pas, TF-IDF: 0.00412\n",
      "\tWord: appear, TF-IDF: 0.0041\n",
      "\tWord: remind, TF-IDF: 0.00408\n",
      "\tWord: go, TF-IDF: 0.00406\n",
      "\tWord: confid, TF-IDF: 0.00404\n",
      "\tWord: journal, TF-IDF: 0.00404\n",
      "\tWord: energi, TF-IDF: 0.00402\n",
      "\tWord: piec, TF-IDF: 0.00398\n",
      "\tWord: oppon, TF-IDF: 0.00394\n",
      "\tWord: usual, TF-IDF: 0.00392\n",
      "\tWord: age, TF-IDF: 0.00389\n",
      "\tWord: fellow, TF-IDF: 0.00388\n",
      "\tWord: valu, TF-IDF: 0.00387\n",
      "\tWord: fair, TF-IDF: 0.00381\n",
      "\tWord: foundat, TF-IDF: 0.00379\n",
      "\tWord: damag, TF-IDF: 0.00379\n",
      "\tWord: that., TF-IDF: 0.00378\n",
      "\tWord: els, TF-IDF: 0.00376\n",
      "\tWord: crime, TF-IDF: 0.00376\n",
      "\tWord: freedom, TF-IDF: 0.00371\n",
      "\tWord: wrong, TF-IDF: 0.0037\n",
      "\tWord: throughout, TF-IDF: 0.00369\n",
      "\tWord: hous, TF-IDF: 0.00368\n",
      "\tWord: struggl, TF-IDF: 0.00367\n",
      "\tWord: insist, TF-IDF: 0.00367\n",
      "\tWord: election,, TF-IDF: 0.00365\n",
      "\tWord: admit, TF-IDF: 0.00364\n",
      "\tWord: practic, TF-IDF: 0.00363\n",
      "\tWord: strategi, TF-IDF: 0.00361\n",
      "\tWord: surpris, TF-IDF: 0.00361\n",
      "\tWord: tie, TF-IDF: 0.00358\n",
      "\tWord: ahead, TF-IDF: 0.00358\n",
      "\tWord: ago, TF-IDF: 0.00357\n",
      "\tWord: risk, TF-IDF: 0.00357\n",
      "\tWord: exist, TF-IDF: 0.00356\n",
      "\tWord: respect, TF-IDF: 0.00354\n",
      "\tWord: still, TF-IDF: 0.00354\n",
      "\tWord: front, TF-IDF: 0.00354\n",
      "\tWord: approach, TF-IDF: 0.00352\n",
      "\tWord: compar, TF-IDF: 0.00352\n",
      "\tWord: destroy, TF-IDF: 0.00351\n",
      "\tWord: origin, TF-IDF: 0.00348\n",
      "\tWord: deni, TF-IDF: 0.00344\n",
      "\tWord: crimin, TF-IDF: 0.00344\n",
      "\tWord: alleg, TF-IDF: 0.0034\n",
      "\tWord: longer, TF-IDF: 0.00339\n",
      "\tWord: quick, TF-IDF: 0.00337\n",
      "\tWord: exact, TF-IDF: 0.00332\n",
      "\tWord: cnn, TF-IDF: 0.00332\n",
      "\tWord: election., TF-IDF: 0.00332\n",
      "\tWord: what, TF-IDF: 0.00332\n",
      "\tWord: declar, TF-IDF: 0.00331\n",
      "\tWord: time., TF-IDF: 0.00331\n",
      "\tWord: isn't, TF-IDF: 0.00331\n",
      "\tWord: clinton., TF-IDF: 0.00329\n",
      "\tWord: form, TF-IDF: 0.00328\n",
      "\tWord: belief, TF-IDF: 0.00327\n",
      "\tWord: gave, TF-IDF: 0.00326\n",
      "\tWord: whole, TF-IDF: 0.00325\n",
      "\tWord: stay, TF-IDF: 0.00324\n",
      "\tWord: reveal, TF-IDF: 0.00324\n",
      "\tWord: hit, TF-IDF: 0.00323\n",
      "\tWord: sent, TF-IDF: 0.00323\n",
      "\tWord: senior, TF-IDF: 0.00323\n",
      "\tWord: publish, TF-IDF: 0.00323\n",
      "\tWord: latest, TF-IDF: 0.00322\n",
      "\tWord: paul, TF-IDF: 0.00321\n",
      "\tWord: articl, TF-IDF: 0.00321\n",
      "\tWord: right, TF-IDF: 0.0032\n",
      "\tWord: target, TF-IDF: 0.00318\n",
      "\tWord: futur, TF-IDF: 0.00318\n",
      "\tWord: respond, TF-IDF: 0.00318\n",
      "\tWord: know, TF-IDF: 0.00317\n",
      "\tWord: presidenti, TF-IDF: 0.00317\n",
      "\tWord: promis, TF-IDF: 0.00317\n",
      "\tWord: charg, TF-IDF: 0.00317\n",
      "\tWord: short, TF-IDF: 0.00316\n",
      "\tWord: hour, TF-IDF: 0.0031\n",
      "\tWord: trump., TF-IDF: 0.00309\n",
      "\tWord: much, TF-IDF: 0.00308\n",
      "\tWord: develop, TF-IDF: 0.00308\n",
      "\tWord: threat, TF-IDF: 0.00308\n",
      "\tWord: role, TF-IDF: 0.00305\n",
      "\tWord: led, TF-IDF: 0.00305\n",
      "\tWord: instead, TF-IDF: 0.00304\n",
      "\tWord: u, TF-IDF: 0.00303\n",
      "\tWord: either, TF-IDF: 0.003\n",
      "\tWord: step, TF-IDF: 0.00299\n",
      "\tWord: word, TF-IDF: 0.00295\n",
      "\tWord: establish, TF-IDF: 0.00287\n",
      "\tWord: republican, TF-IDF: 0.00287\n",
      "\tWord: watch, TF-IDF: 0.00287\n",
      "\tWord: thought, TF-IDF: 0.00285\n",
      "\tWord: one, TF-IDF: 0.00284\n",
      "\tWord: often, TF-IDF: 0.00284\n",
      "\tWord: entir, TF-IDF: 0.00278\n",
      "\tWord: stori, TF-IDF: 0.00278\n",
      "\tWord: within, TF-IDF: 0.00278\n",
      "\tWord: oper, TF-IDF: 0.00276\n",
      "\tWord: want, TF-IDF: 0.00276\n",
      "\tWord: protect, TF-IDF: 0.00274\n",
      "\tWord: head, TF-IDF: 0.00272\n",
      "\tWord: debat, TF-IDF: 0.00271\n",
      "\tWord: idea, TF-IDF: 0.00271\n",
      "\tWord: kind, TF-IDF: 0.00269\n",
      "\tWord: bring, TF-IDF: 0.00267\n",
      "\tWord: new, TF-IDF: 0.00265\n",
      "\tWord: enough, TF-IDF: 0.00264\n",
      "\tWord: hard, TF-IDF: 0.00264\n",
      "\tWord: ever, TF-IDF: 0.00258\n",
      "\tWord: ad, TF-IDF: 0.00254\n",
      "\tWord: given, TF-IDF: 0.00254\n",
      "\tWord: play, TF-IDF: 0.00254\n",
      "\tWord: there, TF-IDF: 0.00253\n",
      "\tWord: result, TF-IDF: 0.00251\n",
      "\tWord: inform, TF-IDF: 0.00251\n",
      "\tWord: live, TF-IDF: 0.0025\n",
      "\tWord: current, TF-IDF: 0.0025\n",
      "\tWord: interest, TF-IDF: 0.00249\n",
      "\tWord: he, TF-IDF: 0.00247\n",
      "\tWord: feder, TF-IDF: 0.00245\n",
      "\tWord: better, TF-IDF: 0.00244\n",
      "\tWord: senat, TF-IDF: 0.00235\n",
      "\tWord: possibl, TF-IDF: 0.00227\n",
      "\tWord: allow, TF-IDF: 0.00226\n",
      "\tWord: place, TF-IDF: 0.00226\n",
      "\tWord: could, TF-IDF: 0.00225\n",
      "\tWord: secur, TF-IDF: 0.00224\n",
      "\tWord: close, TF-IDF: 0.00222\n",
      "\tWord: near, TF-IDF: 0.00221\n",
      "\tWord: find, TF-IDF: 0.00218\n",
      "\tWord: don't, TF-IDF: 0.00217\n",
      "\tWord: power, TF-IDF: 0.00214\n",
      "\tWord: keep, TF-IDF: 0.00213\n",
      "\tWord: start, TF-IDF: 0.00209\n",
      "\tWord: long, TF-IDF: 0.00208\n",
      "\tWord: good, TF-IDF: 0.00207\n",
      "\tWord: lead, TF-IDF: 0.00206\n",
      "\tWord: whether, TF-IDF: 0.00206\n",
      "\tWord: plan, TF-IDF: 0.00205\n",
      "\tWord: talk, TF-IDF: 0.00205\n",
      "\tWord: never, TF-IDF: 0.00203\n",
      "\tWord: believ, TF-IDF: 0.002\n",
      "\tWord: run, TF-IDF: 0.00194\n",
      "\tWord: continu, TF-IDF: 0.00194\n",
      "\tWord: well, TF-IDF: 0.00193\n",
      "\tWord: thing, TF-IDF: 0.00191\n",
      "\tWord: parti, TF-IDF: 0.00185\n",
      "\tWord: follow, TF-IDF: 0.00184\n",
      "\tWord: major, TF-IDF: 0.00182\n",
      "\tWord: public, TF-IDF: 0.00178\n",
      "\tWord: candid, TF-IDF: 0.00177\n",
      "\tWord: unit, TF-IDF: 0.00177\n",
      "\tWord: countri, TF-IDF: 0.00176\n",
      "\tWord: like, TF-IDF: 0.00172\n",
      "\tWord: news, TF-IDF: 0.00169\n",
      "\tWord: also, TF-IDF: 0.00167\n",
      "\tWord: look, TF-IDF: 0.00162\n",
      "\tWord: may, TF-IDF: 0.0015\n",
      "\tWord: trump, TF-IDF: 0.00147\n",
      "\tWord: back, TF-IDF: 0.00145\n",
      "\tWord: american, TF-IDF: 0.00133\n",
      "\tWord: last, TF-IDF: 0.00133\n",
      "\tWord: come, TF-IDF: 0.0013\n",
      "\tWord: call, TF-IDF: 0.00127\n",
      "\tWord: two, TF-IDF: 0.00124\n",
      "\tWord: take, TF-IDF: 0.00119\n",
      "\tWord: even, TF-IDF: 0.00107\n",
      "\tWord: would, TF-IDF: 0.00076\n"
     ]
    }
   ],
   "source": [
    "print(\"Top words in document 1\")\n",
    "sorted_words = sorted(tf_idf_manual[0].items(), key=lambda x: x[1], reverse=True)\n",
    "for word, score in sorted_words[:]:\n",
    "    print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we apply this to our former word2vec model trained on titleandtext corpus to see if there are improvements on its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Haorui Ji 2018-9-24\n",
    "# Develope doc vectors from word vectors by taking average of word vectors(Naive Doc2Vec)\n",
    "# Remove those with no text---deleteindex\n",
    "\n",
    "titleandtext_list = []\n",
    "idx = -1\n",
    "for review in clean_titleandtext:\n",
    "    idx += 1\n",
    "    init_vec = np.zeros([300,])\n",
    "    num_words = len(review)\n",
    "        \n",
    "    for word in review:\n",
    "        init_vec = init_vec + model.wv[word] * tf_idf_manual[idx][word]\n",
    "    init_vec /= num_words\n",
    "    titleandtext_list.append(init_vec)\n",
    "        \n",
    "titleandtext_array=np.asarray(titleandtext_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_f1_score: 0.82 (+/- 0.01) [LR]\n",
      "text_f1_score: 0.85 (+/- 0.00) [RF]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_f1_score: 0.88 (+/- 0.01) [XGB]\n",
      "text_f1_score: 0.67 (+/- 0.00) [SVM]\n"
     ]
    }
   ],
   "source": [
    "clf1=LogisticRegression()\n",
    "clf2=RandomForestClassifier()\n",
    "clf3=XGBClassifier()\n",
    "clf4=SVC(probability=True)\n",
    "\n",
    "for clf,label in zip([clf1,clf2,clf3,clf4],['LR','RF','XGB','SVM']):\n",
    "    scores=cross_validation.cross_val_score(clf,titleandtext_array,realnumber_y,scoring='f1')\n",
    "    print(\"text_f1_score: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result we can see that this consideration of multiplying tf-idf coefficient and wor2vec doesn't really work that well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Haorui Ji 2018-9-24\n",
    "# Develope doc vectors from word vectors by taking average of word vectors(Naive Doc2Vec)\n",
    "# Remove those with no text\n",
    "# Optional: whether to multiply tf-idf coefficient\n",
    "\n",
    "titleandtext_list = []\n",
    "idx = -1\n",
    "for review in clean_titleandtext:\n",
    "    idx += 1\n",
    "    init_vec = np.zeros([300,])\n",
    "    num_words = len(review)\n",
    "    for word in review:\n",
    "        init_vec += model.wv[word]\n",
    "    init_vec /= num_words\n",
    "    titleandtext_list.append(init_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6299, 300)\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Haorui Ji 2018-9-24\n",
    "\n",
    "w_titleandtext_array=np.asarray(titleandtext_list)\n",
    "print(w_titleandtext_array.shape)\n",
    "\n",
    "print(np.isnan(w_titleandtext_array).any())\n",
    "w_titleandtext_array[np.isnan(w_titleandtext_array)] = np.mean(w_titleandtext_array[~np.isnan(w_titleandtext_array)])\n",
    "print(np.isnan(w_titleandtext_array).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:531: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:535: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6299, 300)\n"
     ]
    }
   ],
   "source": [
    "# Haorui Ji 2018-9-24\n",
    "# take doc vectors by using d2v method trained on titleandtext corpus\n",
    "\n",
    "tagged_docs = [TaggedDocument(doc, tags=[idx]) for idx, doc in enumerate(clean_titleandtext)]\n",
    "pretrained_emb = 'GoogleNews-vectors-negative300.bin'\n",
    "doc2vec = Doc2Vec(tagged_docs,size=300, window=5, min_count=5, dm = 0.5, iter=10, pretrained_emb=pretrained_emb)\n",
    "doc2vec.train(tagged_docs, epochs=50, total_examples=doc2vec.corpus_count)\n",
    "\n",
    "new_list2=[]\n",
    "for i in range(doc2vec.docvecs.count):\n",
    "    new_list2.append(doc2vec.docvecs[i])\n",
    "    \n",
    "d_titleandtext_array=np.asarray(new_list2)\n",
    "print(d_titleandtext_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(17949 unique tokens: ['60%', 'abedin.', 'abus', 'accus', 'act']...)\n",
      "(6299, 300)\n"
     ]
    }
   ],
   "source": [
    "# Weihao Tan 2018-9-27\n",
    "# Generate tf-idf vector using gensim \n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(clean_titleandtext)\n",
    "\n",
    "small_freq_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < 10]\n",
    "dictionary.filter_tokens(small_freq_ids)\n",
    "dictionary.compactify()\n",
    "print(dictionary)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in clean_titleandtext]\n",
    "tfidf_model = models.TfidfModel(corpus=corpus,\n",
    "                                dictionary=dictionary)\n",
    "corpus_tfidf = [tfidf_model[doc] for doc in corpus]\n",
    "lsi_model = models.LsiModel(corpus = corpus_tfidf, \n",
    "                            id2word = dictionary, \n",
    "                            num_topics=300)\n",
    "corpus_lsi = [lsi_model[doc] for doc in corpus]\n",
    "#print(corpus_lsi[0])\n",
    "\n",
    "data = []\n",
    "rows = []\n",
    "cols = []\n",
    "line_count = 0\n",
    "for line in corpus_lsi:  # lsi_corpus_total 是之前由gensim生成的lsi向量\n",
    "    for elem in line:\n",
    "        rows.append(line_count)\n",
    "        cols.append(elem[0])\n",
    "        data.append(elem[1])\n",
    "    line_count += 1\n",
    "lsi_sparse_matrix = csr_matrix((data,(rows,cols))) # 稀疏向量\n",
    "lsi_matrix = lsi_sparse_matrix.toarray()  # 密集向量\n",
    "\n",
    "tf_titleandtext_array=np.asarray(lsi_matrix)\n",
    "print(tf_titleandtext_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6299, 900)\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# process the lael array and doc array made from d2v, so that all three arrays can have the same shape.\n",
    "\n",
    "feature_list=[]\n",
    "for word,doc,tf in zip(w_titleandtext_array,d_titleandtext_array, tf_titleandtext_array):\n",
    "    feature_list.append([*list(word),*list(doc), *list(tf)])\n",
    "    \n",
    "feature_array=np.asarray(feature_list)\n",
    "print(feature_array.shape)\n",
    "\n",
    "print(np.isnan(feature_array).any())\n",
    "feature_array[np.isnan(feature_array)] = np.mean(feature_array[~np.isnan(feature_array)])\n",
    "print(np.isnan(feature_array).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_f1_score: 0.93 (+/- 0.00) [LR]\n",
      "text_f1_score: 0.87 (+/- 0.01) [RF]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_f1_score: 0.91 (+/- 0.00) [XGB]\n",
      "text_f1_score: 0.94 (+/- 0.00) [SVM]\n"
     ]
    }
   ],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# test the concatenate vectors on four classifiers.\n",
    "\n",
    "clf1=LogisticRegression()\n",
    "clf2=RandomForestClassifier()\n",
    "clf3=XGBClassifier()\n",
    "clf4=SVC(probability=True)\n",
    "\n",
    "for clf,label in zip([clf1,clf2,clf3,clf4],['LR','RF','XGB','SVM']):\n",
    "    scores=cross_validation.cross_val_score(clf,feature_array,realnumber_y,scoring='f1')\n",
    "    print(\"text_f1_score: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6299, 900)\n",
      "(6299,)\n",
      "torch.Size([6299, 900])\n",
      "torch.Size([6299])\n"
     ]
    }
   ],
   "source": [
    "print(feature_array.shape)\n",
    "print(realnumber_y.shape)\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "\n",
    "feature_tensor = torch.from_numpy(feature_array).type(torch.FloatTensor)\n",
    "realnumber_y_tensor = torch.from_numpy(realnumber_y).type(torch.LongTensor)\n",
    "\n",
    "x, y = Variable(feature_tensor), Variable(realnumber_y_tensor)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden_1): Sequential(\n",
      "    (0): Linear(in_features=900, out_features=100, bias=True)\n",
      "    (1): Dropout(p=0.5)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (hidden_2): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=10, bias=True)\n",
      "    (1): Dropout(p=0.5)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (out): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden_1, n_hidden_2, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_1 = nn.Sequential(\n",
    "                                        torch.nn.Linear(n_feature, n_hidden_1),\n",
    "                                        torch.nn.Dropout(0.5),  # drop 50% of the neuron\n",
    "                                        torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.hidden_2 = nn.Sequential(\n",
    "                                        torch.nn.Linear(n_hidden_1, n_hidden_2),\n",
    "                                        torch.nn.Dropout(0.5),  # drop 50% of the neuron\n",
    "                                        torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.out = torch.nn.Linear(n_hidden_2, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_1(x)\n",
    "        x = self.hidden_2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "net = Net(n_feature=900, n_hidden_1=100, n_hidden_2=10, n_output=2)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=%.4f 0.50404826162883\n",
      "Accuracy=%.4f 0.8306080330211144\n",
      "Accuracy=%.4f 0.892998888712494\n",
      "Accuracy=%.4f 0.9028417209080807\n",
      "Accuracy=%.4f 0.9234799174472138\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    \n",
    "    out = net(x)\n",
    "    loss = loss_func(out, y)  # loss are defined as value before Softmax\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        # 过了一道 softmax 的激励函数后的最大概率才是预测值\n",
    "        # torch.max既返回某个维度上的最大值，同时返回该最大值的索引值\n",
    "        prediction = torch.max(F.softmax(out), 1)[1]  # 在第1维度取最大值并返回索引值\n",
    "        pred_y = prediction.data.numpy().squeeze()\n",
    "        target_y = y.data.numpy()\n",
    "        accuracy = sum(pred_y == target_y) / len(target_y)  # 预测中有多少和真实值一样\n",
    "        print('Accuracy=%.4f',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the results are so much better than only using text vectors, all the four classifiers we tested got a considerable amount of improvement which means this kind of combination is actually working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Ensemble Voting Classifier\n",
    "Now we have our desired vectors, we need to consider which classifier we need to use. Then  we realized in stead of just using one classifier, maybe using a voting classifier which can actually take several different classifiers' results into account and then give a final classification would be our best choice. And luckily we found that with some small tunes on the parameters, the Ensemble Classifier class we just created can do the trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to take those classifiers whose performance are in the top three list, by the previous result, these classifiers are clf1(Logistic Regression), clf3(XGBoost) and clf4(SVC). Since we only use vectors build from text, we passed all these three models as the first parameter to the EnsembleClassifier, and set all their weights to be 1. But Before that, we plan to use random grid search on each classifier to tune their hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# setting three sets of parameters for random grid search\n",
    "# random_grid1 for clf1,random_grid3 for clf3,random_grid4 for clf4\n",
    "\n",
    "c=np.linspace(1,50,200)\n",
    "random_grid1={'C':c}\n",
    "\n",
    "\n",
    "random_grid3 = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': np.linspace(0.1,5,100),\n",
    "        'max_depth': list(range(1,9,1)),\n",
    "        'n_estimators': list(range(200,2000,10))\n",
    "        }\n",
    "\n",
    "\n",
    "random_grid4= {'C':linspace(1,50,200),'gamma':linspace(0.001,1,200), 'kernel':['linear','rbf']}\n",
    "n_iter_search=100\n",
    "random_search1=RandomizedSearchCV(clf1,cv=5,param_distributions=random_grid1,n_iter=n_iter_search,return_train_score=True,n_jobs=6)\n",
    "random_search3=RandomizedSearchCV(clf3,cv=5,param_distributions=random_grid3,n_iter=n_iter_search,return_train_score=True,n_jobs=6)\n",
    "random_search4=RandomizedSearchCV(clf4,cv=5,param_distributions=random_grid4,n_iter=n_iter_search,return_train_score=True,n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# divide the whole corpus into three parts: validation set(25%), test set(25%) and training set(25%)\n",
    "test_size=0.25\n",
    "validation_size=0.25\n",
    "test_validation_size=test_size+validation_size\n",
    "test_size1=test_size/test_validation_size\n",
    "x_train,x_testandvalid,y_train,y_testandvalid=tts(wordanddoc_array,final_y,test_size=test_validation_size,random_state=22)\n",
    "x_valid,x_test,y_valid,y_test=tts(x_testandvalid,y_testandvalid,test_size=test_size1,random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=6,\n",
       "          param_distributions={'C': array([ 1.     ,  1.24623, ..., 49.75377, 50.     ])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search1.fit(x_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-ffabe7b8ef16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrandom_search3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    636\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    637\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 638\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_search3.fit(x_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_search4.fit(x_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# assign all three best estimators of three random gridsearch to clf1, clf3 and clf4\n",
    "# use the ensemble classifier to train and test on the training set and test set.\n",
    "# evaluate its performance.\n",
    "\n",
    "clf1=random_search1.best_estimator_\n",
    "clf3=random_search3.best_estimator_\n",
    "#clf4=random_search4.best_estimator_\n",
    "ultraclf=EnsembleClassifier(textclfs=[clf1,clf3,clf4],titleclfs=[],weights=[1,1,1])\n",
    "ultraclf.fit(x_train,x_train,y_train)\n",
    "prediction=ultraclf.predict(x_test,x_test)\n",
    "target_names=['FAKE','REAL']\n",
    "print(cr(y_test,prediction,target_names=target_names,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result given by the voting classifier is even better than every single one of its component classifier, it reaches an f-1 score of over 0.94 as shown above. Now as always, we'll present the confusion matrix of the result from our best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# make a function that can plot cofusion matrix\n",
    "#This is a compact version of the orginal function(The original function is here:http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm,classes,title='Confusion Matrix',cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm,interpolation='nearest',cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MengZe Zhang 2018-9-23\n",
    "# plot the confusion matrix\n",
    "\n",
    "%matplotlib inline\n",
    "cm=metrics.confusion_matrix(y_test,prediction,target_names)\n",
    "plot_confusion_matrix(cm,classes=target_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
