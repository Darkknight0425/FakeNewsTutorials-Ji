{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jerry', 'Tom']\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = ['Jerry','Tom','Lemon']\n",
    "label = np.array([1,1,0])\n",
    "label_list = label.tolist()\n",
    "idx = -1\n",
    "for text in a:\n",
    "    idx = idx + 1\n",
    "    if text == 'Lemon':\n",
    "        a.remove(text)\n",
    "        label_list.remove(label_list[idx])\n",
    "label = np.array(label_list)\n",
    "print(a)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(100,1000):\n",
    "    hundred = i/100\n",
    "    ten = (i-hundred*100) / 10\n",
    "    one = i % 10\n",
    "    if (i == hundred**2+ten**2+one**2):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['imag', 'hello', '2020'], ['look', 'forward', 'to', 'see', 'you']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk.stem\n",
    "from string import punctuation\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "punct = punctuation\n",
    "\n",
    "tokenized_list = [['imaging','hello',',','2020'],['looking','forward','to','seeing','you']]\n",
    "tokens = []\n",
    "for doc in tokenized_list:\n",
    "    tokens.append([token.lower() for token in doc if token.lower() not in punct])\n",
    "stemmed = []\n",
    "for doc in tokens:\n",
    "    stemmed.append([s.stem(token) for token in doc])\n",
    "print(stemmed)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['imag', 'hello', '2020'], ['look', 'forward', 'to', 'see', 'you']]\n",
      "['imag', 'hello', '2020', 'look', 'forward', 'to', 'see', 'you']\n",
      "[['imag_hello', 'hello_2020'], ['look_forward', 'forward_to', 'to_see', 'see_you']]\n",
      "['imag_hello', 'hello_2020', 'look_forward', 'forward_to', 'to_see', 'see_you']\n",
      "[['imag_hello_2020'], ['look_forward_to', 'forward_to_see', 'to_see_you']]\n",
      "['imag_hello_2020', 'look_forward_to', 'forward_to_see', 'to_see_you']\n"
     ]
    }
   ],
   "source": [
    "def getUnigram(words):\n",
    "    \"\"\"\n",
    "        Input: a list of words, e.g., ['I', 'am', 'Denny']\n",
    "        Output: a list of unigram\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    return words\n",
    "\n",
    "def getBigram(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., ['I', 'am', 'Denny']\n",
    "        Output: a list of bigram, e.g., ['I_am', 'am_Denny']\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "        #print words\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L-1):\n",
    "            for k in range(1,skip+2):\n",
    "                if i+k < L:\n",
    "                    lst.append( join_string.join([words[i], words[i+k]]) )\n",
    "    else:\n",
    "        # set it as unigram\n",
    "            lst = getUnigram(words)\n",
    "        #print 'lst returned'\n",
    "    return lst\n",
    "\n",
    "def getTrigram(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., ['I', 'am', 'Denny']\n",
    "       Output: a list of trigram, e.g., ['I_am_Denny']\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in range(L-2):\n",
    "            for k1 in range(1,skip+2):\n",
    "                for k2 in range(1,skip+2):\n",
    "                    if i+k1 < L and i+k1+k2 < L:\n",
    "                        lst.append( join_string.join([words[i], words[i+k1], words[i+k1+k2]]) )\n",
    "    else:\n",
    "        # set it as bigram\n",
    "        lst = getBigram(words, join_string, skip)\n",
    "    return lst\n",
    "\n",
    "join_str = \"_\"\n",
    "unigram = []\n",
    "unigram_corpus = []\n",
    "bigram = []\n",
    "bigram_corpus = []\n",
    "trigram = []\n",
    "trigram_corpus = []\n",
    "for doc in stemmed:\n",
    "    unigram.append(getUnigram(doc))\n",
    "    unigram_corpus.extend(getUnigram(doc))\n",
    "    bigram.append(getBigram(doc, join_str))\n",
    "    bigram_corpus.extend(getBigram(doc, join_str))\n",
    "    trigram.append(getTrigram(doc, join_str))\n",
    "    trigram_corpus.extend(getTrigram(doc, join_str))\n",
    "\n",
    "print(unigram)\n",
    "print(unigram_corpus)\n",
    "print(bigram)\n",
    "print(bigram_corpus)\n",
    "print(trigram)\n",
    "print(trigram_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grams = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "for gram in grams:\n",
    "        df[\"count_of_%s_%s\" % (feat_name, gram)] = list(df.apply(lambda x: len(x[feat_name + \"_\" + gram]), axis=1))\n",
    "        df[\"count_of_unique_%s_%s\" % (feat_name, gram)] = \\\n",
    "        list(df.apply(lambda x: len(set(x[feat_name + \"_\" + gram])), axis=1))\n",
    "        df[\"ratio_of_unique_%s_%s\" % (feat_name, gram)] = \\\n",
    "        map(try_divide, df[\"count_of_unique_%s_%s\"%(feat_name,gram)], df[\"count_of_%s_%s\"%(feat_name,gram)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-717cfc7aa949>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mvecB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvecB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munigram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvecB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(unigram_corpus)\n",
    "print(tfidf.shape)\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "vecB = TfidfVectorizer(vocabulary=vocabulary)\n",
    "tfidf = vecB.fit_transform(unigram)\n",
    "\n",
    "words = vecB.get_feature_names()\n",
    "print(words)\n",
    "for i in range(len(unigram_corpus)):\n",
    "    print('----Document %d----' % (i))\n",
    "    for j in range(len(words)):\n",
    "        if tfidf[i,j] > 1e-5:\n",
    "              print( words[j], tfidf[i,j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5 3.5 4.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "list = [[1,2,3],[4,5,6]]\n",
    "\n",
    "print(np.array(list).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0},\n",
       " {'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0},\n",
       " {'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0},\n",
       " {'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "def compute_sentiment(sentences):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        vs = sid.polarity_scores(sentence)\n",
    "        result.append(vs)\n",
    "    \n",
    "#    result_np = np.array(result).mean(axis=0)\n",
    "#    result = result_np.tolist()\n",
    "    return result\n",
    "\n",
    "compute_sentiment(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most automated sentiment analysis tools are shit.\n",
      "{'neg': 0.375, 'neu': 0.625, 'pos': 0.0, 'compound': -0.5574}\n",
      "compound: -0.5574, neg: 0.375, neu: 0.625, pos: 0.0, VADER sentiment analysis is the shit.\n",
      "{'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.6124}\n",
      "compound: 0.6124, neg: 0.0, neu: 0.556, pos: 0.444, Sentiment analysis has never been good.\n",
      "{'neg': 0.325, 'neu': 0.675, 'pos': 0.0, 'compound': -0.3412}\n",
      "compound: -0.3412, neg: 0.325, neu: 0.675, pos: 0.0, Sentiment analysis with VADER has never been this good.\n",
      "{'neg': 0.0, 'neu': 0.703, 'pos': 0.297, 'compound': 0.5228}\n",
      "compound: 0.5228, neg: 0.0, neu: 0.703, pos: 0.297, Warren Beatty has never been so entertaining.\n",
      "{'neg': 0.0, 'neu': 0.616, 'pos': 0.384, 'compound': 0.5777}\n",
      "compound: 0.5777, neg: 0.0, neu: 0.616, pos: 0.384, I won't say that the movie is astounding and I wouldn't claim that the movie is too banal either.\n",
      "{'neg': 0.0, 'neu': 0.851, 'pos': 0.149, 'compound': 0.4215}\n",
      "compound: 0.4215, neg: 0.0, neu: 0.851, pos: 0.149, "
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "\n",
    "sentences = []\n",
    "tricky_sentences = [\n",
    "\"Most automated sentiment analysis tools are shit.\",\n",
    "\"VADER sentiment analysis is the shit.\",\n",
    "\"Sentiment analysis has never been good.\",\n",
    "\"Sentiment analysis with VADER has never been this good.\",\n",
    "\"Warren Beatty has never been so entertaining.\",\n",
    "\"I won't say that the movie is astounding and I wouldn't claim that \\\n",
    "the movie is too banal either.\",\n",
    "]\n",
    "sentences.extend(tricky_sentences)\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    print(ss)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print ([0.]*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "list = [1,2,3,4]\n",
    "list.remove(list[0])\n",
    "print(list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'looking'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.stem.WordNetLemmatizer().lemmatize('looking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>title_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[ 1.1533764e-02  4.2144405e-03  1.9692603e-02 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[ 0.11267698  0.02518966 -0.00212591  0.021095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[ 0.04253004  0.04300297  0.01848392  0.048672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[ 0.10801624  0.11583211  0.02874823  0.061732...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[ 1.69016439e-02  7.13498285e-03 -7.81233795e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                       title_vectors  \n",
       "0  [ 1.1533764e-02  4.2144405e-03  1.9692603e-02 ...  \n",
       "1  [ 0.11267698  0.02518966 -0.00212591  0.021095...  \n",
       "2  [ 0.04253004  0.04300297  0.01848392  0.048672...  \n",
       "3  [ 0.10801624  0.11583211  0.02874823  0.061732...  \n",
       "4  [ 1.69016439e-02  7.13498285e-03 -7.81233795e-...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/fake_or_real_news.csv')\n",
    "title=np.asarray(df.title)\n",
    "text=np.asarray(df.text)\n",
    "y=np.asarray(df.label)\n",
    "title=[titles.lower().split() for titles in title]\n",
    "text=[texts.lower().split() for texts in text]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "1768\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5aeed5172f4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4526\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4526\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitleandtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4526\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "titleandtext = []\n",
    "for i in range(len(titleandtext)):\n",
    "    titleandtext[i].extend(title[i])\n",
    "    titleandtext[i].extend(text[i])\n",
    "\n",
    "print(len(title[4526]))\n",
    "print(len(text[4526]))\n",
    "print(len(titleandtext[4526]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example review:\n",
      "   Raw: Daniel Greenfield, a Shillman Journalism Fellow at the Freedom Center, is a New York writer focusing on radical Islam. \r\n",
      "In the final stretch of the election, Hillary Rodham Clinton has gone to war with the FBI. \r\n",
      "The word “unprecedented” has been thrown around so often this election that it ought to be retired. But it’s still unprecedented for the nominee of a major political party to go war with the FBI. \r\n",
      "But that’s exactly what Hillary and her people have done. Coma patients just waking up now and watching an hour of CNN from their hospital beds would assume that FBI Director James Comey is Hillary’s opponent in this election. \r\n",
      "The FBI is under attack by everyone from Obama to CNN. Hillary’s people have circulated a letter attacking Comey. There are currently more media hit pieces lambasting him than targeting Trump. It wouldn’t be too surprising if the Clintons or their allies were to start running attack ads against the FBI. \r\n",
      "The FBI’s leadership is being warned that the entire left-wing establishment will form a lynch mob if they continue going after Hillary. And the FBI’s credibility is being attacked by the media and the Democrats to preemptively head off the results of the investigation of the Clinton Foundation and Hillary Clinton. \r\n",
      "The covert struggle between FBI agents and Obama’s DOJ people has gone explosively public. \r\n",
      "The New York Times has compared Comey to J. Edgar Hoover. Its bizarre headline, “James Comey Role Recalls Hoover’s FBI, Fairly or Not” practically admits up front that it’s spouting nonsense. The Boston Globe has published a column calling for Comey’s resignation. Not to be outdone, Time has an editorial claiming that the scandal is really an attack on all women. \r\n",
      "James Carville appeared on MSNBC to remind everyone that he was still alive and insane. He accused Comey of coordinating with House Republicans and the KGB. And you thought the “vast right wing conspiracy” was a stretch. \r\n",
      "Countless media stories charge Comey with violating procedure. Do you know what’s a procedural violation? Emailing classified information stored on your bathroom server. \r\n",
      "Senator Harry Reid has sent Comey a letter accusing him of violating the Hatch Act. The Hatch Act is a nice idea that has as much relevance in the age of Obama as the Tenth Amendment. But the cable news spectrum quickly filled with media hacks glancing at the Wikipedia article on the Hatch Act under the table while accusing the FBI director of one of the most awkward conspiracies against Hillary ever. \r\n",
      "If James Comey is really out to hurt Hillary, he picked one hell of a strange way to do it. \r\n",
      "Not too long ago Democrats were breathing a sigh of relief when he gave Hillary Clinton a pass in a prominent public statement. If he really were out to elect Trump by keeping the email scandal going, why did he trash the investigation? Was he on the payroll of House Republicans and the KGB back then and playing it coy or was it a sudden development where Vladimir Putin and Paul Ryan talked him into taking a look at Anthony Weiner’s computer? \r\n",
      "Either Comey is the most cunning FBI director that ever lived or he’s just awkwardly trying to navigate a political mess that has trapped him between a DOJ leadership whose political futures are tied to Hillary’s victory and his own bureau whose apolitical agents just want to be allowed to do their jobs. \r\n",
      "The only truly mysterious thing is why Hillary and her associates decided to go to war with a respected Federal agency. Most Americans like the FBI while Hillary Clinton enjoys a 60% unfavorable rating. \r\n",
      "And it’s an interesting question. \r\n",
      "Hillary’s old strategy was to lie and deny that the FBI even had a criminal investigation underway. Instead her associates insisted that it was a security review. The FBI corrected her and she shrugged it off. But the old breezy denial approach has given way to a savage assault on the FBI. \r\n",
      "Pretending that nothing was wrong was a bad strategy, but it was a better one that picking a fight with the FBI while lunatic Clinton associates try to claim that the FBI is really the KGB. \r\n",
      "There are two possible explanations. \r\n",
      "Hillary Clinton might be arrogant enough to lash out at the FBI now that she believes that victory is near. The same kind of hubris that led her to plan her victory fireworks display could lead her to declare a war on the FBI for irritating her during the final miles of her campaign. \r\n",
      "But the other explanation is that her people panicked. \r\n",
      "Going to war with the FBI is not the behavior of a smart and focused presidential campaign. It’s an act of desperation. When a presidential candidate decides that her only option is to try and destroy the credibility of the FBI, that’s not hubris, it’s fear of what the FBI might be about to reveal about her. \r\n",
      "During the original FBI investigation, Hillary Clinton was confident that she could ride it out. And she had good reason for believing that. But that Hillary Clinton is gone. In her place is a paranoid wreck. Within a short space of time the “positive” Clinton campaign promising to unite the country has been replaced by a desperate and flailing operation that has focused all its energy on fighting the FBI. \r\n",
      "There’s only one reason for such bizarre behavior. \r\n",
      "The Clinton campaign has decided that an FBI investigation of the latest batch of emails poses a threat to its survival. And so it’s gone all in on fighting the FBI. It’s an unprecedented step born of fear. It’s hard to know whether that fear is justified. But the existence of that fear already tells us a whole lot. \r\n",
      "Clinton loyalists rigged the old investigation. They knew the outcome ahead of time as well as they knew the debate questions. Now suddenly they are no longer in control. And they are afraid. \r\n",
      "You can smell the fear. \r\n",
      "The FBI has wiretaps from the investigation of the Clinton Foundation. It’s finding new emails all the time. And Clintonworld panicked. The spinmeisters of Clintonworld have claimed that the email scandal is just so much smoke without fire. All that’s here is the appearance of impropriety without any of the substance. But this isn’t how you react to smoke. It’s how you respond to a fire. \r\n",
      "The misguided assault on the FBI tells us that Hillary Clinton and her allies are afraid of a revelation bigger than the fundamental illegality of her email setup. The email setup was a preemptive cover up. The Clinton campaign has panicked badly out of the belief, right or wrong, that whatever crime the illegal setup was meant to cover up is at risk of being exposed. \r\n",
      "The Clintons have weathered countless scandals over the years. Whatever they are protecting this time around is bigger than the usual corruption, bribery, sexual assaults and abuses of power that have followed them around throughout the years. This is bigger and more damaging than any of the allegations that have already come out. And they don’t want FBI investigators anywhere near it. \r\n",
      "The campaign against Comey is pure intimidation. It’s also a warning. Any senior FBI people who value their careers are being warned to stay away. The Democrats are closing ranks around their nominee against the FBI. It’s an ugly and unprecedented scene. It may also be their last stand. \r\n",
      "Hillary Clinton has awkwardly wound her way through numerous scandals in just this election cycle. But she’s never shown fear or desperation before. Now that has changed. Whatever she is afraid of, it lies buried in her emails with Huma Abedin. And it can bring her down like nothing else has.   \n",
      "\n",
      "   Tokenized: ['Daniel', 'Greenfield', ',', 'a', 'Shillman', 'Journalism', 'Fellow', 'at', 'the', 'Freedom', 'Center', ',', 'is', 'a', 'New', 'York', 'writer', 'focusing', 'on', 'radical', 'Islam', '.', 'In', 'the', 'final', 'stretch', 'of', 'the', 'election', ',', 'Hillary', 'Rodham', 'Clinton', 'has', 'gone', 'to', 'war', 'with', 'the', 'FBI', '.', 'The', 'word', '“', 'unprecedented', '”', 'has', 'been', 'thrown', 'around', 'so', 'often', 'this', 'election', 'that', 'it', 'ought', 'to', 'be', 'retired', '.', 'But', 'it', '’', 's', 'still', 'unprecedented', 'for', 'the', 'nominee', 'of', 'a', 'major', 'political', 'party', 'to', 'go', 'war', 'with', 'the', 'FBI', '.', 'But', 'that', '’', 's', 'exactly', 'what', 'Hillary', 'and', 'her', 'people', 'have', 'done', '.', 'Coma', 'patients', 'just', 'waking', 'up', 'now', 'and', 'watching', 'an', 'hour', 'of', 'CNN', 'from', 'their', 'hospital', 'beds', 'would', 'assume', 'that', 'FBI', 'Director', 'James', 'Comey', 'is', 'Hillary', '’', 's', 'opponent', 'in', 'this', 'election', '.', 'The', 'FBI', 'is', 'under', 'attack', 'by', 'everyone', 'from', 'Obama', 'to', 'CNN', '.', 'Hillary', '’', 's', 'people', 'have', 'circulated', 'a', 'letter', 'attacking', 'Comey', '.', 'There', 'are', 'currently', 'more', 'media', 'hit', 'pieces', 'lambasting', 'him', 'than', 'targeting', 'Trump', '.', 'It', 'wouldn', '’', 't', 'be', 'too', 'surprising', 'if', 'the', 'Clintons', 'or', 'their', 'allies', 'were', 'to', 'start', 'running', 'attack', 'ads', 'against', 'the', 'FBI', '.', 'The', 'FBI', '’', 's', 'leadership', 'is', 'being', 'warned', 'that', 'the', 'entire', 'left-wing', 'establishment', 'will', 'form', 'a', 'lynch', 'mob', 'if', 'they', 'continue', 'going', 'after', 'Hillary', '.', 'And', 'the', 'FBI', '’', 's', 'credibility', 'is', 'being', 'attacked', 'by', 'the', 'media', 'and', 'the', 'Democrats', 'to', 'preemptively', 'head', 'off', 'the', 'results', 'of', 'the', 'investigation', 'of', 'the', 'Clinton', 'Foundation', 'and', 'Hillary', 'Clinton', '.', 'The', 'covert', 'struggle', 'between', 'FBI', 'agents', 'and', 'Obama', '’', 's', 'DOJ', 'people', 'has', 'gone', 'explosively', 'public', '.', 'The', 'New', 'York', 'Times', 'has', 'compared', 'Comey', 'to', 'J.', 'Edgar', 'Hoover', '.', 'Its', 'bizarre', 'headline', ',', '“', 'James', 'Comey', 'Role', 'Recalls', 'Hoover', '’', 's', 'FBI', ',', 'Fairly', 'or', 'Not', '”', 'practically', 'admits', 'up', 'front', 'that', 'it', '’', 's', 'spouting', 'nonsense', '.', 'The', 'Boston', 'Globe', 'has', 'published', 'a', 'column', 'calling', 'for', 'Comey', '’', 's', 'resignation', '.', 'Not', 'to', 'be', 'outdone', ',', 'Time', 'has', 'an', 'editorial', 'claiming', 'that', 'the', 'scandal', 'is', 'really', 'an', 'attack', 'on', 'all', 'women', '.', 'James', 'Carville', 'appeared', 'on', 'MSNBC', 'to', 'remind', 'everyone', 'that', 'he', 'was', 'still', 'alive', 'and', 'insane', '.', 'He', 'accused', 'Comey', 'of', 'coordinating', 'with', 'House', 'Republicans', 'and', 'the', 'KGB', '.', 'And', 'you', 'thought', 'the', '“', 'vast', 'right', 'wing', 'conspiracy', '”', 'was', 'a', 'stretch', '.', 'Countless', 'media', 'stories', 'charge', 'Comey', 'with', 'violating', 'procedure', '.', 'Do', 'you', 'know', 'what', '’', 's', 'a', 'procedural', 'violation', '?', 'Emailing', 'classified', 'information', 'stored', 'on', 'your', 'bathroom', 'server', '.', 'Senator', 'Harry', 'Reid', 'has', 'sent', 'Comey', 'a', 'letter', 'accusing', 'him', 'of', 'violating', 'the', 'Hatch', 'Act', '.', 'The', 'Hatch', 'Act', 'is', 'a', 'nice', 'idea', 'that', 'has', 'as', 'much', 'relevance', 'in', 'the', 'age', 'of', 'Obama', 'as', 'the', 'Tenth', 'Amendment', '.', 'But', 'the', 'cable', 'news', 'spectrum', 'quickly', 'filled', 'with', 'media', 'hacks', 'glancing', 'at', 'the', 'Wikipedia', 'article', 'on', 'the', 'Hatch', 'Act', 'under', 'the', 'table', 'while', 'accusing', 'the', 'FBI', 'director', 'of', 'one', 'of', 'the', 'most', 'awkward', 'conspiracies', 'against', 'Hillary', 'ever', '.', 'If', 'James', 'Comey', 'is', 'really', 'out', 'to', 'hurt', 'Hillary', ',', 'he', 'picked', 'one', 'hell', 'of', 'a', 'strange', 'way', 'to', 'do', 'it', '.', 'Not', 'too', 'long', 'ago', 'Democrats', 'were', 'breathing', 'a', 'sigh', 'of', 'relief', 'when', 'he', 'gave', 'Hillary', 'Clinton', 'a', 'pass', 'in', 'a', 'prominent', 'public', 'statement', '.', 'If', 'he', 'really', 'were', 'out', 'to', 'elect', 'Trump', 'by', 'keeping', 'the', 'email', 'scandal', 'going', ',', 'why', 'did', 'he', 'trash', 'the', 'investigation', '?', 'Was', 'he', 'on', 'the', 'payroll', 'of', 'House', 'Republicans', 'and', 'the', 'KGB', 'back', 'then', 'and', 'playing', 'it', 'coy', 'or', 'was', 'it', 'a', 'sudden', 'development', 'where', 'Vladimir', 'Putin', 'and', 'Paul', 'Ryan', 'talked', 'him', 'into', 'taking', 'a', 'look', 'at', 'Anthony', 'Weiner', '’', 's', 'computer', '?', 'Either', 'Comey', 'is', 'the', 'most', 'cunning', 'FBI', 'director', 'that', 'ever', 'lived', 'or', 'he', '’', 's', 'just', 'awkwardly', 'trying', 'to', 'navigate', 'a', 'political', 'mess', 'that', 'has', 'trapped', 'him', 'between', 'a', 'DOJ', 'leadership', 'whose', 'political', 'futures', 'are', 'tied', 'to', 'Hillary', '’', 's', 'victory', 'and', 'his', 'own', 'bureau', 'whose', 'apolitical', 'agents', 'just', 'want', 'to', 'be', 'allowed', 'to', 'do', 'their', 'jobs', '.', 'The', 'only', 'truly', 'mysterious', 'thing', 'is', 'why', 'Hillary', 'and', 'her', 'associates', 'decided', 'to', 'go', 'to', 'war', 'with', 'a', 'respected', 'Federal', 'agency', '.', 'Most', 'Americans', 'like', 'the', 'FBI', 'while', 'Hillary', 'Clinton', 'enjoys', 'a', '60', '%', 'unfavorable', 'rating', '.', 'And', 'it', '’', 's', 'an', 'interesting', 'question', '.', 'Hillary', '’', 's', 'old', 'strategy', 'was', 'to', 'lie', 'and', 'deny', 'that', 'the', 'FBI', 'even', 'had', 'a', 'criminal', 'investigation', 'underway', '.', 'Instead', 'her', 'associates', 'insisted', 'that', 'it', 'was', 'a', 'security', 'review', '.', 'The', 'FBI', 'corrected', 'her', 'and', 'she', 'shrugged', 'it', 'off', '.', 'But', 'the', 'old', 'breezy', 'denial', 'approach', 'has', 'given', 'way', 'to', 'a', 'savage', 'assault', 'on', 'the', 'FBI', '.', 'Pretending', 'that', 'nothing', 'was', 'wrong', 'was', 'a', 'bad', 'strategy', ',', 'but', 'it', 'was', 'a', 'better', 'one', 'that', 'picking', 'a', 'fight', 'with', 'the', 'FBI', 'while', 'lunatic', 'Clinton', 'associates', 'try', 'to', 'claim', 'that', 'the', 'FBI', 'is', 'really', 'the', 'KGB', '.', 'There', 'are', 'two', 'possible', 'explanations', '.', 'Hillary', 'Clinton', 'might', 'be', 'arrogant', 'enough', 'to', 'lash', 'out', 'at', 'the', 'FBI', 'now', 'that', 'she', 'believes', 'that', 'victory', 'is', 'near', '.', 'The', 'same', 'kind', 'of', 'hubris', 'that', 'led', 'her', 'to', 'plan', 'her', 'victory', 'fireworks', 'display', 'could', 'lead', 'her', 'to', 'declare', 'a', 'war', 'on', 'the', 'FBI', 'for', 'irritating', 'her', 'during', 'the', 'final', 'miles', 'of', 'her', 'campaign', '.', 'But', 'the', 'other', 'explanation', 'is', 'that', 'her', 'people', 'panicked', '.', 'Going', 'to', 'war', 'with', 'the', 'FBI', 'is', 'not', 'the', 'behavior', 'of', 'a', 'smart', 'and', 'focused', 'presidential', 'campaign', '.', 'It', '’', 's', 'an', 'act', 'of', 'desperation', '.', 'When', 'a', 'presidential', 'candidate', 'decides', 'that', 'her', 'only', 'option', 'is', 'to', 'try', 'and', 'destroy', 'the', 'credibility', 'of', 'the', 'FBI', ',', 'that', '’', 's', 'not', 'hubris', ',', 'it', '’', 's', 'fear', 'of', 'what', 'the', 'FBI', 'might', 'be', 'about', 'to', 'reveal', 'about', 'her', '.', 'During', 'the', 'original', 'FBI', 'investigation', ',', 'Hillary', 'Clinton', 'was', 'confident', 'that', 'she', 'could', 'ride', 'it', 'out', '.', 'And', 'she', 'had', 'good', 'reason', 'for', 'believing', 'that', '.', 'But', 'that', 'Hillary', 'Clinton', 'is', 'gone', '.', 'In', 'her', 'place', 'is', 'a', 'paranoid', 'wreck', '.', 'Within', 'a', 'short', 'space', 'of', 'time', 'the', '“', 'positive', '”', 'Clinton', 'campaign', 'promising', 'to', 'unite', 'the', 'country', 'has', 'been', 'replaced', 'by', 'a', 'desperate', 'and', 'flailing', 'operation', 'that', 'has', 'focused', 'all', 'its', 'energy', 'on', 'fighting', 'the', 'FBI', '.', 'There', '’', 's', 'only', 'one', 'reason', 'for', 'such', 'bizarre', 'behavior', '.', 'The', 'Clinton', 'campaign', 'has', 'decided', 'that', 'an', 'FBI', 'investigation', 'of', 'the', 'latest', 'batch', 'of', 'emails', 'poses', 'a', 'threat', 'to', 'its', 'survival', '.', 'And', 'so', 'it', '’', 's', 'gone', 'all', 'in', 'on', 'fighting', 'the', 'FBI', '.', 'It', '’', 's', 'an', 'unprecedented', 'step', 'born', 'of', 'fear', '.', 'It', '’', 's', 'hard', 'to', 'know', 'whether', 'that', 'fear', 'is', 'justified', '.', 'But', 'the', 'existence', 'of', 'that', 'fear', 'already', 'tells', 'us', 'a', 'whole', 'lot', '.', 'Clinton', 'loyalists', 'rigged', 'the', 'old', 'investigation', '.', 'They', 'knew', 'the', 'outcome', 'ahead', 'of', 'time', 'as', 'well', 'as', 'they', 'knew', 'the', 'debate', 'questions', '.', 'Now', 'suddenly', 'they', 'are', 'no', 'longer', 'in', 'control', '.', 'And', 'they', 'are', 'afraid', '.', 'You', 'can', 'smell', 'the', 'fear', '.', 'The', 'FBI', 'has', 'wiretaps', 'from', 'the', 'investigation', 'of', 'the', 'Clinton', 'Foundation', '.', 'It', '’', 's', 'finding', 'new', 'emails', 'all', 'the', 'time', '.', 'And', 'Clintonworld', 'panicked', '.', 'The', 'spinmeisters', 'of', 'Clintonworld', 'have', 'claimed', 'that', 'the', 'email', 'scandal', 'is', 'just', 'so', 'much', 'smoke', 'without', 'fire', '.', 'All', 'that', '’', 's', 'here', 'is', 'the', 'appearance', 'of', 'impropriety', 'without', 'any', 'of', 'the', 'substance', '.', 'But', 'this', 'isn', '’', 't', 'how', 'you', 'react', 'to', 'smoke', '.', 'It', '’', 's', 'how', 'you', 'respond', 'to', 'a', 'fire', '.', 'The', 'misguided', 'assault', 'on', 'the', 'FBI', 'tells', 'us', 'that', 'Hillary', 'Clinton', 'and', 'her', 'allies', 'are', 'afraid', 'of', 'a', 'revelation', 'bigger', 'than', 'the', 'fundamental', 'illegality', 'of', 'her', 'email', 'setup', '.', 'The', 'email', 'setup', 'was', 'a', 'preemptive', 'cover', 'up', '.', 'The', 'Clinton', 'campaign', 'has', 'panicked', 'badly', 'out', 'of', 'the', 'belief', ',', 'right', 'or', 'wrong', ',', 'that', 'whatever', 'crime', 'the', 'illegal', 'setup', 'was', 'meant', 'to', 'cover', 'up', 'is', 'at', 'risk', 'of', 'being', 'exposed', '.', 'The', 'Clintons', 'have', 'weathered', 'countless', 'scandals', 'over', 'the', 'years', '.', 'Whatever', 'they', 'are', 'protecting', 'this', 'time', 'around', 'is', 'bigger', 'than', 'the', 'usual', 'corruption', ',', 'bribery', ',', 'sexual', 'assaults', 'and', 'abuses', 'of', 'power', 'that', 'have', 'followed', 'them', 'around', 'throughout', 'the', 'years', '.', 'This', 'is', 'bigger', 'and', 'more', 'damaging', 'than', 'any', 'of', 'the', 'allegations', 'that', 'have', 'already', 'come', 'out', '.', 'And', 'they', 'don', '’', 't', 'want', 'FBI', 'investigators', 'anywhere', 'near', 'it', '.', 'The', 'campaign', 'against', 'Comey', 'is', 'pure', 'intimidation', '.', 'It', '’', 's', 'also', 'a', 'warning', '.', 'Any', 'senior', 'FBI', 'people', 'who', 'value', 'their', 'careers', 'are', 'being', 'warned', 'to', 'stay', 'away', '.', 'The', 'Democrats', 'are', 'closing', 'ranks', 'around', 'their', 'nominee', 'against', 'the', 'FBI', '.', 'It', '’', 's', 'an', 'ugly', 'and', 'unprecedented', 'scene', '.', 'It', 'may', 'also', 'be', 'their', 'last', 'stand', '.', 'Hillary', 'Clinton', 'has', 'awkwardly', 'wound', 'her', 'way', 'through', 'numerous', 'scandals', 'in', 'just', 'this', 'election', 'cycle', '.', 'But', 'she', '’', 's', 'never', 'shown', 'fear', 'or', 'desperation', 'before', '.', 'Now', 'that', 'has', 'changed', '.', 'Whatever', 'she', 'is', 'afraid', 'of', ',', 'it', 'lies', 'buried', 'in', 'her', 'emails', 'with', 'Huma', 'Abedin', '.', 'And', 'it', 'can', 'bring', 'her', 'down', 'like', 'nothing', 'else', 'has', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Transform each review string as a list of token strings. May take a few seconds\n",
    "text_token = [nltk.word_tokenize(text) for text in texts]\n",
    "\n",
    "n = 0 #arbitrary pick\n",
    "print('Example review:\\n   Raw: {} \\n\\n   Tokenized: {}'.format(texts[n], [i for i in text_token[n]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many texts are empty: 35\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from itertools import chain\n",
    "\n",
    "def clean_text(tokenized_list, sw, punct, lemmatize=True):\n",
    "    new_list = []\n",
    "    for doc in tokenized_list:\n",
    "        new_list.append([token.lower() for token in doc if token.lower() not in chain(punct, sw)])\n",
    "    return new_list\n",
    "\n",
    "# Remove punctuations and stopwords, and lower-case text\n",
    "sw = stopwords.words('english')\n",
    "punct = punctuation\n",
    "text_cleaned = clean_text(text_token, sw, punct)\n",
    "\n",
    "idx = -1\n",
    "count = 0\n",
    "label_list = labels.tolist()\n",
    "for text in text_cleaned:\n",
    "    idx = idx + 1\n",
    "    if len(text) == 0:\n",
    "        count = count + 1\n",
    "        text_cleaned.remove(text)\n",
    "        label_list.remove(label_list[idx])\n",
    "        idx = idx - 1\n",
    "labels = np.array(label_list)\n",
    "\n",
    "print('how many texts are empty:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:531: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_docs:\n",
      " ['daniel', 'greenfield', 'shillman', 'journalism', 'fellow', 'freedom', 'center', 'new', 'york', 'writer', 'focusing', 'radical', 'islam', 'final', 'stretch', 'election', 'hillary', 'rodham', 'clinton', 'gone', 'war', 'fbi', 'word', '“', 'unprecedented', '”', 'thrown', 'around', 'often', 'election', 'ought', 'retired', '’', 'still', 'unprecedented', 'nominee', 'major', 'political', 'party', 'go', 'war', 'fbi', '’', 'exactly', 'hillary', 'people', 'done', 'coma', 'patients', 'waking', 'watching', 'hour', 'cnn', 'hospital', 'beds', 'would', 'assume', 'fbi', 'director', 'james', 'comey', 'hillary', '’', 'opponent', 'election', 'fbi', 'attack', 'everyone', 'obama', 'cnn', 'hillary', '’', 'people', 'circulated', 'letter', 'attacking', 'comey', 'currently', 'media', 'hit', 'pieces', 'lambasting', 'targeting', 'trump', '’', 'surprising', 'clintons', 'allies', 'start', 'running', 'attack', 'ads', 'fbi', 'fbi', '’', 'leadership', 'warned', 'entire', 'left-wing', 'establishment', 'form', 'lynch', 'mob', 'continue', 'going', 'hillary', 'fbi', '’', 'credibility', 'attacked', 'media', 'democrats', 'preemptively', 'head', 'results', 'investigation', 'clinton', 'foundation', 'hillary', 'clinton', 'covert', 'struggle', 'fbi', 'agents', 'obama', '’', 'doj', 'people', 'gone', 'explosively', 'public', 'new', 'york', 'times', 'compared', 'comey', 'j.', 'edgar', 'hoover', 'bizarre', 'headline', '“', 'james', 'comey', 'role', 'recalls', 'hoover', '’', 'fbi', 'fairly', '”', 'practically', 'admits', 'front', '’', 'spouting', 'nonsense', 'boston', 'globe', 'published', 'column', 'calling', 'comey', '’', 'resignation', 'outdone', 'time', 'editorial', 'claiming', 'scandal', 'really', 'attack', 'women', 'james', 'carville', 'appeared', 'msnbc', 'remind', 'everyone', 'still', 'alive', 'insane', 'accused', 'comey', 'coordinating', 'house', 'republicans', 'kgb', 'thought', '“', 'vast', 'right', 'wing', 'conspiracy', '”', 'stretch', 'countless', 'media', 'stories', 'charge', 'comey', 'violating', 'procedure', 'know', '’', 'procedural', 'violation', 'emailing', 'classified', 'information', 'stored', 'bathroom', 'server', 'senator', 'harry', 'reid', 'sent', 'comey', 'letter', 'accusing', 'violating', 'hatch', 'act', 'hatch', 'act', 'nice', 'idea', 'much', 'relevance', 'age', 'obama', 'tenth', 'amendment', 'cable', 'news', 'spectrum', 'quickly', 'filled', 'media', 'hacks', 'glancing', 'wikipedia', 'article', 'hatch', 'act', 'table', 'accusing', 'fbi', 'director', 'one', 'awkward', 'conspiracies', 'hillary', 'ever', 'james', 'comey', 'really', 'hurt', 'hillary', 'picked', 'one', 'hell', 'strange', 'way', 'long', 'ago', 'democrats', 'breathing', 'sigh', 'relief', 'gave', 'hillary', 'clinton', 'pass', 'prominent', 'public', 'statement', 'really', 'elect', 'trump', 'keeping', 'email', 'scandal', 'going', 'trash', 'investigation', 'payroll', 'house', 'republicans', 'kgb', 'back', 'playing', 'coy', 'sudden', 'development', 'vladimir', 'putin', 'paul', 'ryan', 'talked', 'taking', 'look', 'anthony', 'weiner', '’', 'computer', 'either', 'comey', 'cunning', 'fbi', 'director', 'ever', 'lived', '’', 'awkwardly', 'trying', 'navigate', 'political', 'mess', 'trapped', 'doj', 'leadership', 'whose', 'political', 'futures', 'tied', 'hillary', '’', 'victory', 'bureau', 'whose', 'apolitical', 'agents', 'want', 'allowed', 'jobs', 'truly', 'mysterious', 'thing', 'hillary', 'associates', 'decided', 'go', 'war', 'respected', 'federal', 'agency', 'americans', 'like', 'fbi', 'hillary', 'clinton', 'enjoys', '60', 'unfavorable', 'rating', '’', 'interesting', 'question', 'hillary', '’', 'old', 'strategy', 'lie', 'deny', 'fbi', 'even', 'criminal', 'investigation', 'underway', 'instead', 'associates', 'insisted', 'security', 'review', 'fbi', 'corrected', 'shrugged', 'old', 'breezy', 'denial', 'approach', 'given', 'way', 'savage', 'assault', 'fbi', 'pretending', 'nothing', 'wrong', 'bad', 'strategy', 'better', 'one', 'picking', 'fight', 'fbi', 'lunatic', 'clinton', 'associates', 'try', 'claim', 'fbi', 'really', 'kgb', 'two', 'possible', 'explanations', 'hillary', 'clinton', 'might', 'arrogant', 'enough', 'lash', 'fbi', 'believes', 'victory', 'near', 'kind', 'hubris', 'led', 'plan', 'victory', 'fireworks', 'display', 'could', 'lead', 'declare', 'war', 'fbi', 'irritating', 'final', 'miles', 'campaign', 'explanation', 'people', 'panicked', 'going', 'war', 'fbi', 'behavior', 'smart', 'focused', 'presidential', 'campaign', '’', 'act', 'desperation', 'presidential', 'candidate', 'decides', 'option', 'try', 'destroy', 'credibility', 'fbi', '’', 'hubris', '’', 'fear', 'fbi', 'might', 'reveal', 'original', 'fbi', 'investigation', 'hillary', 'clinton', 'confident', 'could', 'ride', 'good', 'reason', 'believing', 'hillary', 'clinton', 'gone', 'place', 'paranoid', 'wreck', 'within', 'short', 'space', 'time', '“', 'positive', '”', 'clinton', 'campaign', 'promising', 'unite', 'country', 'replaced', 'desperate', 'flailing', 'operation', 'focused', 'energy', 'fighting', 'fbi', '’', 'one', 'reason', 'bizarre', 'behavior', 'clinton', 'campaign', 'decided', 'fbi', 'investigation', 'latest', 'batch', 'emails', 'poses', 'threat', 'survival', '’', 'gone', 'fighting', 'fbi', '’', 'unprecedented', 'step', 'born', 'fear', '’', 'hard', 'know', 'whether', 'fear', 'justified', 'existence', 'fear', 'already', 'tells', 'us', 'whole', 'lot', 'clinton', 'loyalists', 'rigged', 'old', 'investigation', 'knew', 'outcome', 'ahead', 'time', 'well', 'knew', 'debate', 'questions', 'suddenly', 'longer', 'control', 'afraid', 'smell', 'fear', 'fbi', 'wiretaps', 'investigation', 'clinton', 'foundation', '’', 'finding', 'new', 'emails', 'time', 'clintonworld', 'panicked', 'spinmeisters', 'clintonworld', 'claimed', 'email', 'scandal', 'much', 'smoke', 'without', 'fire', '’', 'appearance', 'impropriety', 'without', 'substance', '’', 'react', 'smoke', '’', 'respond', 'fire', 'misguided', 'assault', 'fbi', 'tells', 'us', 'hillary', 'clinton', 'allies', 'afraid', 'revelation', 'bigger', 'fundamental', 'illegality', 'email', 'setup', 'email', 'setup', 'preemptive', 'cover', 'clinton', 'campaign', 'panicked', 'badly', 'belief', 'right', 'wrong', 'whatever', 'crime', 'illegal', 'setup', 'meant', 'cover', 'risk', 'exposed', 'clintons', 'weathered', 'countless', 'scandals', 'years', 'whatever', 'protecting', 'time', 'around', 'bigger', 'usual', 'corruption', 'bribery', 'sexual', 'assaults', 'abuses', 'power', 'followed', 'around', 'throughout', 'years', 'bigger', 'damaging', 'allegations', 'already', 'come', '’', 'want', 'fbi', 'investigators', 'anywhere', 'near', 'campaign', 'comey', 'pure', 'intimidation', '’', 'also', 'warning', 'senior', 'fbi', 'people', 'value', 'careers', 'warned', 'stay', 'away', 'democrats', 'closing', 'ranks', 'around', 'nominee', 'fbi', '’', 'ugly', 'unprecedented', 'scene', 'may', 'also', 'last', 'stand', 'hillary', 'clinton', 'awkwardly', 'wound', 'way', 'numerous', 'scandals', 'election', 'cycle', '’', 'never', 'shown', 'fear', 'desperation', 'changed', 'whatever', 'afraid', 'lies', 'buried', 'emails', 'huma', 'abedin', 'bring', 'like', 'nothing', 'else']\n",
      "tagged_docs:\n",
      " TaggedDocument(['daniel', 'greenfield', 'shillman', 'journalism', 'fellow', 'freedom', 'center', 'new', 'york', 'writer', 'focusing', 'radical', 'islam', 'final', 'stretch', 'election', 'hillary', 'rodham', 'clinton', 'gone', 'war', 'fbi', 'word', '“', 'unprecedented', '”', 'thrown', 'around', 'often', 'election', 'ought', 'retired', '’', 'still', 'unprecedented', 'nominee', 'major', 'political', 'party', 'go', 'war', 'fbi', '’', 'exactly', 'hillary', 'people', 'done', 'coma', 'patients', 'waking', 'watching', 'hour', 'cnn', 'hospital', 'beds', 'would', 'assume', 'fbi', 'director', 'james', 'comey', 'hillary', '’', 'opponent', 'election', 'fbi', 'attack', 'everyone', 'obama', 'cnn', 'hillary', '’', 'people', 'circulated', 'letter', 'attacking', 'comey', 'currently', 'media', 'hit', 'pieces', 'lambasting', 'targeting', 'trump', '’', 'surprising', 'clintons', 'allies', 'start', 'running', 'attack', 'ads', 'fbi', 'fbi', '’', 'leadership', 'warned', 'entire', 'left-wing', 'establishment', 'form', 'lynch', 'mob', 'continue', 'going', 'hillary', 'fbi', '’', 'credibility', 'attacked', 'media', 'democrats', 'preemptively', 'head', 'results', 'investigation', 'clinton', 'foundation', 'hillary', 'clinton', 'covert', 'struggle', 'fbi', 'agents', 'obama', '’', 'doj', 'people', 'gone', 'explosively', 'public', 'new', 'york', 'times', 'compared', 'comey', 'j.', 'edgar', 'hoover', 'bizarre', 'headline', '“', 'james', 'comey', 'role', 'recalls', 'hoover', '’', 'fbi', 'fairly', '”', 'practically', 'admits', 'front', '’', 'spouting', 'nonsense', 'boston', 'globe', 'published', 'column', 'calling', 'comey', '’', 'resignation', 'outdone', 'time', 'editorial', 'claiming', 'scandal', 'really', 'attack', 'women', 'james', 'carville', 'appeared', 'msnbc', 'remind', 'everyone', 'still', 'alive', 'insane', 'accused', 'comey', 'coordinating', 'house', 'republicans', 'kgb', 'thought', '“', 'vast', 'right', 'wing', 'conspiracy', '”', 'stretch', 'countless', 'media', 'stories', 'charge', 'comey', 'violating', 'procedure', 'know', '’', 'procedural', 'violation', 'emailing', 'classified', 'information', 'stored', 'bathroom', 'server', 'senator', 'harry', 'reid', 'sent', 'comey', 'letter', 'accusing', 'violating', 'hatch', 'act', 'hatch', 'act', 'nice', 'idea', 'much', 'relevance', 'age', 'obama', 'tenth', 'amendment', 'cable', 'news', 'spectrum', 'quickly', 'filled', 'media', 'hacks', 'glancing', 'wikipedia', 'article', 'hatch', 'act', 'table', 'accusing', 'fbi', 'director', 'one', 'awkward', 'conspiracies', 'hillary', 'ever', 'james', 'comey', 'really', 'hurt', 'hillary', 'picked', 'one', 'hell', 'strange', 'way', 'long', 'ago', 'democrats', 'breathing', 'sigh', 'relief', 'gave', 'hillary', 'clinton', 'pass', 'prominent', 'public', 'statement', 'really', 'elect', 'trump', 'keeping', 'email', 'scandal', 'going', 'trash', 'investigation', 'payroll', 'house', 'republicans', 'kgb', 'back', 'playing', 'coy', 'sudden', 'development', 'vladimir', 'putin', 'paul', 'ryan', 'talked', 'taking', 'look', 'anthony', 'weiner', '’', 'computer', 'either', 'comey', 'cunning', 'fbi', 'director', 'ever', 'lived', '’', 'awkwardly', 'trying', 'navigate', 'political', 'mess', 'trapped', 'doj', 'leadership', 'whose', 'political', 'futures', 'tied', 'hillary', '’', 'victory', 'bureau', 'whose', 'apolitical', 'agents', 'want', 'allowed', 'jobs', 'truly', 'mysterious', 'thing', 'hillary', 'associates', 'decided', 'go', 'war', 'respected', 'federal', 'agency', 'americans', 'like', 'fbi', 'hillary', 'clinton', 'enjoys', '60', 'unfavorable', 'rating', '’', 'interesting', 'question', 'hillary', '’', 'old', 'strategy', 'lie', 'deny', 'fbi', 'even', 'criminal', 'investigation', 'underway', 'instead', 'associates', 'insisted', 'security', 'review', 'fbi', 'corrected', 'shrugged', 'old', 'breezy', 'denial', 'approach', 'given', 'way', 'savage', 'assault', 'fbi', 'pretending', 'nothing', 'wrong', 'bad', 'strategy', 'better', 'one', 'picking', 'fight', 'fbi', 'lunatic', 'clinton', 'associates', 'try', 'claim', 'fbi', 'really', 'kgb', 'two', 'possible', 'explanations', 'hillary', 'clinton', 'might', 'arrogant', 'enough', 'lash', 'fbi', 'believes', 'victory', 'near', 'kind', 'hubris', 'led', 'plan', 'victory', 'fireworks', 'display', 'could', 'lead', 'declare', 'war', 'fbi', 'irritating', 'final', 'miles', 'campaign', 'explanation', 'people', 'panicked', 'going', 'war', 'fbi', 'behavior', 'smart', 'focused', 'presidential', 'campaign', '’', 'act', 'desperation', 'presidential', 'candidate', 'decides', 'option', 'try', 'destroy', 'credibility', 'fbi', '’', 'hubris', '’', 'fear', 'fbi', 'might', 'reveal', 'original', 'fbi', 'investigation', 'hillary', 'clinton', 'confident', 'could', 'ride', 'good', 'reason', 'believing', 'hillary', 'clinton', 'gone', 'place', 'paranoid', 'wreck', 'within', 'short', 'space', 'time', '“', 'positive', '”', 'clinton', 'campaign', 'promising', 'unite', 'country', 'replaced', 'desperate', 'flailing', 'operation', 'focused', 'energy', 'fighting', 'fbi', '’', 'one', 'reason', 'bizarre', 'behavior', 'clinton', 'campaign', 'decided', 'fbi', 'investigation', 'latest', 'batch', 'emails', 'poses', 'threat', 'survival', '’', 'gone', 'fighting', 'fbi', '’', 'unprecedented', 'step', 'born', 'fear', '’', 'hard', 'know', 'whether', 'fear', 'justified', 'existence', 'fear', 'already', 'tells', 'us', 'whole', 'lot', 'clinton', 'loyalists', 'rigged', 'old', 'investigation', 'knew', 'outcome', 'ahead', 'time', 'well', 'knew', 'debate', 'questions', 'suddenly', 'longer', 'control', 'afraid', 'smell', 'fear', 'fbi', 'wiretaps', 'investigation', 'clinton', 'foundation', '’', 'finding', 'new', 'emails', 'time', 'clintonworld', 'panicked', 'spinmeisters', 'clintonworld', 'claimed', 'email', 'scandal', 'much', 'smoke', 'without', 'fire', '’', 'appearance', 'impropriety', 'without', 'substance', '’', 'react', 'smoke', '’', 'respond', 'fire', 'misguided', 'assault', 'fbi', 'tells', 'us', 'hillary', 'clinton', 'allies', 'afraid', 'revelation', 'bigger', 'fundamental', 'illegality', 'email', 'setup', 'email', 'setup', 'preemptive', 'cover', 'clinton', 'campaign', 'panicked', 'badly', 'belief', 'right', 'wrong', 'whatever', 'crime', 'illegal', 'setup', 'meant', 'cover', 'risk', 'exposed', 'clintons', 'weathered', 'countless', 'scandals', 'years', 'whatever', 'protecting', 'time', 'around', 'bigger', 'usual', 'corruption', 'bribery', 'sexual', 'assaults', 'abuses', 'power', 'followed', 'around', 'throughout', 'years', 'bigger', 'damaging', 'allegations', 'already', 'come', '’', 'want', 'fbi', 'investigators', 'anywhere', 'near', 'campaign', 'comey', 'pure', 'intimidation', '’', 'also', 'warning', 'senior', 'fbi', 'people', 'value', 'careers', 'warned', 'stay', 'away', 'democrats', 'closing', 'ranks', 'around', 'nominee', 'fbi', '’', 'ugly', 'unprecedented', 'scene', 'may', 'also', 'last', 'stand', 'hillary', 'clinton', 'awkwardly', 'wound', 'way', 'numerous', 'scandals', 'election', 'cycle', '’', 'never', 'shown', 'fear', 'desperation', 'changed', 'whatever', 'afraid', 'lies', 'buried', 'emails', 'huma', 'abedin', 'bring', 'like', 'nothing', 'else'], [0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:535: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "tokenized_docs = text_cleaned\n",
    "\n",
    "print('tokenized_docs:\\n',tokenized_docs[0])\n",
    "\n",
    "# Convert tokenized documents to TaggedDocuments\n",
    "tagged_docs = [TaggedDocument(doc, tags=[idx]) for idx, doc in enumerate(tokenized_docs)]\n",
    "\n",
    "print('tagged_docs:\\n',tagged_docs[0])\n",
    "\n",
    "# Create and train the doc2vec model. May take a few seconds\n",
    "doc2vec = Doc2Vec(size=300, window=5, min_count=5, dm = 1, iter=10)\n",
    "\n",
    "# Build the word2vec model from the corpus\n",
    "doc2vec.build_vocab(tagged_docs)\n",
    "\n",
    "# Train the models\n",
    "doc2vec.train(tagged_docs, epochs=10, total_examples=doc2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.61935633  1.31671917  0.86740804 -0.1553306   0.15756744  0.43971166\n",
      "  0.25862303 -0.08088062  0.91815299 -0.11218839 -0.6973688   0.37162742\n",
      "  0.30071414  0.59860969  0.39462444  0.63498664 -0.41228274 -0.81001973\n",
      " -0.45109209  0.06411766 -0.58660787 -0.19415447  0.23305048  0.46412504\n",
      " -0.15703677  1.0946511  -0.9181062   0.12459357 -0.0564952  -0.10358798\n",
      "  0.48112518  0.22236991 -0.24725345  0.16025317  1.18726146  0.53739101\n",
      " -0.19515121 -0.13357428 -0.51477522 -0.80267876  0.26043892 -0.28965446\n",
      "  0.17460492 -0.84917319  1.16470981  0.30701593  0.4209992  -0.49751049\n",
      " -1.13366485 -0.19177148 -0.04489039  0.20668036 -1.20697701 -0.38073269\n",
      "  0.3211171  -1.01165211 -0.03382358  0.13255063  0.12060357 -0.17878529\n",
      " -0.3298957  -0.79989678 -0.20509571  0.34377712  0.06300732  0.4184415\n",
      " -0.04797495 -0.04306135 -0.12024839 -0.5250262   0.13047108  0.37586787\n",
      " -0.55637836  0.3299264  -0.2715697  -0.138197   -0.71471792 -0.09325261\n",
      "  0.29613003  0.38286316 -0.67791384 -0.04511396 -0.02479083 -0.08532371\n",
      " -0.85487819 -0.27683988  0.88061517 -0.07939442  0.33512381 -0.39484367\n",
      "  0.72548896 -0.72163576 -0.41918299 -0.41022182 -0.27808738  0.23206992\n",
      " -0.22374173 -0.0114202  -0.43343398 -1.01567447 -0.13116783 -0.2925283\n",
      " -0.37558717 -0.4022651   0.65163261 -0.4311229   0.18186635  0.55395877\n",
      "  0.03498362  0.14613895 -0.47161865 -0.15594934  0.38317531  0.58234745\n",
      " -0.88904482  0.25174627  1.15230012 -0.41464853 -0.08837119  0.10244485\n",
      "  0.77429283  0.49359334 -0.29597813 -1.1900779   0.4421472  -0.22034459\n",
      "  0.11348139  0.01684769 -0.33672988  0.44116163  0.74348068  0.10906176\n",
      " -0.77611738 -0.26098043  0.50636655  0.06223967  0.55982286 -0.70614988\n",
      "  0.49500975 -0.83710706 -0.50757957 -0.04323902  0.46191889 -0.25828469\n",
      "  0.81566024  0.59162629  0.3247804  -0.75850248  0.83491617 -0.02665762\n",
      " -0.58059496  0.07462389  0.63142443  0.27366513  0.05289268 -0.07879202\n",
      " -0.66342819 -0.06354077  0.06691673  0.25341988  1.06270874 -0.28521958\n",
      " -0.76503968 -0.38603118  0.29934132 -0.25512224  0.02195052  0.36032298\n",
      " -0.00840682  0.28747058  0.39752412  0.02259941 -0.5352779   0.1655871\n",
      "  0.53404856  0.27164537  0.27348506  0.4592582   0.99483979 -0.09725315\n",
      " -0.12143937 -0.24939382 -0.29514551 -0.29493085  0.12427089 -0.06598872\n",
      "  0.32102549  0.40810937 -0.2354053   1.0235393  -0.39945567  0.16150612\n",
      "  0.1374121   1.12887919 -0.11683855  0.03444398 -0.22332193  0.34340274\n",
      " -0.23248477  0.65156609  0.81368756  0.22351456 -0.53300947  0.02390262\n",
      "  0.16082036 -0.19993068  0.44621864 -0.21388926  0.14980258 -0.4358803\n",
      " -0.23502575 -0.9249438  -0.65493673 -0.46864375  0.16497663  0.40545508\n",
      " -0.42065886 -0.35448262  1.23096025  0.63232851  0.0631613  -0.27180392\n",
      "  0.8276571  -0.35806429  0.04018091  0.24975675  0.93277067 -0.5188722\n",
      "  0.4443523  -0.08993714 -0.92945898  0.14686234  0.76233864 -0.07021268\n",
      "  0.1547443  -0.50913686 -0.03074897 -0.37845263  0.21723497  0.1354066\n",
      "  1.35564101 -1.55423641 -0.92333585  0.06674555 -0.09572782  1.83510089\n",
      "  0.44970587 -0.87641275  0.86246145 -1.12742305 -0.27519244 -0.00796114\n",
      " -0.48907831 -0.33246398  0.24532185 -0.08378406  0.7434721   1.15114999\n",
      "  0.65114677 -0.39748105 -0.35987008 -0.35224906  1.29130125 -0.39972705\n",
      "  0.51154459  0.05477078 -0.98623657 -0.11759426  0.41598481 -0.70598835\n",
      "  0.00965725 -0.99913192  0.30779371 -0.52686638  0.21418634 -0.04593559\n",
      " -0.18458372 -0.51664674 -0.78631645 -0.76237726 -0.64753222 -0.23559378\n",
      " -0.8313117   0.29210177  0.40183979 -0.01223348  0.24974927 -0.06101559\n",
      " -0.3049711   0.4590641   0.17298558 -0.1464562   1.23261225 -0.75447881\n",
      " -0.2804656   0.36086813  0.15915409 -0.2755487  -0.29200119 -0.64813691]\n"
     ]
    }
   ],
   "source": [
    "doc2vec_data = np.zeros((len(text_cleaned),300))\n",
    "for i in range (len(text_cleaned)):\n",
    "    doc2vec_data[i] = doc2vec.infer_vector(text_cleaned[i])\n",
    "\n",
    "print(doc2vec_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2079, 300)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train and test sets\n",
    "seed = 42\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc2vec_data, labels, test_size=test_size, random_state=seed)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 1 1 1 0]\n",
      "[[502 519]\n",
      " [527 531]]\n",
      "accuracy: 0.49687349687349686\n",
      "precision: 0.5057142857142857\n",
      "recall: 0.501890359168242\n",
      "f1score: 0.5037950664136623\n"
     ]
    }
   ],
   "source": [
    "# instantiate a SVM regression model, and fit with X and y\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X_train, y_train.astype(int))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(y_pred[0:10])\n",
    "# check the accuracy on the training set\n",
    "print(confusion_matrix(y_true=y_test.astype(int), y_pred=y_pred))\n",
    "score = model.score(X_test, y_test.astype(int))\n",
    "\n",
    "p = precision_score(y_test.astype(int), y_pred, average='binary')\n",
    "r = recall_score(y_test.astype(int), y_pred, average='binary')\n",
    "f1score = f1_score(y_test.astype(int), y_pred, average='binary')\n",
    "print('accuracy:',score)\n",
    "print('precision:',p)\n",
    "print('recall:',r)\n",
    "print('f1score:',f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "title = ['1']\n",
    "text = ['1','2','3']\n",
    "for title,text in zip(title,text):\n",
    "    print(title,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.14641257 -0.98182771  1.92635137 -2.04952002  0.00528416 -0.40082222\n",
      " -0.72675502 -0.61174331 -0.70071689  0.52017262]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x = np.random.randn(10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.nan\n",
    "print(x*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'svsdfs', 'sdfsdg', 'title', 'gdsfgsdfg', 'ghjhgjghd']\n"
     ]
    }
   ],
   "source": [
    "title = ['text','svsdfs','sdfsdg']\n",
    "text= ['title','gdsfgsdfg','ghjhgjghd']\n",
    "\n",
    "title.extend(text)\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8 12 16]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([2,3,4])\n",
    "a = a*4\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20,), (40, 40), (60, 60, 60), (80, 80, 80, 80)]\n"
     ]
    }
   ],
   "source": [
    "x = [(i*20,)*i  for i in range(1,5) ]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(200.0, 50.0, 5.0), (200.0, 50.0, 10.0), (200.0, 60.0, 5.0), (200.0, 60.0, 10.0), (200.0, 70.0, 5.0), (200.0, 70.0, 10.0), (200.0, 80.0, 5.0), (200.0, 80.0, 10.0), (200.0, 90.0, 5.0), (200.0, 90.0, 10.0), (200.0, 100.0, 5.0), (200.0, 100.0, 10.0), (225.0, 50.0, 5.0), (225.0, 50.0, 10.0), (225.0, 60.0, 5.0), (225.0, 60.0, 10.0), (225.0, 70.0, 5.0), (225.0, 70.0, 10.0), (225.0, 80.0, 5.0), (225.0, 80.0, 10.0), (225.0, 90.0, 5.0), (225.0, 90.0, 10.0), (225.0, 100.0, 5.0), (225.0, 100.0, 10.0), (250.0, 50.0, 5.0), (250.0, 50.0, 10.0), (250.0, 60.0, 5.0), (250.0, 60.0, 10.0), (250.0, 70.0, 5.0), (250.0, 70.0, 10.0), (250.0, 80.0, 5.0), (250.0, 80.0, 10.0), (250.0, 90.0, 5.0), (250.0, 90.0, 10.0), (250.0, 100.0, 5.0), (250.0, 100.0, 10.0), (275.0, 50.0, 5.0), (275.0, 50.0, 10.0), (275.0, 60.0, 5.0), (275.0, 60.0, 10.0), (275.0, 70.0, 5.0), (275.0, 70.0, 10.0), (275.0, 80.0, 5.0), (275.0, 80.0, 10.0), (275.0, 90.0, 5.0), (275.0, 90.0, 10.0), (275.0, 100.0, 5.0), (275.0, 100.0, 10.0), (300.0, 50.0, 5.0), (300.0, 50.0, 10.0), (300.0, 60.0, 5.0), (300.0, 60.0, 10.0), (300.0, 70.0, 5.0), (300.0, 70.0, 10.0), (300.0, 80.0, 5.0), (300.0, 80.0, 10.0), (300.0, 90.0, 5.0), (300.0, 90.0, 10.0), (300.0, 100.0, 5.0), (300.0, 100.0, 10.0)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hidden_layer_size = []\n",
    "for i in np.linspace(200,300,5):\n",
    "    for j in np.linspace(50,100,6):\n",
    "        for k in np.linspace(5,10,2):\n",
    "            hidden_layer_size.append((i,j,k))\n",
    "print(hidden_layer_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
